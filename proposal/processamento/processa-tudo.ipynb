{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processa todos os datasets da saída\n",
    "\n",
    "Divisão: 80% treino e 20% validação | kfold com 5 folds, pegando os datasets originais para validação e os sintéticos para treino. \n",
    "\n",
    "# 1) Processamento dos dados originais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:   4%|▎         | 1/28 [20:01<9:00:31, 1201.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\1-tratamento\\tratamento_china_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:   7%|▋         | 2/28 [37:11<7:57:04, 1100.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\1-tratamento\\tratamento_cocomo81_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:  11%|█         | 3/28 [46:36<5:56:39, 855.98s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\1-tratamento\\tratamento_desharnais_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:  14%|█▍        | 4/28 [1:09:50<7:07:25, 1068.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\1-tratamento\\tratamento_maxwell_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:  18%|█▊        | 5/28 [1:31:30<7:21:33, 1151.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-25\\tratamento_china_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:  21%|██▏       | 6/28 [1:51:58<7:11:48, 1177.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-25\\tratamento_cocomo81_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:  25%|██▌       | 7/28 [2:01:36<5:43:33, 981.59s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-25\\tratamento_desharnais_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:  29%|██▊       | 8/28 [2:27:58<6:30:54, 1172.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-25\\tratamento_maxwell_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:  32%|███▏      | 9/28 [2:55:28<6:58:38, 1322.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-50\\tratamento_china_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:  36%|███▌      | 10/28 [3:13:17<6:13:12, 1244.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-50\\tratamento_cocomo81_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:  39%|███▉      | 11/28 [3:24:04<5:00:40, 1061.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-50\\tratamento_desharnais_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:  43%|████▎     | 12/28 [3:54:20<5:44:13, 1290.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-50\\tratamento_maxwell_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:  46%|████▋     | 13/28 [4:27:26<6:15:22, 1501.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-75\\tratamento_china_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:  50%|█████     | 14/28 [4:55:08<6:01:39, 1549.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-75\\tratamento_cocomo81_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:  54%|█████▎    | 15/28 [5:10:37<4:55:14, 1362.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-75\\tratamento_desharnais_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:  57%|█████▋    | 16/28 [5:43:59<5:11:02, 1555.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-75\\tratamento_maxwell_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:  61%|██████    | 17/28 [6:02:12<4:19:36, 1416.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-25\\tratamento_china_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:  64%|██████▍   | 18/28 [6:20:43<3:40:44, 1324.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-25\\tratamento_cocomo81_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:  68%|██████▊   | 19/28 [6:30:15<2:44:46, 1098.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-25\\tratamento_desharnais_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:  71%|███████▏  | 20/28 [6:55:21<2:42:46, 1220.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-25\\tratamento_maxwell_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:  75%|███████▌  | 21/28 [7:17:24<2:26:01, 1251.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-50\\tratamento_china_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:  79%|███████▊  | 22/28 [7:41:28<2:10:54, 1309.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-50\\tratamento_cocomo81_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:  82%|████████▏ | 23/28 [7:51:35<1:31:33, 1098.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-50\\tratamento_desharnais_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:  86%|████████▌ | 24/28 [8:23:28<1:29:32, 1343.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-50\\tratamento_maxwell_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:  89%|████████▉ | 25/28 [8:47:37<1:08:44, 1374.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-75\\tratamento_china_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:  93%|█████████▎| 26/28 [9:14:23<48:08, 1444.18s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-75\\tratamento_cocomo81_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets:  96%|█████████▋| 27/28 [9:29:10<21:17, 1277.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-75\\tratamento_desharnais_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets: 100%|██████████| 28/28 [10:05:01<00:00, 1296.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-75\\tratamento_maxwell_resultados_processamento_simples.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.utils.validation import check_X_y\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Função para carregar todos os arquivos .txt de todas as subpastas de um diretório\n",
    "def load_datasets_from_directories(base_directories):\n",
    "    datasets = {}\n",
    "    for base_directory in base_directories:\n",
    "        for root, _, files in os.walk(base_directory):\n",
    "            for file in files:\n",
    "                filepath = os.path.join(root, file)\n",
    "                filename, ext = os.path.splitext(file)\n",
    "\n",
    "                if ext != '.txt':  # Ignorar arquivos que não sejam .txt\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    with open(filepath, 'r') as f:\n",
    "                        first_line = f.readline()\n",
    "                        delimiter = ',' if ',' in first_line else ('\\t' if '\\t' in first_line else ' ')\n",
    "                    datasets[filepath] = pd.read_csv(filepath, delimiter=delimiter)\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao carregar {filepath}: {e}\")\n",
    "\n",
    "    return datasets\n",
    "\n",
    "# Função para processar um único modelo com KFold\n",
    "def process_model_kfold(X, y, model_name, model, param_grid, repetitions, n_splits=5):\n",
    "    metrics = {'mse': [], 'rmse': [], 'r2': [], 'mae': []}\n",
    "\n",
    "    for _ in range(repetitions):\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        grid = GridSearchCV(model, param_grid[model_name], cv=kf, scoring='neg_mean_squared_error', error_score='raise')\n",
    "\n",
    "        try:\n",
    "            grid.fit(X, y)\n",
    "            best_model = grid.best_estimator_\n",
    "\n",
    "            # Avaliação com KFold\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "                best_model.fit(X_train, y_train)\n",
    "                y_pred = best_model.predict(X_test)\n",
    "\n",
    "                # Cálculo das métricas\n",
    "                mse = mean_squared_error(y_test, y_pred)\n",
    "                rmse = np.sqrt(mse)\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "                mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "                metrics['mse'].append(mse)\n",
    "                metrics['rmse'].append(rmse)\n",
    "                metrics['r2'].append(r2)\n",
    "                metrics['mae'].append(mae)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"{model_name}: Falha no treinamento - {e}\")\n",
    "            continue\n",
    "\n",
    "    return {\n",
    "        'mse': np.mean(metrics['mse']),\n",
    "        'rmse': np.mean(metrics['rmse']),\n",
    "        'r2': np.mean(metrics['r2']),\n",
    "        'mae': np.mean(metrics['mae']),\n",
    "        'best_params': grid.best_params_ if 'grid' in locals() else {}\n",
    "    }\n",
    "\n",
    "# Função para treinar e testar modelos de regressão\n",
    "def test_regression_models(data, repetitions=30):\n",
    "    models = {\n",
    "        'LinearRegression': LinearRegression(),\n",
    "        'Ridge': Ridge(),\n",
    "        'RandomForest': RandomForestRegressor(),\n",
    "    }\n",
    "\n",
    "    param_grid = {\n",
    "        'LinearRegression': {},\n",
    "        'Ridge': {'alpha': [0.1, 1, 10]},\n",
    "        'RandomForest': {'n_estimators': [50, 100, 200]},\n",
    "    }\n",
    "\n",
    "    for filepath, df in tqdm(data.items(), desc=\"Processando datasets\"):\n",
    "        output_directory = os.path.dirname(filepath)\n",
    "        output_filename = os.path.splitext(os.path.basename(filepath))[0] + '_resultados_processamento_simples.xlsx'\n",
    "        output_path = os.path.join(output_directory, output_filename)\n",
    "\n",
    "        df = df.dropna()\n",
    "\n",
    "        if df.shape[0] < 2 or df.shape[1] < 2:\n",
    "            print(f\"{filepath}: Dataset insuficiente para treino. Tamanho: {df.shape}\")\n",
    "            continue\n",
    "\n",
    "        X = df.iloc[:, :-1].values\n",
    "        y = df.iloc[:, -1].values\n",
    "\n",
    "        try:\n",
    "            X, y = check_X_y(X, y)\n",
    "        except ValueError as e:\n",
    "            print(f\"{filepath}: Erro de validação dos dados - {e}\")\n",
    "            continue\n",
    "\n",
    "        # Processar cada modelo em paralelo\n",
    "        results = {}\n",
    "        model_results = Parallel(n_jobs=-1)(\n",
    "            delayed(process_model_kfold)(X, y, model_name, model, param_grid, repetitions)\n",
    "            for model_name, model in models.items()\n",
    "        )\n",
    "\n",
    "        # Armazenar resultados\n",
    "        for model_name, metrics in zip(models.keys(), model_results):\n",
    "            results[model_name] = metrics\n",
    "\n",
    "        results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "\n",
    "        try:\n",
    "            results_df.to_excel(output_path)\n",
    "            print(f\"Resultados salvos em '{output_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao salvar os resultados: {e}\")\n",
    "\n",
    "base_directories = [\n",
    "    r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao',\n",
    "]\n",
    "\n",
    "# Carregar os datasets de todas as subpastas\n",
    "data = load_datasets_from_directories(base_directories)\n",
    "\n",
    "# Processar os datasets e salvar resultados\n",
    "if data:\n",
    "    test_regression_models(data, repetitions=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) dados sintéticos | usa dados sintéticos para treino e dados originais para teste.\n",
    "\n",
    "Usa 5 folds, sendo 4 com dados sintéticos e 1 com dados originais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets originais:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão inconsistente entre tratamento_china.txt nos dados sintéticos e originais.\n",
      "Colunas nos dados originais: ['ID', 'AFP', 'Output', 'Enquiry', 'Interface', 'Changed', 'Deleted', 'PDR_AFP', 'Resource', 'Dev.Type', 'Duration', 'N_effort', 'Effort']\n",
      "Colunas nos dados sintéticos: ['ID', 'AFP', 'Input', 'Output', 'Enquiry', 'File', 'Interface', 'Added', 'Changed', 'Deleted', 'PDR_AFP', 'Resource', 'Dev.Type', 'Duration', 'N_effort', 'Effort']\n",
      "Ajustando para usar apenas colunas em comum: ['ID', 'AFP', 'Output', 'Enquiry', 'Interface', 'Changed', 'Deleted', 'PDR_AFP', 'Resource', 'Dev.Type', 'Duration', 'N_effort', 'Effort']\n",
      "Processando tratamento_china.txt - Treino: 1250 amostras, Validação: 499 amostras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets originais:  25%|██▌       | 1/4 [02:25<07:15, 145.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão inconsistente entre tratamento_cocomo81.txt nos dados sintéticos e originais.\n",
      "Colunas nos dados originais: ['rely', 'data', 'cplx', 'time', 'stor', 'virt', 'turn', 'acap', 'aexp', 'pcap', 'vexp', 'lexp', 'modp', 'tool', 'sced', 'loc', 'actual']\n",
      "Colunas nos dados sintéticos: ['rely', 'data', 'virt', 'acap', 'pcap', 'modp', 'actual']\n",
      "Ajustando para usar apenas colunas em comum: ['rely', 'data', 'virt', 'acap', 'pcap', 'modp', 'actual']\n",
      "Processando tratamento_cocomo81.txt - Treino: 1250 amostras, Validação: 63 amostras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets originais:  25%|██▌       | 1/4 [02:42<08:06, 162.03s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 142\u001b[0m\n\u001b[0;32m    139\u001b[0m synthetic_data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCALEO\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive - Hexagon\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mGitHub\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSoftware_effort_estimation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mproposal\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mabordagem2\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msaida\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m4-escolha\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcorrelacao\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m3-simulacao\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1-tratamento\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Executar o processo\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m \u001b[43mtest_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynthetic_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepetitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[66], line 113\u001b[0m, in \u001b[0;36mtest_models\u001b[1;34m(original_data_path, synthetic_data_path, repetitions, n_splits)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# Processar modelos em paralelo\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m     model_results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_model_kfold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepetitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m     results[file] \u001b[38;5;241m=\u001b[39m {model_name: result \u001b[38;5;28;01mfor\u001b[39;00m model_name, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(models\u001b[38;5;241m.\u001b[39mkeys(), model_results)}\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Salvar resultados em um Excel\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Função para carregar datasets\n",
    "def load_datasets(path):\n",
    "    datasets = {}\n",
    "    for root, _, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    datasets[file] = pd.read_csv(file_path, delimiter=None, engine='python')\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao carregar {file}: {e}\")\n",
    "    return datasets\n",
    "\n",
    "# Função para processar o modelo com KFold\n",
    "def process_model_kfold(train_data, test_data, model_name, model, repetitions=30, n_splits=5):\n",
    "    metrics = {'mse': [], 'rmse': [], 'r2': [], 'mae': []}\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    X_train = train_data.iloc[:, :-1].values\n",
    "    y_train = train_data.iloc[:, -1].values\n",
    "    X_test = test_data.iloc[:, :-1].values\n",
    "    y_test = test_data.iloc[:, -1].values\n",
    "\n",
    "    for _ in range(repetitions):\n",
    "        for train_index, val_index in kf.split(X_train):\n",
    "            X_fold_train, X_fold_val = X_train[train_index], X_train[val_index]\n",
    "            y_fold_train, y_fold_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "            try:\n",
    "                model.fit(X_fold_train, y_fold_train)\n",
    "                y_pred = model.predict(X_test)  # Validação com dados originais\n",
    "                mse = mean_squared_error(y_test, y_pred)\n",
    "                rmse = np.sqrt(mse)\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "                mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "                metrics['mse'].append(mse)\n",
    "                metrics['rmse'].append(rmse)\n",
    "                metrics['r2'].append(r2)\n",
    "                metrics['mae'].append(mae)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao treinar/avaliar modelo {model_name}: {e}\")\n",
    "\n",
    "    return {\n",
    "        'mse': np.mean(metrics['mse']),\n",
    "        'rmse': np.mean(metrics['rmse']),\n",
    "        'r2': np.mean(metrics['r2']),\n",
    "        'mae': np.mean(metrics['mae']),\n",
    "        'hyperparameters': model.get_params() if hasattr(model, 'get_params') else 'N/A'\n",
    "    }\n",
    "\n",
    "# Função principal para processar os modelos\n",
    "def test_models(original_data_path, synthetic_data_path, repetitions=30, n_splits=5):\n",
    "    original_datasets = load_datasets(original_data_path)\n",
    "    synthetic_datasets = load_datasets(synthetic_data_path)\n",
    "\n",
    "    if not original_datasets:\n",
    "        print(\"Nenhum dataset original encontrado.\")\n",
    "        return\n",
    "    if not synthetic_datasets:\n",
    "        print(\"Nenhum dataset sintético encontrado.\")\n",
    "        return\n",
    "\n",
    "    models = {\n",
    "        'LinearRegression': LinearRegression(),\n",
    "        'Ridge': Ridge(),\n",
    "        'RandomForest': RandomForestRegressor(),\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for file, test_data in tqdm(original_datasets.items(), desc=\"Processando datasets originais\"):\n",
    "        train_data = synthetic_datasets.get(file)  # Busca pelo mesmo nome do arquivo\n",
    "\n",
    "        # Verificar se o dataset sintético correspondente foi encontrado\n",
    "        if train_data is None:\n",
    "            print(f\"Dataset sintético correspondente para {file} não encontrado. Pulando...\")\n",
    "            continue\n",
    "\n",
    "        # Mostrar colunas para debug em caso de inconsistência\n",
    "        if train_data.shape[1] != test_data.shape[1]:\n",
    "            print(f\"Dimensão inconsistente entre {file} nos dados sintéticos e originais.\")\n",
    "            print(f\"Colunas nos dados originais: {list(test_data.columns)}\")\n",
    "            print(f\"Colunas nos dados sintéticos: {list(train_data.columns)}\")\n",
    "            \n",
    "            # Tentar ajustar colunas automaticamente\n",
    "            common_columns = test_data.columns.intersection(train_data.columns)\n",
    "            if len(common_columns) > 0:\n",
    "                print(f\"Ajustando para usar apenas colunas em comum: {list(common_columns)}\")\n",
    "                train_data = train_data[common_columns]\n",
    "                test_data = test_data[common_columns]\n",
    "            else:\n",
    "                print(f\"Não foi possível ajustar {file}. Pulando...\")\n",
    "                continue\n",
    "\n",
    "        # Log de tamanhos dos datasets\n",
    "        print(f\"Processando {file} - Treino: {train_data.shape[0]} amostras, Validação: {test_data.shape[0]} amostras\")\n",
    "\n",
    "        if train_data.shape[0] < 5 or test_data.shape[0] < 2:\n",
    "            print(f\"Dataset {file} insuficiente para processamento.\")\n",
    "            continue\n",
    "\n",
    "        # Processar modelos em paralelo\n",
    "        model_results = Parallel(n_jobs=-1)(\n",
    "            delayed(process_model_kfold)(train_data, test_data, model_name, model, repetitions, n_splits)\n",
    "            for model_name, model in models.items()\n",
    "        )\n",
    "\n",
    "        results[file] = {model_name: result for model_name, result in zip(models.keys(), model_results)}\n",
    "\n",
    "    # Salvar resultados em um Excel\n",
    "    for file, result in results.items():\n",
    "        rows = []\n",
    "        for model_name, metrics in result.items():\n",
    "            row = metrics.copy()\n",
    "            row['model'] = model_name\n",
    "            rows.append(row)\n",
    "\n",
    "        result_df = pd.DataFrame(rows)\n",
    "        output_file = os.path.splitext(file)[0] + '_resultados_kfold.xlsx'\n",
    "        output_path = os.path.join(synthetic_data_path, output_file)\n",
    "        try:\n",
    "            result_df.to_excel(output_path, index=False)\n",
    "            print(f\"Resultados para {file} salvos em {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao salvar resultados para {file}: {e}\")\n",
    "\n",
    "# Caminhos para os datasets\n",
    "original_data_path = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\correlacao\\1-tratamento'\n",
    "synthetic_data_path = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\correlacao\\3-simulacao\\1-tratamento'\n",
    "\n",
    "# Executar o processo\n",
    "test_models(original_data_path, synthetic_data_path, repetitions=30, n_splits=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dados sintéticos | usa dados sintéticos com dados originais para treinamento e para teste usa apenas os dados originais.\n",
    "\n",
    "usa 5 folds, com dados originais e sintéticos. usa 1 fold para teste com dados originais sempre. Os outros 4 folds usam uma mescla de dados sintéticos e originais juntos. Mesma quantidade de variáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets originais: 100%|██████████| 4/4 [18:00<00:00, 270.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-25\\tratamento_china.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-25\\tratamento_cocomo81.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-25\\tratamento_desharnais.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-25\\tratamento_maxwell.txt_resultados_alisson.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets originais: 100%|██████████| 4/4 [23:05<00:00, 346.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-50\\tratamento_china.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-50\\tratamento_cocomo81.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-50\\tratamento_desharnais.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-50\\tratamento_maxwell.txt_resultados_alisson.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets originais: 100%|██████████| 4/4 [24:59<00:00, 374.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-75\\tratamento_china.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-75\\tratamento_cocomo81.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-75\\tratamento_desharnais.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-75\\tratamento_maxwell.txt_resultados_alisson.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets originais: 100%|██████████| 4/4 [17:29<00:00, 262.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-25\\tratamento_china.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-25\\tratamento_cocomo81.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-25\\tratamento_desharnais.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-25\\tratamento_maxwell.txt_resultados_alisson.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets originais: 100%|██████████| 4/4 [23:03<00:00, 345.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-50\\tratamento_china.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-50\\tratamento_cocomo81.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-50\\tratamento_desharnais.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-50\\tratamento_maxwell.txt_resultados_alisson.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets originais: 100%|██████████| 4/4 [25:08<00:00, 377.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-75\\tratamento_china.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-75\\tratamento_cocomo81.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-75\\tratamento_desharnais.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-75\\tratamento_maxwell.txt_resultados_alisson.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Função para carregar datasets\n",
    "def load_datasets(path):\n",
    "    datasets = {}\n",
    "    for root, _, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    datasets[file] = pd.read_csv(file_path, delimiter=None, engine='python')\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao carregar {file}: {e}\")\n",
    "    return datasets\n",
    "\n",
    "# Função para processar o modelo com KFold adaptado\n",
    "def process_kfold_with_synthetic(original_data, synthetic_data, models, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    metrics = {model_name: {'mse': [], 'rmse': [], 'r2': [], 'mae': []} for model_name in models.keys()}\n",
    "    best_params = {}\n",
    "\n",
    "    # Divisão dos dados originais em folds\n",
    "    X_original = original_data.iloc[:, :-1].values\n",
    "    y_original = original_data.iloc[:, -1].values\n",
    "    X_synthetic = synthetic_data.iloc[:, :-1].values\n",
    "    y_synthetic = synthetic_data.iloc[:, -1].values\n",
    "\n",
    "    for fold_idx, (train_index, test_index) in enumerate(kf.split(X_original), 1):\n",
    "        # Dados de teste (fold atual)\n",
    "        X_fold_test, y_fold_test = X_original[test_index], y_original[test_index]\n",
    "\n",
    "        # Dados de treino (sintéticos + outros folds de dados originais)\n",
    "        X_fold_train, y_fold_train = X_original[train_index], y_original[train_index]\n",
    "        X_train_combined = np.vstack([X_synthetic, X_fold_train])\n",
    "        y_train_combined = np.concatenate([y_synthetic, y_fold_train])\n",
    "\n",
    "        # Avaliar cada modelo\n",
    "        for model_name, model in models.items():\n",
    "            try:\n",
    "                model.fit(X_train_combined, y_train_combined)\n",
    "                y_pred = model.predict(X_fold_test)\n",
    "\n",
    "                # Métricas de avaliação\n",
    "                mse = mean_squared_error(y_fold_test, y_pred)\n",
    "                rmse = np.sqrt(mse)\n",
    "                r2 = r2_score(y_fold_test, y_pred)\n",
    "                mae = mean_absolute_error(y_fold_test, y_pred)\n",
    "\n",
    "                # Armazenar métricas\n",
    "                metrics[model_name]['mse'].append(mse)\n",
    "                metrics[model_name]['rmse'].append(rmse)\n",
    "                metrics[model_name]['r2'].append(r2)\n",
    "                metrics[model_name]['mae'].append(mae)\n",
    "\n",
    "                # Salvar hiperparâmetros\n",
    "                best_params[model_name] = model.get_params()\n",
    "            except Exception as e:\n",
    "                print(f\"Erro no modelo {model_name} no Fold {fold_idx}: {e}\")\n",
    "\n",
    "    # Calcular métricas médias para cada modelo\n",
    "    final_metrics = {\n",
    "        model_name: {\n",
    "            'mse': np.mean(metrics[model_name]['mse']),\n",
    "            'rmse': np.mean(metrics[model_name]['rmse']),\n",
    "            'r2': np.mean(metrics[model_name]['r2']),\n",
    "            'mae': np.mean(metrics[model_name]['mae'])\n",
    "        }\n",
    "        for model_name in models.keys()\n",
    "    }\n",
    "\n",
    "    return final_metrics, best_params\n",
    "\n",
    "# Função principal\n",
    "def test_models_with_synthetic_and_kfold(original_data_path, synthetic_data_path, n_splits=5, test_size=0.2, repetitions=30):\n",
    "    original_datasets = load_datasets(original_data_path)\n",
    "    synthetic_datasets = load_datasets(synthetic_data_path)\n",
    "\n",
    "    if not original_datasets:\n",
    "        print(\"Nenhum dataset original encontrado.\")\n",
    "        return\n",
    "    if not synthetic_datasets:\n",
    "        print(\"Nenhum dataset sintético encontrado.\")\n",
    "        return\n",
    "\n",
    "    models = {\n",
    "        'LinearRegression': LinearRegression(),\n",
    "        'Ridge': Ridge(),\n",
    "        'RandomForest': RandomForestRegressor(),\n",
    "    }\n",
    "\n",
    "    overall_results = {}\n",
    "    best_model_params = {}\n",
    "\n",
    "    for file, original_data in tqdm(original_datasets.items(), desc=\"Processando datasets originais\"):\n",
    "        synthetic_data = synthetic_datasets.get(file)\n",
    "\n",
    "        if synthetic_data is None:\n",
    "            print(f\"Dataset sintético correspondente para {file} não encontrado. Pulando...\")\n",
    "            continue\n",
    "\n",
    "        # Garantir divisão 80/20\n",
    "        n_test = int(len(original_data) * test_size)\n",
    "        if n_test < n_splits:\n",
    "            print(f\"Dataset {file} possui amostras insuficientes para dividir em {n_splits} folds com 80/20. Pulando...\")\n",
    "            continue\n",
    "\n",
    "        # Rodar processamento 30 vezes\n",
    "        file_results = {model_name: {'mse': [], 'rmse': [], 'r2': [], 'mae': []} for model_name in models.keys()}\n",
    "        for _ in range(repetitions):\n",
    "            metrics, best_params = process_kfold_with_synthetic(original_data, synthetic_data, models, n_splits)\n",
    "            for model_name, metric_values in metrics.items():\n",
    "                for key in metric_values:\n",
    "                    file_results[model_name][key].append(metric_values[key])\n",
    "                best_model_params[model_name] = best_params[model_name]\n",
    "\n",
    "        # Calcular médias finais das métricas\n",
    "        overall_results[file] = {\n",
    "            model_name: {metric: np.mean(values) for metric, values in model_metrics.items()}\n",
    "            for model_name, model_metrics in file_results.items()\n",
    "        }\n",
    "\n",
    "    # Salvar resultados\n",
    "    for file, metrics in overall_results.items():\n",
    "        results = []\n",
    "        for model_name, model_metrics in metrics.items():\n",
    "            results.append({'Model': model_name, **model_metrics, 'Best Params': best_model_params[model_name]})\n",
    "        result_df = pd.DataFrame(results)\n",
    "        output_file = os.path.join(synthetic_data_path, f'{file}_resultados_alisson.xlsx')\n",
    "        result_df.to_excel(output_file, index=False)\n",
    "        print(f\"Resultados finais salvos em {output_file}\")\n",
    "\n",
    "# Caminhos para os datasets\n",
    "original_data_path = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\1-tratamento-target-encoding-normalizado'\n",
    "synthetic_data_path = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\1-tratamento'\n",
    "\n",
    "original_data_path2 = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\2-geracao-variaveis\\GAN-25'\n",
    "synthetic_data_path2 = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-25'\n",
    "\n",
    "original_data_path3 = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\2-geracao-variaveis\\GAN-50'\n",
    "synthetic_data_path3 = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-50'\n",
    "\n",
    "original_data_path4 = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\2-geracao-variaveis\\GAN-75'\n",
    "synthetic_data_path4 = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\GAN-75'\n",
    "\n",
    "original_data_path5 = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\2-geracao-variaveis\\MIMIC-25'\n",
    "synthetic_data_path5 = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-25'\n",
    "\n",
    "original_data_path6 = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\2-geracao-variaveis\\MIMIC-50'\n",
    "synthetic_data_path6 = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-50'\n",
    "\n",
    "original_data_path7 = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\2-geracao-variaveis\\MIMIC-75'\n",
    "synthetic_data_path7 = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\MIMIC-75'\n",
    "\n",
    "# Executar o processo\n",
    "test_models_with_synthetic_and_kfold(original_data_path2, synthetic_data_path2, n_splits=5, test_size=0.2, repetitions=30)\n",
    "test_models_with_synthetic_and_kfold(original_data_path3, synthetic_data_path3, n_splits=5, test_size=0.2, repetitions=30)\n",
    "test_models_with_synthetic_and_kfold(original_data_path4, synthetic_data_path4, n_splits=5, test_size=0.2, repetitions=30)\n",
    "test_models_with_synthetic_and_kfold(original_data_path5, synthetic_data_path5, n_splits=5, test_size=0.2, repetitions=30)\n",
    "test_models_with_synthetic_and_kfold(original_data_path6, synthetic_data_path6, n_splits=5, test_size=0.2, repetitions=30)\n",
    "test_models_with_synthetic_and_kfold(original_data_path7, synthetic_data_path7, n_splits=5, test_size=0.2, repetitions=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dados sintéticos | usa dados sintéticos com dados originais para treinamento e para teste usa apenas os dados originais.\n",
    "\n",
    "usa 5 folds, com dados originais e sintéticos. usa 1 fold para teste com dados originais sempre. Os outros 4 folds usam uma mescla de dados sintéticos e originais juntos. \n",
    "\n",
    "# (usa apenas as variáveis presentes em ambos os datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets originais: 100%|██████████| 4/4 [11:38<00:00, 174.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\1-tratamento\\tratamento_china.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\1-tratamento\\tratamento_cocomo81.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\1-tratamento\\tratamento_desharnais.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\1-tratamento\\tratamento_maxwell.txt_resultados_alisson.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets originais: 100%|██████████| 4/4 [13:15<00:00, 198.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\GAN-25\\tratamento_china.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\GAN-25\\tratamento_cocomo81.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\GAN-25\\tratamento_desharnais.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\GAN-25\\tratamento_maxwell.txt_resultados_alisson.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets originais: 100%|██████████| 4/4 [16:56<00:00, 254.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\GAN-50\\tratamento_china.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\GAN-50\\tratamento_cocomo81.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\GAN-50\\tratamento_desharnais.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\GAN-50\\tratamento_maxwell.txt_resultados_alisson.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets originais: 100%|██████████| 4/4 [19:11<00:00, 287.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\GAN-75\\tratamento_china.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\GAN-75\\tratamento_cocomo81.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\GAN-75\\tratamento_desharnais.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\GAN-75\\tratamento_maxwell.txt_resultados_alisson.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets originais: 100%|██████████| 4/4 [14:33<00:00, 218.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\MIMIC-25\\tratamento_china.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\MIMIC-25\\tratamento_cocomo81.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\MIMIC-25\\tratamento_desharnais.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\MIMIC-25\\tratamento_maxwell.txt_resultados_alisson.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets originais: 100%|██████████| 4/4 [15:37<00:00, 234.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\MIMIC-50\\tratamento_china.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\MIMIC-50\\tratamento_cocomo81.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\MIMIC-50\\tratamento_desharnais.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\MIMIC-50\\tratamento_maxwell.txt_resultados_alisson.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando datasets originais: 100%|██████████| 4/4 [16:14<00:00, 243.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\MIMIC-75\\tratamento_china.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\MIMIC-75\\tratamento_cocomo81.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\MIMIC-75\\tratamento_desharnais.txt_resultados_alisson.xlsx\n",
      "Resultados finais salvos em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\MIMIC-75\\tratamento_maxwell.txt_resultados_alisson.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Função para carregar datasets com seleção de variáveis comuns\n",
    "def load_datasets_with_variable_selection(original_path, synthetic_path):\n",
    "    original_datasets = {}\n",
    "    synthetic_datasets = {}\n",
    "\n",
    "    for root, _, files in os.walk(original_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                original_file_path = os.path.join(root, file)\n",
    "                synthetic_file_path = os.path.join(synthetic_path, file)\n",
    "                \n",
    "                if not os.path.exists(synthetic_file_path):\n",
    "                    print(f\"Dataset sintético correspondente para {file} não encontrado. Pulando...\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Carregar datasets\n",
    "                    original_data = pd.read_csv(original_file_path, delimiter=None, engine='python')\n",
    "                    synthetic_data = pd.read_csv(synthetic_file_path, delimiter=None, engine='python')\n",
    "\n",
    "                    # Verificar e selecionar variáveis comuns\n",
    "                    common_columns = original_data.columns.intersection(synthetic_data.columns)\n",
    "                    if len(common_columns) < 2:  # Pelo menos uma variável + target são esperados\n",
    "                        print(f\"Dataset {file} não possui variáveis comuns suficientes. Pulando...\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Manter apenas colunas comuns\n",
    "                    original_datasets[file] = original_data[common_columns]\n",
    "                    synthetic_datasets[file] = synthetic_data[common_columns]\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao carregar {file}: {e}\")\n",
    "    \n",
    "    return original_datasets, synthetic_datasets\n",
    "\n",
    "# Função para processar o modelo com KFold adaptado\n",
    "def process_kfold_with_synthetic(original_data, synthetic_data, models, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    metrics = {model_name: {'mse': [], 'rmse': [], 'r2': [], 'mae': []} for model_name in models.keys()}\n",
    "    best_params = {}\n",
    "\n",
    "    # Divisão dos dados originais em folds\n",
    "    X_original = original_data.iloc[:, :-1].values\n",
    "    y_original = original_data.iloc[:, -1].values\n",
    "    X_synthetic = synthetic_data.iloc[:, :-1].values\n",
    "    y_synthetic = synthetic_data.iloc[:, -1].values\n",
    "\n",
    "    for fold_idx, (train_index, test_index) in enumerate(kf.split(X_original), 1):\n",
    "        # Dados de teste (fold atual)\n",
    "        X_fold_test, y_fold_test = X_original[test_index], y_original[test_index]\n",
    "\n",
    "        # Dados de treino (sintéticos + outros folds de dados originais)\n",
    "        X_fold_train, y_fold_train = X_original[train_index], y_original[train_index]\n",
    "        X_train_combined = np.vstack([X_synthetic, X_fold_train])\n",
    "        y_train_combined = np.concatenate([y_synthetic, y_fold_train])\n",
    "\n",
    "        # Avaliar cada modelo\n",
    "        for model_name, model in models.items():\n",
    "            try:\n",
    "                model.fit(X_train_combined, y_train_combined)\n",
    "                y_pred = model.predict(X_fold_test)\n",
    "\n",
    "                # Métricas de avaliação\n",
    "                mse = mean_squared_error(y_fold_test, y_pred)\n",
    "                rmse = np.sqrt(mse)\n",
    "                r2 = r2_score(y_fold_test, y_pred)\n",
    "                mae = mean_absolute_error(y_fold_test, y_pred)\n",
    "\n",
    "                # Armazenar métricas\n",
    "                metrics[model_name]['mse'].append(mse)\n",
    "                metrics[model_name]['rmse'].append(rmse)\n",
    "                metrics[model_name]['r2'].append(r2)\n",
    "                metrics[model_name]['mae'].append(mae)\n",
    "\n",
    "                # Salvar hiperparâmetros\n",
    "                best_params[model_name] = model.get_params()\n",
    "            except Exception as e:\n",
    "                print(f\"Erro no modelo {model_name} no Fold {fold_idx}: {e}\")\n",
    "\n",
    "    # Calcular métricas médias para cada modelo\n",
    "    final_metrics = {\n",
    "        model_name: {\n",
    "            'mse': np.mean(metrics[model_name]['mse']),\n",
    "            'rmse': np.mean(metrics[model_name]['rmse']),\n",
    "            'r2': np.mean(metrics[model_name]['r2']),\n",
    "            'mae': np.mean(metrics[model_name]['mae'])\n",
    "        }\n",
    "        for model_name in models.keys()\n",
    "    }\n",
    "\n",
    "    return final_metrics, best_params\n",
    "\n",
    "# Função principal com seleção de variáveis comuns\n",
    "def test_models_with_selected_variables(original_data_path, synthetic_data_path, n_splits=5, test_size=0.2, repetitions=30):\n",
    "    original_datasets, synthetic_datasets = load_datasets_with_variable_selection(original_data_path, synthetic_data_path)\n",
    "\n",
    "    if not original_datasets:\n",
    "        print(\"Nenhum dataset original encontrado ou compatível.\")\n",
    "        return\n",
    "    if not synthetic_datasets:\n",
    "        print(\"Nenhum dataset sintético encontrado ou compatível.\")\n",
    "        return\n",
    "\n",
    "    models = {\n",
    "        'LinearRegression': LinearRegression(),\n",
    "        'Ridge': Ridge(),\n",
    "        'RandomForest': RandomForestRegressor(),\n",
    "    }\n",
    "\n",
    "    overall_results = {}\n",
    "    best_model_params = {}\n",
    "\n",
    "    for file, original_data in tqdm(original_datasets.items(), desc=\"Processando datasets originais\"):\n",
    "        synthetic_data = synthetic_datasets.get(file)\n",
    "\n",
    "        # Garantir divisão 80/20\n",
    "        n_test = int(len(original_data) * test_size)\n",
    "        if n_test < n_splits:\n",
    "            print(f\"Dataset {file} possui amostras insuficientes para dividir em {n_splits} folds com 80/20. Pulando...\")\n",
    "            continue\n",
    "\n",
    "        # Rodar processamento 30 vezes\n",
    "        file_results = {model_name: {'mse': [], 'rmse': [], 'r2': [], 'mae': []} for model_name in models.keys()}\n",
    "        for _ in range(repetitions):\n",
    "            metrics, best_params = process_kfold_with_synthetic(original_data, synthetic_data, models, n_splits)\n",
    "            for model_name, metric_values in metrics.items():\n",
    "                for key in metric_values:\n",
    "                    file_results[model_name][key].append(metric_values[key])\n",
    "                best_model_params[model_name] = best_params[model_name]\n",
    "\n",
    "        # Calcular médias finais das métricas\n",
    "        overall_results[file] = {\n",
    "            model_name: {metric: np.mean(values) for metric, values in model_metrics.items()}\n",
    "            for model_name, model_metrics in file_results.items()\n",
    "        }\n",
    "\n",
    "    # Salvar resultados\n",
    "    for file, metrics in overall_results.items():\n",
    "        results = []\n",
    "        for model_name, model_metrics in metrics.items():\n",
    "            results.append({'Model': model_name, **model_metrics, 'Best Params': best_model_params[model_name]})\n",
    "        result_df = pd.DataFrame(results)\n",
    "        output_file = os.path.join(synthetic_data_path, f'{file}_resultados_alisson.xlsx')\n",
    "        result_df.to_excel(output_file, index=False)\n",
    "        print(f\"Resultados finais salvos em {output_file}\")\n",
    "\n",
    "# Caminhos para os datasets\n",
    "original_data_path1 = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\1-tratamento-target-encoding-normalizado'\n",
    "synthetic_data_path1 = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\1-tratamento'\n",
    "\n",
    "original_data_path2 = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\2-geracao-variaveis\\GAN-25'\n",
    "synthetic_data_path2 = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\GAN-25'\n",
    "\n",
    "original_data_path3 = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\2-geracao-variaveis\\GAN-50'\n",
    "synthetic_data_path3 = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\GAN-50'\n",
    "\n",
    "original_data_path4 = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\2-geracao-variaveis\\GAN-75'\n",
    "synthetic_data_path4 = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\GAN-75'\n",
    "\n",
    "original_data_path5 = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\2-geracao-variaveis\\MIMIC-25'\n",
    "synthetic_data_path5 = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\MIMIC-25'\n",
    "\n",
    "original_data_path6 = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\2-geracao-variaveis\\MIMIC-50'\n",
    "synthetic_data_path6 = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\MIMIC-50'\n",
    "\n",
    "original_data_path7 = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\2-geracao-variaveis\\MIMIC-75'\n",
    "synthetic_data_path7 = r'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\4-escolha\\selectKbest\\75\\3-simulacao\\MIMIC-75'\n",
    "\n",
    "# Executar o processo com seleção de variáveis comuns\n",
    "test_models_with_selected_variables(original_data_path1, synthetic_data_path1, n_splits=5, test_size=0.2, repetitions=30)\n",
    "test_models_with_selected_variables(original_data_path2, synthetic_data_path2, n_splits=5, test_size=0.2, repetitions=30)\n",
    "test_models_with_selected_variables(original_data_path3, synthetic_data_path3, n_splits=5, test_size=0.2, repetitions=30)\n",
    "test_models_with_selected_variables(original_data_path4, synthetic_data_path4, n_splits=5, test_size=0.2, repetitions=30)\n",
    "test_models_with_selected_variables(original_data_path5, synthetic_data_path5, n_splits=5, test_size=0.2, repetitions=30)\n",
    "test_models_with_selected_variables(original_data_path6, synthetic_data_path6, n_splits=5, test_size=0.2, repetitions=30)\n",
    "test_models_with_selected_variables(original_data_path7, synthetic_data_path7, n_splits=5, test_size=0.2, repetitions=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
