{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previsões com as variáveis independentes para verificar se o dataset consegue prever a variável dependente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando a leitura do diretório: C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem\\saida\\1-tratamento\n",
      "\n",
      "Processamento concluído.\n",
      "Nenhum arquivo válido foi carregado.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_txt_files_from_folders(root_directory):\n",
    "    relevant_features_data = {}\n",
    "\n",
    "    # Verifica se o diretório raiz existe\n",
    "    if not os.path.exists(root_directory):\n",
    "        print(f\"Diretório raiz não encontrado: {root_directory}\")\n",
    "        return relevant_features_data\n",
    "\n",
    "    print(f\"Iniciando a leitura do diretório: {root_directory}\")\n",
    "\n",
    "    # Percorre todas as subpastas dentro do diretório raiz\n",
    "    for folder_name in os.listdir(root_directory):\n",
    "        folder_path = os.path.join(root_directory, folder_name)\n",
    "\n",
    "        if os.path.isdir(folder_path):\n",
    "            print(f\"Entrando na pasta: {folder_path}\")\n",
    "            txt_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "\n",
    "            if not txt_files:\n",
    "                print(f\"NENHUM arquivo .txt encontrado na pasta: {folder_name}\")\n",
    "                continue  # Pula para a próxima pasta se não houver arquivos .txt\n",
    "\n",
    "            for filename in txt_files:\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                print(f\"\\nProcessando arquivo: {file_path}\")\n",
    "\n",
    "                try:\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        lines = file.readlines()\n",
    "\n",
    "                        # Verifica se o arquivo está vazio\n",
    "                        if len(lines) == 0:\n",
    "                            print(f\"Arquivo vazio: {file_path}\")\n",
    "                            continue\n",
    "\n",
    "                        header = lines[0].strip()\n",
    "                        data = [line.strip().split(',') for line in lines[1:] if line.strip()]\n",
    "\n",
    "                        # Verifica se o conteúdo após o cabeçalho está vazio\n",
    "                        if len(data) == 0:\n",
    "                            print(f\"Sem dados após o cabeçalho: {file_path}\")\n",
    "                            continue\n",
    "\n",
    "                        # Conversão segura para float\n",
    "                        try:\n",
    "                            data = np.array(data, dtype=float)\n",
    "                        except ValueError as e:\n",
    "                            print(f\"Erro ao converter dados em {file_path}: {e}\")\n",
    "                            print(f\"Primeiras linhas problemáticas: {lines[1:5]}\")  # Mostra as 5 primeiras linhas após o cabeçalho\n",
    "                            continue\n",
    "\n",
    "                        key = f\"{folder_name}/{filename}\"\n",
    "                        relevant_features_data[key] = (header, data)\n",
    "                        print(f\"Arquivo carregado com sucesso: {key} - Shape: {data.shape}\")\n",
    "\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"Arquivo não encontrado: {file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro inesperado ao processar {file_path}: {e}\")\n",
    "\n",
    "    print(\"\\nProcessamento concluído.\")\n",
    "    return relevant_features_data\n",
    "\n",
    "# Exemplo de uso:\n",
    "root_directory = r\"C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem\\saida\\1-tratamento\"\n",
    "datasets = load_txt_files_from_folders(root_directory)\n",
    "\n",
    "# Exibindo o resumo dos arquivos carregados\n",
    "if datasets:\n",
    "    print(\"\\nResumo dos arquivos carregados:\")\n",
    "    for key, (header, data) in datasets.items():\n",
    "        print(f\"Arquivo processado: {key}\")\n",
    "        print(f\"Header: {header}\")\n",
    "        print(f\"Shape dos dados: {data.shape}\")\n",
    "        print(\"-\" * 50)\n",
    "else:\n",
    "    print(\"Nenhum arquivo válido foi carregado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos os resultados foram salvos em 'C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\processamento\\resultados\\1-tratamento\\resultados.xlsx'\n",
      "Todos os resultados foram salvos em 'resultados_hiperparametros.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from itertools import product\n",
    "\n",
    "# Função para realizar a previsão e avaliação usando KFold com diferentes hiperparâmetros\n",
    "def predict_and_evaluate_kfold(data, header, n_splits=10, fit_intercept=True, positive=False):\n",
    "    # Converter o array para um DataFrame\n",
    "    df = pd.DataFrame(data, columns=header.split(','))\n",
    "\n",
    "    # Separar variáveis independentes (X) e variável dependente (y)\n",
    "    X = df.iloc[:, :-1].values  # Todas as colunas, exceto a última\n",
    "    y = df.iloc[:, -1].values   # Última coluna (variável dependente)\n",
    "\n",
    "    # Configurar KFold\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    rmse_list = []\n",
    "    r2_list = []\n",
    "\n",
    "    # Treinar e testar o modelo para cada fold\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model = LinearRegression(fit_intercept=fit_intercept, positive=positive)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calcular métricas para cada fold\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        rmse_list.append(rmse)\n",
    "        r2_list.append(r2)\n",
    "\n",
    "    # Retornar a média das métricas entre os folds\n",
    "    return np.mean(rmse_list), np.mean(r2_list)\n",
    "\n",
    "# Hiperparâmetros possíveis para LinearRegression\n",
    "hyperparameters = {\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"positive\": [True, False]\n",
    "}\n",
    "\n",
    "# Aplicar o modelo para cada dataset\n",
    "results = []\n",
    "for key, (header, data) in datasets.items():\n",
    "    print(f\"\\nProcessando arquivo: {key}\")\n",
    "    best_rmse, best_r2 = float('inf'), float('-inf')\n",
    "    best_params = {}\n",
    "\n",
    "    for params in product(*hyperparameters.values()):\n",
    "        fit_intercept, positive = params\n",
    "\n",
    "        for _ in range(30):  # Executa 30 vezes com diferentes divisões de KFold\n",
    "            try:\n",
    "                rmse, r2 = predict_and_evaluate_kfold(data, header, fit_intercept=fit_intercept, positive=positive)\n",
    "\n",
    "                # Atualiza os melhores valores e hiperparâmetros\n",
    "                if r2 > best_r2 or (r2 == best_r2 and rmse < best_rmse):\n",
    "                    best_rmse, best_r2 = rmse, r2\n",
    "                    best_params = {\"fit_intercept\": fit_intercept, \"positive\": positive}\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar {key} na iteração {_ + 1} com {params}: {e}\")\n",
    "\n",
    "    # Armazena o melhor resultado para a base\n",
    "    results.append((key, best_rmse, best_r2, best_params))\n",
    "\n",
    "# Converter os resultados para um DataFrame\n",
    "all_results_df = pd.DataFrame(results, columns=[\"Arquivo\", \"RMSE\", \"R²\", \"Melhores Hiperparâmetros\"])\n",
    "\n",
    "# Salvar o DataFrame em um arquivo Excel\n",
    "output_path = r\"C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\processamento\\resultados\\1-tratamento\\resultados.xlsx\"\n",
    "all_results_df.to_excel(output_path, index=False)\n",
    "print(f\"Todos os resultados foram salvos em '{output_path}'\")\n",
    "\n",
    "\n",
    "print(\"Todos os resultados foram salvos em 'resultados_hiperparametros.xlsx'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
