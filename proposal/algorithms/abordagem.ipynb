{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de arquivos lidos e salvos: 5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def extract_arff_variables_and_data(directory_path, output_directory):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    arff_data = {}\n",
    "\n",
    "    # Itera sobre todos os arquivos do diretório\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.arff'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "                # Captura as variáveis antes de @data\n",
    "                variables = []\n",
    "                for line in lines:\n",
    "                    line = line.strip()\n",
    "                    if line.lower().startswith('@attribute'):\n",
    "                        # Extrai o nome da variável\n",
    "                        parts = line.split()\n",
    "                        if len(parts) > 1:\n",
    "                            variables.append(parts[1])\n",
    "                    elif line.lower().startswith('@data'):\n",
    "                        break\n",
    "\n",
    "                # Encontrar a linha onde começa o @data\n",
    "                data_index = next((i for i, line in enumerate(lines) if '@data' in line.lower()), None)\n",
    "                data_content = lines[data_index + 1:] if data_index is not None else []\n",
    "\n",
    "                # Organiza os dados em uma lista\n",
    "                organized_data = [\",\".join(variables)] + [line.strip() for line in data_content if line.strip()]\n",
    "                arff_data[filename] = organized_data\n",
    "\n",
    "                # Salva em um arquivo TXT\n",
    "                output_file_path = os.path.join(output_directory, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "                with open(output_file_path, 'w') as output_file:\n",
    "                    for line in organized_data:\n",
    "                        output_file.write(line + \"\\n\")\n",
    "\n",
    "    return arff_data\n",
    "\n",
    "# Caminho para o diretório de entrada e saída\n",
    "input_directory_path = '../datasets'\n",
    "output_directory_path = './output_pre_processamento/'\n",
    "arff_data = extract_arff_variables_and_data(input_directory_path, output_directory_path)\n",
    "\n",
    "# Exibir o conteúdo e a quantidade de arquivos lidos\n",
    "cont = len(arff_data)\n",
    "print(f\"Quantidade de arquivos lidos e salvos: {cont}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# China\n",
    "\n",
    "O dataset fornecido contém 499 registros e 19 variáveis diferentes. Cada registro corresponde a uma observação do sistema, detalhando características relacionadas ao desenvolvimento de software. Abaixo está a descrição das variáveis:\n",
    "\n",
    "1. **ID**: Um identificador numérico único para cada instância no dataset. Ele é usado para diferenciar as observações.\n",
    "\n",
    "2. **AFP**: \"Adjusted Function Points\" – Refere-se aos pontos de função ajustados, que representam uma métrica de complexidade funcional de software, levando em consideração fatores como entradas, saídas, consultas, arquivos internos e interfaces.\n",
    "\n",
    "3. **Input**: Número de entradas fornecidas para o sistema. Estas entradas podem ser formulários, comandos ou outras formas de dados recebidos.\n",
    "\n",
    "4. **Output**: Número de saídas geradas pelo sistema, como relatórios, mensagens ou outros dados fornecidos aos usuários.\n",
    "\n",
    "5. **Enquiry**: Quantidade de consultas ou buscas que o sistema é capaz de realizar. Estas são interações que resultam na recuperação de informações sem modificar o sistema.\n",
    "\n",
    "6. **File**: Número de arquivos lógicos internos que o sistema gerencia. Isso inclui conjuntos de dados internos ou bancos de dados.\n",
    "\n",
    "7. **Interface**: Número de interfaces externas, como APIs ou conexões com outros sistemas.\n",
    "\n",
    "8. **Added**: Número de funcionalidades ou módulos adicionados ao sistema durante o desenvolvimento.\n",
    "\n",
    "9. **Changed**: Número de funcionalidades ou módulos alterados. Esta variável captura as modificações feitas durante a atualização ou manutenção do sistema.\n",
    "\n",
    "10. **Deleted**: Número de funcionalidades ou módulos removidos do sistema, indicando a limpeza ou simplificação da funcionalidade.\n",
    "\n",
    "11. **PDR_AFP**: \"Productivity Delivery Rate – Adjusted Function Points\" – Refere-se à taxa de produtividade medida em relação aos pontos de função ajustados.\n",
    "\n",
    "12. **PDR_UFP**: \"Productivity Delivery Rate – Unadjusted Function Points\" – Taxa de produtividade com base nos pontos de função não ajustados. Isso mede a produtividade sem ajustes de complexidade.\n",
    "\n",
    "13. **NPDR_AFP**: \"Normalized Productivity Delivery Rate – Adjusted Function Points\" – Taxa de produtividade normalizada com base nos pontos de função ajustados, levando em consideração fatores de normalização.\n",
    "\n",
    "14. **NPDU_UFP**: \"Normalized Productivity Delivery Rate – Unadjusted Function Points\" – Taxa de produtividade normalizada com pontos de função não ajustados.\n",
    "\n",
    "15. **Resource**: Número de recursos ou pessoas envolvidas no desenvolvimento ou manutenção do sistema.\n",
    "\n",
    "16. **Dev.Type**: Tipo de desenvolvimento, representado por um valor numérico. Isso pode indicar diferentes categorias, como desenvolvimento de novo software, manutenção ou aprimoramento.\n",
    "\n",
    "17. **Duration**: Duração do projeto em unidades de tempo, como semanas ou meses. Refere-se ao tempo total gasto na realização do desenvolvimento ou manutenção.\n",
    "\n",
    "18. **N_effort**: Esforço normalizado requerido para o projeto, representado em unidades de trabalho, como horas ou dias-homem.\n",
    "\n",
    "19. **Effort**: Esforço total gasto no projeto, em unidades de trabalho, sem normalização.\n",
    "\n",
    "### Descrição Geral:\n",
    "Este dataset é útil para estudos de estimativa de esforço em projetos de software, uma vez que oferece uma ampla gama de métricas que podem ser usadas para entender a complexidade do software e os fatores que influenciam o esforço necessário para desenvolvê-lo ou mantê-lo. As variáveis cobrem aspectos tanto do desenvolvimento quanto da manutenção, incluindo a contagem de pontos de função (uma métrica amplamente utilizada na engenharia de software) e a quantidade de esforço e recursos envolvidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cocomo81\n",
    "\n",
    "O dataset apresentado é uma versão em formato ARFF do conjunto de dados COCOMO81, que é utilizado para estimativas de esforço em projetos de software. Ele é baseado no modelo COCOMO (Constructive Cost Model), desenvolvido por Barry Boehm em 1981, e utilizado para prever o esforço, o tempo e o custo de desenvolvimento de projetos de software com base no tamanho estimado do projeto em KSLOC (mil linhas de código-fonte) e em vários fatores de esforço.\n",
    "\n",
    "Aqui está a descrição das variáveis presentes:\n",
    "\n",
    "1. **rely**: Confiabilidade requerida do software. Este fator mede a importância da confiabilidade do software e quanto ela impacta no esforço necessário. \n",
    "\n",
    "2. **data**: Tamanho do banco de dados. Refere-se ao tamanho dos dados a serem manipulados pelo sistema, o que pode influenciar diretamente o esforço de desenvolvimento.\n",
    "\n",
    "3. **cplx**: Complexidade do processo. Representa a complexidade das funções e dos processos que o sistema precisa realizar. Um sistema com processos mais complexos tende a exigir mais esforço.\n",
    "\n",
    "4. **time**: Restrição de tempo da CPU. Indica o grau de exigência quanto ao uso da CPU. Uma maior restrição de tempo geralmente aumenta o esforço necessário.\n",
    "\n",
    "5. **stor**: Restrição de memória principal. Representa limitações na memória principal disponível, o que pode aumentar a complexidade do desenvolvimento e, consequentemente, o esforço necessário.\n",
    "\n",
    "6. **virt**: Volatilidade da máquina. Mede a frequência de mudanças no ambiente de hardware em que o software será executado, que pode exigir ajustes no desenvolvimento e aumentar o esforço.\n",
    "\n",
    "7. **turn**: Tempo de resposta. Refere-se ao tempo que o sistema deve demorar para responder. Exigências de menor tempo de resposta aumentam o esforço necessário para desenvolver o sistema.\n",
    "\n",
    "8. **acap**: Capacidade dos analistas. Representa o nível de competência dos analistas de sistemas envolvidos no projeto. Analistas mais capacitados podem reduzir o esforço necessário.\n",
    "\n",
    "9. **aexp**: Experiência com a aplicação. Indica o grau de experiência dos desenvolvedores com o tipo de aplicação sendo desenvolvido. Maior experiência com a aplicação tende a reduzir o esforço.\n",
    "\n",
    "10. **pcap**: Capacidade dos programadores. Representa a habilidade técnica dos programadores, onde uma maior capacidade técnica pode diminuir o esforço.\n",
    "\n",
    "11. **vexp**: Experiência com máquinas virtuais. Mede a experiência da equipe com o uso de máquinas virtuais, o que pode facilitar o desenvolvimento e reduzir o esforço.\n",
    "\n",
    "12. **lexp**: Experiência com linguagens. Indica o nível de familiaridade da equipe com a linguagem de programação utilizada. Uma equipe experiente na linguagem utilizada tende a exigir menos esforço.\n",
    "\n",
    "13. **modp**: Práticas de programação modernas. Refere-se ao uso de práticas de programação modernas, como metodologias ágeis e outras técnicas que podem otimizar o desenvolvimento.\n",
    "\n",
    "14. **tool**: Uso de ferramentas de software. Mede o grau de utilização de ferramentas de suporte ao desenvolvimento, como IDEs, sistemas de controle de versão e ferramentas de automação, que podem diminuir o esforço necessário.\n",
    "\n",
    "15. **sced**: Restrições de cronograma. Representa o grau de restrição temporal do projeto, onde prazos mais apertados podem aumentar o esforço de desenvolvimento.\n",
    "\n",
    "16. **loc**: Linhas de Código (LOC). Representa o tamanho do sistema em termos de quantidade de linhas de código. Geralmente, quanto maior o sistema, maior o esforço necessário.\n",
    "\n",
    "17. **actual**: Esforço real (em meses-homem). É a variável alvo, que representa o esforço total necessário para desenvolver o software, levando em conta todas as variáveis e fatores acima. \n",
    "\n",
    "O modelo COCOMO usa essas variáveis para calcular o esforço total necessário para o desenvolvimento de software, considerando tanto o tamanho do software (loc) quanto as variáveis que influenciam direta ou indiretamente o esforço (como a capacidade da equipe, a complexidade do software, e as restrições técnicas e de prazo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desharnais\n",
    "\n",
    "\n",
    "O dataset apresentado é utilizado para estimativa de esforço em projetos de desenvolvimento de software, com informações sobre características do projeto e do time de desenvolvimento. Aqui está uma descrição detalhada das variáveis:\n",
    "\n",
    "1. **TeamExp**: Experiência da equipe, medida em uma escala numérica, representando o nível de experiência do time de desenvolvimento. Valores mais altos indicam uma equipe com mais experiência, o que pode impactar positivamente no tempo e na qualidade do desenvolvimento.\n",
    "\n",
    "2. **ManagerExp**: Experiência do gerente, também em uma escala numérica. Reflete o nível de experiência do gerente de projeto, o que pode influenciar o gerenciamento do projeto e a eficiência da equipe.\n",
    "\n",
    "3. **YearEnd**: Ano de finalização do projeto. Indica o ano em que o projeto foi concluído, útil para identificar tendências temporais ou mudanças nas práticas de desenvolvimento ao longo dos anos.\n",
    "\n",
    "4. **Transactions**: Número de transações do sistema. Representa a quantidade de transações ou operações principais que o sistema precisa realizar. Mais transações geralmente indicam uma maior complexidade do sistema.\n",
    "\n",
    "5. **Entities**: Número de entidades do sistema. Refere-se à quantidade de entidades (ou elementos de dados principais) que o sistema precisa gerenciar, podendo influenciar a complexidade e o esforço necessário para o desenvolvimento.\n",
    "\n",
    "6. **PointsAdjust**: Pontos ajustados. Este valor ajusta os pontos de função, levando em consideração características específicas do sistema. Pontos de função ajustados são uma métrica comum para medir o tamanho funcional do software.\n",
    "\n",
    "7. **Envergure**: Tamanho ou abrangência do sistema. Representa uma métrica geral do tamanho do projeto, que pode influenciar diretamente o esforço necessário para o desenvolvimento.\n",
    "\n",
    "8. **Language**: Linguagem de programação utilizada. Este atributo é categórico, com três valores possíveis (1, 2 ou 3), representando diferentes linguagens. Diferentes linguagens de programação podem impactar a produtividade da equipe e o esforço necessário.\n",
    "\n",
    "9. **Effort**: Esforço total do projeto, medido em horas. Este é o valor alvo, representando o esforço real necessário para completar o projeto. É utilizado como referência para prever o esforço em projetos futuros com características semelhantes.\n",
    "\n",
    "Esse dataset é útil para desenvolver modelos de aprendizado de máquina que busquem prever o esforço (Effort) de desenvolvimento com base nas variáveis de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISBSG\n",
    "\n",
    "O dataset é uma coleção de projetos de desenvolvimento de software, com informações que podem ser usadas para modelar e prever o esforço necessário para projetos futuros. A seguir, estão descritas todas as variáveis presentes no dataset:\n",
    "\n",
    "1. **FunctionalSize**: Tamanho funcional do software, medido em pontos de função ou alguma métrica equivalente. Reflete a complexidade funcional do sistema, que pode afetar o esforço necessário.\n",
    "\n",
    "2. **ValueAdjustmentFactor**: Fator de ajuste de valor, um coeficiente usado para ajustar o tamanho funcional do projeto com base em fatores específicos do ambiente ou do projeto, como complexidade técnica ou requisitos não funcionais.\n",
    "\n",
    "3. **ProjectElapsedTime**: Tempo decorrido do projeto, medido em algum período (meses, anos, etc.). Refere-se à duração total do projeto, desde o início até a conclusão.\n",
    "\n",
    "4. **DevelopmentType**: Tipo de desenvolvimento, com valores categóricos como \"Enhancement\" (melhoria) ou \"New_Development\" (novo desenvolvimento). Indica se o projeto é uma melhoria de um sistema existente ou o desenvolvimento de um novo sistema.\n",
    "\n",
    "5. **BusinessAreaType**: Área de negócios em que o software será utilizado, como \"Banking\" (bancos) ou \"Telecommunications\" (telecomunicações). Pode influenciar os requisitos do projeto e a complexidade.\n",
    "\n",
    "6. **ClientServer**: Um valor categórico indicando se o projeto segue uma arquitetura cliente-servidor (\"Yes\" ou \"No\"). A arquitetura pode ter um impacto no esforço de desenvolvimento.\n",
    "\n",
    "7. **DevelopmentPlatform**: Plataforma de desenvolvimento, com valores como \"MF\" (mainframe), \"PC\" (computador pessoal), \"Multi\" (plataforma múltipla) ou \"MR\" (sistema misto). Indica a tecnologia ou plataforma em que o software está sendo desenvolvido.\n",
    "\n",
    "8. **LanguageType**: Tipo de linguagem de programação usada no projeto, com valores como \"3GL\" (terceira geração de linguagens) ou \"4GL\" (quarta geração de linguagens). A escolha da linguagem pode afetar a produtividade e o esforço necessário.\n",
    "\n",
    "9. **FirstOS**: Primeiro sistema operacional em que o software será executado, com valores como \"Unix\", \"Mainframe\" ou \"Windows\". O sistema operacional pode influenciar as decisões de design e o esforço de desenvolvimento.\n",
    "\n",
    "10. **MaxTeamSize**: Tamanho máximo da equipe de desenvolvimento durante o projeto, medido em número de membros. Tamanhos de equipe maiores podem indicar projetos mais complexos ou a necessidade de mais coordenação.\n",
    "\n",
    "11. **NormalisedWorkEffortLevel1**: Esforço de trabalho normalizado em algum nível (nível 1), medido em horas, dias, ou outro período de tempo. Representa o esforço necessário para concluir o projeto, ajustado para fatores de normalização.\n",
    "\n",
    "Esse dataset é útil para análise de projetos de software, ajudando na previsão de esforço e planejamento de recursos com base em características específicas do projeto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxwell\n",
    "\n",
    "O dataset é uma coleção de projetos de desenvolvimento de software, contendo diversas características técnicas e organizacionais que influenciam o esforço necessário para concluir cada projeto. Aqui estão as descrições das variáveis:\n",
    "\n",
    "1. **size_D**: Tamanho do software, medido em unidades numéricas, como linhas de código ou pontos de função. Reflete a complexidade e a escala do projeto.\n",
    "\n",
    "2. **duration_D**: Duração do projeto, em algum período (meses ou anos). Indica quanto tempo levou para completar o projeto.\n",
    "\n",
    "3. **app**: Tipo de aplicação. Os valores possíveis incluem:\n",
    "   - **InfServ**: Serviços de informação\n",
    "   - **TransPro**: Processamento de transações\n",
    "   - **CustServ**: Serviços ao cliente\n",
    "   - **ProdCont**: Controle de produção\n",
    "   - **MIS**: Sistemas de informação gerencial\n",
    "\n",
    "4. **har**: Tipo de hardware utilizado no projeto. Os valores possíveis incluem:\n",
    "   - **PC**: Computador pessoal\n",
    "   - **Mainfrm**: Mainframe\n",
    "   - **Multi**: Múltiplas plataformas\n",
    "   - **Mini**: Computador de médio porte\n",
    "   - **Network**: Rede\n",
    "\n",
    "5. **dba**: Tipo de banco de dados usado. Os valores possíveis incluem:\n",
    "   - **Relatnl**: Banco de dados relacional\n",
    "   - **Sequential**: Banco de dados sequencial\n",
    "   - **None**: Sem banco de dados\n",
    "   - **Other**: Outros tipos de banco de dados\n",
    "\n",
    "6. **ifc**: Tipo de interface de usuário. Os valores possíveis são:\n",
    "   - **GUI**: Interface gráfica de usuário\n",
    "   - **TextUI**: Interface de texto\n",
    "\n",
    "7. **source**: Origem do desenvolvimento. Os valores possíveis incluem:\n",
    "   - **Outsrced**: Projeto terceirizado\n",
    "   - **Inhouse**: Desenvolvido internamente\n",
    "\n",
    "8. **nlan**: Número de linguagens de programação usadas no projeto. Os valores possíveis são: 1, 2, 3 ou 4.\n",
    "\n",
    "9. **telonuse**: Uso do Telon (uma ferramenta de automação de software). Os valores possíveis são:\n",
    "   - **No**: Não utilizado\n",
    "   - **Yes**: Utilizado\n",
    "\n",
    "10-24. **t01** a **t15**: Fatores técnicos ou de complexidade do projeto, com valores entre 1 e 5. Esses atributos avaliam a complexidade técnica e outros aspectos qualitativos que podem influenciar o esforço de desenvolvimento.\n",
    "\n",
    "25. **effort_D**: Esforço necessário para completar o projeto, medido em unidades numéricas (como horas de trabalho ou dias-homem). Este é o valor de saída que geralmente se deseja prever.\n",
    "\n",
    "O dataset pode ser usado para modelar o esforço necessário para diferentes projetos de software com base em características específicas, facilitando a gestão e o planejamento de recursos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteúdo processado de isbsg.txt:\n",
      "   FunctionalSize  ValueAdjustmentFactor  ProjectElapsedTime  DevelopmentType  \\\n",
      "0               3               1.090000            0.000000      Enhancement   \n",
      "1             620               1.037937            7.000000  New_Development   \n",
      "2             730               1.140000            6.042686      Enhancement   \n",
      "3             114               1.180000            6.042686      Enhancement   \n",
      "4             460               1.000000            4.000000      Enhancement   \n",
      "\n",
      "  BusinessAreaType ClientServer DevelopmentPlatform LanguageType FirstOS  \\\n",
      "0              NaN          Yes                 NaN          NaN     NaN   \n",
      "1              NaN          Yes                  MF          3GL    Unix   \n",
      "2              NaN          Yes                  MF          3GL    Unix   \n",
      "3              NaN          Yes                  MF          3GL    Unix   \n",
      "4              NaN          Yes                  MF          3GL    Unix   \n",
      "\n",
      "   MaxTeamSize  NormalisedWorkEffortLevel1  \n",
      "0     8.448507                          28  \n",
      "1     8.448507                       18160  \n",
      "2     8.448507                       20975  \n",
      "3     8.448507                        7290  \n",
      "4     5.000000                        2253  \n",
      "\n",
      "==================================================\n",
      "\n",
      "Conteúdo processado de desharnais.txt:\n",
      "   TeamExp  ManagerExp  YearEnd  Transactions  Entities  PointsAdjust  \\\n",
      "0      1.0         4.0       85           253        52           305   \n",
      "1      0.0         0.0       86           197       124           321   \n",
      "2      4.0         4.0       85            40        60           100   \n",
      "3      0.0         0.0       86           200       119           319   \n",
      "4      0.0         0.0       86           140        94           234   \n",
      "\n",
      "   Envergure  Language  Effort  \n",
      "0         34         1    5152  \n",
      "1         33         1    5635  \n",
      "2         18         1     805  \n",
      "3         30         1    3829  \n",
      "4         24         1    2149  \n",
      "\n",
      "==================================================\n",
      "\n",
      "Conteúdo processado de china.txt:\n",
      "   ID   AFP  Input  Output  Enquiry  File  Interface  Added  Changed  Deleted  \\\n",
      "0   1  1587    774     260      340   128          0   1502        0        0   \n",
      "1   2   260      9       4        3   193         41     51      138       61   \n",
      "2   3   152     25      33       28    42         35    163        0        0   \n",
      "3   4   252    151      28        8    39          0     69      153        4   \n",
      "4   5   292     93       0      194    20          0      0      307        0   \n",
      "\n",
      "   PDR_AFP  PDR_UFP  NPDR_AFP  NPDU_UFP  Resource  Dev.Type  Duration  \\\n",
      "0      4.7      5.0       4.7       5.0         4         0       4.0   \n",
      "1     16.0     16.6      16.0      16.6         2         0      17.0   \n",
      "2      4.4      4.1       4.4       4.1         1         0       9.0   \n",
      "3     12.8     14.3      15.5      17.3         1         0       4.0   \n",
      "4     10.3      9.8      12.4      11.7         1         0      13.0   \n",
      "\n",
      "   N_effort  Effort  \n",
      "0      7490    7490  \n",
      "1      4150    4150  \n",
      "2       668     668  \n",
      "3      3901    3238  \n",
      "4      3607    2994  \n",
      "\n",
      "==================================================\n",
      "\n",
      "Conteúdo processado de cocomo81.txt:\n",
      "   rely  data  cplx  time  stor  virt  turn  acap  aexp  pcap  vexp  lexp  \\\n",
      "0  0.88  1.16  0.70   1.0  1.06  1.15  1.07  1.19  1.13  1.17   1.1  1.00   \n",
      "1  0.88  1.16  0.85   1.0  1.06  1.00  1.07  1.00  0.91  1.00   0.9  0.95   \n",
      "2  1.00  1.16  0.85   1.0  1.00  0.87  0.94  0.86  0.82  0.86   0.9  0.95   \n",
      "3  0.75  1.16  0.70   1.0  1.00  0.87  1.00  1.19  0.91  1.42   1.0  0.95   \n",
      "4  0.88  0.94  1.00   1.0  1.00  0.87  1.00  1.00  1.00  0.86   0.9  0.95   \n",
      "\n",
      "   modp  tool  sced    loc  actual  \n",
      "0  1.24  1.10  1.04  113.0  2040.0  \n",
      "1  1.10  1.00  1.00  293.0  1600.0  \n",
      "2  0.91  0.91  1.00  132.0   243.0  \n",
      "3  1.24  1.00  1.04   60.0   240.0  \n",
      "4  1.24  1.00  1.00   16.0    33.0  \n",
      "\n",
      "==================================================\n",
      "\n",
      "Conteúdo processado de maxwell.txt:\n",
      "   size_D  duration_D       app      har      dba     ifc    source  nlan  \\\n",
      "0     562          14   InfServ       PC  Relatnl     GUI  Outsrced     1   \n",
      "1     647          16  TransPro  Mainfrm  Relatnl  TextUI   Inhouse     3   \n",
      "2     130           5  TransPro  Mainfrm  Relatnl  TextUI   Inhouse     3   \n",
      "3     254           8   InfServ  Mainfrm  Relatnl  TextUI   Inhouse     2   \n",
      "4    1056          16  CustServ  Mainfrm  Relatnl  TextUI   Inhouse     3   \n",
      "\n",
      "  telonuse  t01  ...  t07  t08  t09  t10  t11  t12  t13  t14  t15  effort_D  \n",
      "0       No    5  ...    4    4    3    3    2    4    3    5    3      1062  \n",
      "1       No    4  ...    4    5    4    5    4    4    4    4    5      7871  \n",
      "2       No    2  ...    2    2    4    3    4    4    4    4    4       845  \n",
      "3       No    3  ...    2    3    4    5    4    3    2    3    3      2330  \n",
      "4       No    2  ...    3    5    4    4    5    4    3    2    3     21272  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Quantidade de arquivos processados: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_809473/3773196945.py:24: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[column].fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_809473/3773196945.py:24: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[column].fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_809473/3773196945.py:24: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[column].fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_809473/3773196945.py:24: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[column].fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_809473/3773196945.py:24: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[column].fillna(method='ffill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def handle_missing_values_in_txt(directory_path):\n",
    "    processed_data = {}\n",
    "\n",
    "    # Diretório para salvar os arquivos processados\n",
    "    processed_output_directory = os.path.join(directory_path, '1_handle_missing_values/')\n",
    "    if not os.path.exists(processed_output_directory):\n",
    "        os.makedirs(processed_output_directory)\n",
    "\n",
    "    # Itera sobre todos os arquivos no diretório\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "            # Lê o arquivo TXT em um DataFrame, tratando \"?\" como valores faltantes\n",
    "            df = pd.read_csv(file_path, delimiter=',', skipinitialspace=True, na_values=['?'])\n",
    "\n",
    "            # Identifica e trata valores faltantes\n",
    "            for column in df.columns:\n",
    "                if df[column].dtype == 'object' or df[column].nunique() < 10:  # Assume valores categóricos\n",
    "                    # Forward fill para colunas categóricas\n",
    "                    df[column].fillna(method='ffill', inplace=True)\n",
    "                else:\n",
    "                    # Média para colunas numéricas\n",
    "                    df[column].fillna(df[column].mean(), inplace=True)\n",
    "\n",
    "            # Salva o DataFrame processado em um novo arquivo TXT\n",
    "            processed_file_path = os.path.join(processed_output_directory, f\"processed_{filename}\")\n",
    "            df.to_csv(processed_file_path, index=False, sep=',')\n",
    "\n",
    "            # Adiciona os dados processados ao dicionário\n",
    "            processed_data[filename] = df\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "# Caminho para o diretório com os arquivos TXT\n",
    "directory_path = 'output_pre_processamento'\n",
    "processed_txt_data = handle_missing_values_in_txt(directory_path)\n",
    "\n",
    "# Exibir os dados processados\n",
    "cont = len(processed_txt_data)\n",
    "for filename, df in processed_txt_data.items():\n",
    "    print(f\"Conteúdo processado de {filename}:\")\n",
    "    print(df.head())\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(f\"Quantidade de arquivos processados: {cont}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforma valores categóricos em numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos transformados foram salvos em: output_pre_processamento/2_transform_string_numeric\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def load_cleaned_txt_files(directory_path):\n",
    "    \"\"\"\n",
    "    Carrega arquivos TXT limpos e organiza os dados em um dicionário.\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Caminho para o diretório com os arquivos TXT.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Um dicionário com os dados carregados, onde as chaves são os nomes dos arquivos \n",
    "              e os valores são listas de linhas de dados.\n",
    "    \"\"\"\n",
    "    cleaned_data = {}\n",
    "    \n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "                cleaned_data[filename] = [line.strip() for line in lines if line.strip()]\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "def transform_textual_to_numeric(cleaned_arff_data):\n",
    "    \"\"\"\n",
    "    Transforma valores textuais em valores numéricos nos dados, mantendo o cabeçalho.\n",
    "    \n",
    "    Args:\n",
    "        cleaned_arff_data (dict): Um dicionário contendo os dados limpos, onde as chaves são nomes \n",
    "                                  dos arquivos e os valores são listas de linhas de dados.\n",
    "                                  \n",
    "    Returns:\n",
    "        dict: Um novo dicionário com valores textuais transformados em numéricos.\n",
    "    \"\"\"\n",
    "    transformed_data = {}\n",
    "    \n",
    "    for filename, data in cleaned_arff_data.items():\n",
    "        # Divide cada linha de dados em valores separados por vírgulas\n",
    "        split_data = [line.split(\",\") for line in data[1:]]  # Ignorar a primeira linha (cabeçalho)\n",
    "        \n",
    "        # Armazena o cabeçalho separadamente\n",
    "        header = data[0]\n",
    "        \n",
    "        # Transforma os valores textuais em numéricos usando LabelEncoder\n",
    "        label_encoders = [LabelEncoder() for _ in range(len(split_data[0]))]\n",
    "        \n",
    "        for i in range(len(split_data[0])):\n",
    "            column_values = [row[i] for row in split_data]\n",
    "            \n",
    "            # Transforma valores categóricos em numéricos\n",
    "            if any(not value.replace('.', '', 1).isdigit() for value in column_values):\n",
    "                column_values = label_encoders[i].fit_transform(column_values)\n",
    "                for j, value in enumerate(column_values):\n",
    "                    split_data[j][i] = str(value)\n",
    "        \n",
    "        # Junta os valores novamente em linhas de texto\n",
    "        transformed_data[filename] = [header] + [\",\".join(row) for row in split_data]\n",
    "    \n",
    "    return transformed_data\n",
    "\n",
    "def save_transformed_data_with_headers(transformed_data, output_directory):\n",
    "    \"\"\"\n",
    "    Salva os dados transformados com cabeçalho em arquivos TXT.\n",
    "    \n",
    "    Args:\n",
    "        transformed_data (dict): Dados transformados para salvar.\n",
    "        output_directory (str): Caminho para o diretório de saída.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    for filename, data in transformed_data.items():\n",
    "        file_path = os.path.join(output_directory, filename)\n",
    "        with open(file_path, 'w') as file:\n",
    "            for line in data:\n",
    "                file.write(line + \"\\n\")\n",
    "\n",
    "# Caminho para o diretório com os arquivos TXT limpos\n",
    "directory_path = 'output_pre_processamento/1_handle_missing_values'\n",
    "cleaned_txt_data = load_cleaned_txt_files(directory_path)\n",
    "\n",
    "# Aplicar a transformação de valores textuais para numéricos\n",
    "numeric_arff_data = transform_textual_to_numeric(cleaned_txt_data)\n",
    "\n",
    "# Caminho para o diretório de saída\n",
    "output_directory = 'output_pre_processamento/2_transform_string_numeric'\n",
    "\n",
    "# Salvar os dados transformados com os cabeçalhos\n",
    "save_transformed_data_with_headers(numeric_arff_data, output_directory)\n",
    "\n",
    "# Mensagem de sucesso\n",
    "print(f\"Arquivos transformados foram salvos em: {output_directory}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padroniza valores da coluna EFFORT para pessoas-horas (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo salvo: output_txt_files/3_padroniza_coluna_effort/cleaned_cocomo81.txt\n",
      "Arquivo salvo: output_txt_files/3_padroniza_coluna_effort/cleaned_desharnais.txt\n",
      "Arquivo salvo: output_txt_files/3_padroniza_coluna_effort/cleaned_isbsg.txt\n",
      "Arquivo salvo: output_txt_files/3_padroniza_coluna_effort/cleaned_maxwell.txt\n",
      "Arquivo salvo: output_txt_files/3_padroniza_coluna_effort/cleaned_china.txt\n",
      "Quantidade de arquivos padronizados: 5\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def load_cnumeric_txt_files(directory_path):\n",
    "    cleaned_data = {}\n",
    "    \n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "                # Mantém o cabeçalho separado dos dados\n",
    "                header = lines[0].strip()\n",
    "                data = [line.strip() for line in lines[1:] if line.strip()]\n",
    "                cleaned_data[filename] = (header, data)\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "def standardize_effort_to_hours(cleaned_txt_data, effort_column_index, conversion_factor=160):\n",
    "    standardized_data = {}\n",
    "    \n",
    "    for filename, (header, data) in cleaned_txt_data.items():\n",
    "        standardized_lines = [header]  # Adiciona o cabeçalho\n",
    "        \n",
    "        for line in data:\n",
    "            values = line.split(\",\")\n",
    "            effort_value_str = values[effort_column_index]\n",
    "\n",
    "            try:\n",
    "                effort_value = float(effort_value_str)\n",
    "                \n",
    "                # Se o esforço for dado em pessoas-mês, converte para pessoas-horas\n",
    "                if \"mes\" in filename.lower():  # Exemplo de como identificar arquivos em pessoas-mês\n",
    "                    effort_value *= conversion_factor\n",
    "                \n",
    "                values[effort_column_index] = str(effort_value)\n",
    "                standardized_lines.append(\",\".join(values))\n",
    "            except ValueError:\n",
    "                print(f\"Valor não numérico encontrado: {effort_value_str} em {filename}. Ignorando essa linha.\")\n",
    "                continue  # Ignora a linha com valor inválido\n",
    "        \n",
    "        standardized_data[filename] = standardized_lines\n",
    "    \n",
    "    return standardized_data\n",
    "\n",
    "def save_standardized_data(standardized_data, output_directory):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    for filename, data in standardized_data.items():\n",
    "        output_path = os.path.join(output_directory, filename)\n",
    "        with open(output_path, 'w') as file:\n",
    "            for line in data:\n",
    "                file.write(line + '\\n')\n",
    "        print(f\"Arquivo salvo: {output_path}\")\n",
    "\n",
    "# Caminho para o diretório com os arquivos TXT limpos\n",
    "directory_path = 'output_txt_files/2_transforma_string_booleano'\n",
    "cleaned_txt_data = load_cnumeric_txt_files(directory_path)\n",
    "\n",
    "# Padronizar a coluna EFFORT para pessoas-horas\n",
    "effort_column_index = -1  # Suponha que a coluna EFFORT seja a última\n",
    "standardized_txt_data = standardize_effort_to_hours(cleaned_txt_data, effort_column_index)\n",
    "\n",
    "# Salvar os dados padronizados\n",
    "output_directory = 'output_txt_files/3_padroniza_coluna_effort'\n",
    "save_standardized_data(standardized_txt_data, output_directory)\n",
    "\n",
    "print(f\"Quantidade de arquivos padronizados: {len(standardized_txt_data)}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação de novas variáveis com GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando diretório de saída: output_pre_processamento/3_gan_variaveis...\n",
      "Diretório de saída pronto.\n",
      "\n",
      "Processando arquivo: output_pre_processamento/2_transform_string_numeric/processed_desharnais.txt\n",
      "Lendo arquivo de entrada: output_pre_processamento/2_transform_string_numeric/processed_desharnais.txt...\n",
      "Arquivo carregado com sucesso.\n",
      "Convertendo os dados para tensores do PyTorch...\n",
      "A base de dados contém 9 variáveis de entrada.\n",
      "Inicializando modelos Generator e Discriminator...\n",
      "Iniciando o treinamento da GAN...\n",
      "Época [10/100] - Perda Discriminador: 100.0000, Perda Gerador: 0.0000\n",
      "Época [20/100] - Perda Discriminador: 100.0000, Perda Gerador: 0.0000\n",
      "Época [30/100] - Perda Discriminador: 100.0000, Perda Gerador: 0.0000\n",
      "Época [40/100] - Perda Discriminador: 100.0000, Perda Gerador: 0.0000\n",
      "Época [50/100] - Perda Discriminador: 100.0000, Perda Gerador: 0.0000\n",
      "Época [60/100] - Perda Discriminador: 100.0000, Perda Gerador: 0.0000\n",
      "Época [70/100] - Perda Discriminador: 100.0000, Perda Gerador: 0.0000\n",
      "Época [80/100] - Perda Discriminador: 100.0000, Perda Gerador: 0.0000\n",
      "Época [90/100] - Perda Discriminador: 100.0000, Perda Gerador: 0.0000\n",
      "Época [100/100] - Perda Discriminador: 100.0000, Perda Gerador: 0.0000\n",
      "Treinamento concluído. Gerando novas variáveis...\n",
      "Novas variáveis adicionadas e colunas reorganizadas com sucesso.\n",
      "Arquivo processado e salvo em: output_pre_processamento/3_gan_variaveis/processed_desharnais.txt\n",
      "\n",
      "Processando arquivo: output_pre_processamento/2_transform_string_numeric/processed_cocomo81.txt\n",
      "Lendo arquivo de entrada: output_pre_processamento/2_transform_string_numeric/processed_cocomo81.txt...\n",
      "Arquivo carregado com sucesso.\n",
      "Convertendo os dados para tensores do PyTorch...\n",
      "A base de dados contém 17 variáveis de entrada.\n",
      "Inicializando modelos Generator e Discriminator...\n",
      "Iniciando o treinamento da GAN...\n",
      "Época [10/100] - Perda Discriminador: 2.6371, Perda Gerador: 3.0000\n",
      "Época [20/100] - Perda Discriminador: 2.4525, Perda Gerador: 1.5887\n",
      "Época [30/100] - Perda Discriminador: 1.7064, Perda Gerador: 1.6563\n",
      "Época [40/100] - Perda Discriminador: 1.3391, Perda Gerador: 2.1831\n",
      "Época [50/100] - Perda Discriminador: 3.0042, Perda Gerador: 0.9746\n",
      "Época [60/100] - Perda Discriminador: 0.9720, Perda Gerador: 2.2670\n",
      "Época [70/100] - Perda Discriminador: 8.3533, Perda Gerador: 0.5360\n",
      "Época [80/100] - Perda Discriminador: 3.2284, Perda Gerador: 4.1305\n",
      "Época [90/100] - Perda Discriminador: 2.2816, Perda Gerador: 0.5484\n",
      "Época [100/100] - Perda Discriminador: 8.8128, Perda Gerador: 0.3826\n",
      "Treinamento concluído. Gerando novas variáveis...\n",
      "Novas variáveis adicionadas e colunas reorganizadas com sucesso.\n",
      "Arquivo processado e salvo em: output_pre_processamento/3_gan_variaveis/processed_cocomo81.txt\n",
      "\n",
      "Processando arquivo: output_pre_processamento/2_transform_string_numeric/processed_isbsg.txt\n",
      "Lendo arquivo de entrada: output_pre_processamento/2_transform_string_numeric/processed_isbsg.txt...\n",
      "Arquivo carregado com sucesso.\n",
      "Convertendo os dados para tensores do PyTorch...\n",
      "A base de dados contém 11 variáveis de entrada.\n",
      "Inicializando modelos Generator e Discriminator...\n",
      "Iniciando o treinamento da GAN...\n",
      "Época [10/100] - Perda Discriminador: 61.2092, Perda Gerador: 0.0425\n",
      "Época [20/100] - Perda Discriminador: 23.9282, Perda Gerador: 0.2948\n",
      "Época [30/100] - Perda Discriminador: 15.6011, Perda Gerador: 0.2137\n",
      "Época [40/100] - Perda Discriminador: 1.4079, Perda Gerador: 0.3730\n",
      "Época [50/100] - Perda Discriminador: 1.5016, Perda Gerador: 1.2063\n",
      "Época [60/100] - Perda Discriminador: 2.8369, Perda Gerador: 1.6496\n",
      "Época [70/100] - Perda Discriminador: 3.8213, Perda Gerador: 1.4648\n",
      "Época [80/100] - Perda Discriminador: 2.7282, Perda Gerador: 0.3358\n",
      "Época [90/100] - Perda Discriminador: 1.5397, Perda Gerador: 0.4036\n",
      "Época [100/100] - Perda Discriminador: 1.7286, Perda Gerador: 0.5227\n",
      "Treinamento concluído. Gerando novas variáveis...\n",
      "Novas variáveis adicionadas e colunas reorganizadas com sucesso.\n",
      "Arquivo processado e salvo em: output_pre_processamento/3_gan_variaveis/processed_isbsg.txt\n",
      "\n",
      "Processando arquivo: output_pre_processamento/2_transform_string_numeric/processed_maxwell.txt\n",
      "Lendo arquivo de entrada: output_pre_processamento/2_transform_string_numeric/processed_maxwell.txt...\n",
      "Arquivo carregado com sucesso.\n",
      "Convertendo os dados para tensores do PyTorch...\n",
      "A base de dados contém 25 variáveis de entrada.\n",
      "Inicializando modelos Generator e Discriminator...\n",
      "Iniciando o treinamento da GAN...\n",
      "Época [10/100] - Perda Discriminador: 100.0000, Perda Gerador: 0.0000\n",
      "Época [20/100] - Perda Discriminador: 100.0000, Perda Gerador: 0.0000\n",
      "Época [30/100] - Perda Discriminador: 100.0000, Perda Gerador: 0.0000\n",
      "Época [40/100] - Perda Discriminador: 100.0000, Perda Gerador: 0.0000\n",
      "Época [50/100] - Perda Discriminador: 100.0000, Perda Gerador: 0.0000\n",
      "Época [60/100] - Perda Discriminador: 100.0000, Perda Gerador: 0.0000\n",
      "Época [70/100] - Perda Discriminador: 100.0000, Perda Gerador: 0.0000\n",
      "Época [80/100] - Perda Discriminador: 100.0000, Perda Gerador: 0.0000\n",
      "Época [90/100] - Perda Discriminador: 100.0000, Perda Gerador: 0.0000\n",
      "Época [100/100] - Perda Discriminador: 100.0000, Perda Gerador: 0.0000\n",
      "Treinamento concluído. Gerando novas variáveis...\n",
      "Novas variáveis adicionadas e colunas reorganizadas com sucesso.\n",
      "Arquivo processado e salvo em: output_pre_processamento/3_gan_variaveis/processed_maxwell.txt\n",
      "\n",
      "Processando arquivo: output_pre_processamento/2_transform_string_numeric/processed_china.txt\n",
      "Lendo arquivo de entrada: output_pre_processamento/2_transform_string_numeric/processed_china.txt...\n",
      "Arquivo carregado com sucesso.\n",
      "Convertendo os dados para tensores do PyTorch...\n",
      "A base de dados contém 19 variáveis de entrada.\n",
      "Inicializando modelos Generator e Discriminator...\n",
      "Iniciando o treinamento da GAN...\n",
      "Época [10/100] - Perda Discriminador: 5.4896, Perda Gerador: 0.9917\n",
      "Época [20/100] - Perda Discriminador: 19.7214, Perda Gerador: 1.0709\n",
      "Época [30/100] - Perda Discriminador: 51.7443, Perda Gerador: 0.1579\n",
      "Época [40/100] - Perda Discriminador: 13.9915, Perda Gerador: 2.1255\n",
      "Época [50/100] - Perda Discriminador: 13.0824, Perda Gerador: 21.3843\n",
      "Época [60/100] - Perda Discriminador: 45.8624, Perda Gerador: 0.1131\n",
      "Época [70/100] - Perda Discriminador: 10.3992, Perda Gerador: 0.3437\n",
      "Época [80/100] - Perda Discriminador: 2.2779, Perda Gerador: 1.1530\n",
      "Época [90/100] - Perda Discriminador: 13.4392, Perda Gerador: 0.2034\n",
      "Época [100/100] - Perda Discriminador: 2.0361, Perda Gerador: 0.3851\n",
      "Treinamento concluído. Gerando novas variáveis...\n",
      "Novas variáveis adicionadas e colunas reorganizadas com sucesso.\n",
      "Arquivo processado e salvo em: output_pre_processamento/3_gan_variaveis/processed_china.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def apply_gan_to_generate_variables(file_path, num_generated_features=2, num_epochs=100, batch_size=32, learning_rate=0.001):\n",
    "    print(f\"Lendo arquivo de entrada: {file_path}...\")\n",
    "    data = pd.read_csv(file_path, delimiter=\",\", skipinitialspace=True)\n",
    "    print(\"Arquivo carregado com sucesso.\")\n",
    "\n",
    "    #print(\"Convertendo colunas categóricas ou não numéricas...\")\n",
    "    #for col in data.columns:\n",
    "    #    if data[col].dtype == 'object' or not np.issubdtype(data[col].dtype, np.number):\n",
    "    #        data[col] = pd.factorize(data[col])[0]\n",
    "\n",
    "    #print(\"Preenchendo valores ausentes com 0...\")\n",
    "    #data = data.fillna(0)\n",
    "\n",
    "    #print(\"Normalizando os dados no intervalo [0, 1]...\")\n",
    "    #data = (data - data.min()) / (data.max() - data.min())\n",
    "\n",
    "    print(\"Convertendo os dados para tensores do PyTorch...\")\n",
    "    data_tensor = torch.tensor(data.values, dtype=torch.float32)\n",
    "\n",
    "    input_dim = data.shape[1]\n",
    "    print(f\"A base de dados contém {input_dim} variáveis de entrada.\")\n",
    "\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim):\n",
    "            super(Generator, self).__init__()\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(input_dim, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, output_dim)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "    \n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self, input_dim):\n",
    "            super(Discriminator, self).__init__()\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(input_dim, 64),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(64, 64),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(64, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "    \n",
    "    print(\"Inicializando modelos Generator e Discriminator...\")\n",
    "    generator = Generator(input_dim, num_generated_features)\n",
    "    discriminator = Discriminator(input_dim + num_generated_features)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "    dataset = TensorDataset(data_tensor)\n",
    "    data_loader = DataLoader(dataset, batch_size=min(batch_size, len(dataset)), shuffle=True)\n",
    "\n",
    "    print(\"Iniciando o treinamento da GAN...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in data_loader:\n",
    "            real_data = batch[0]\n",
    "            batch_size_actual = real_data.size(0)\n",
    "\n",
    "            # Labels reais e falsas\n",
    "            real_labels = torch.ones((batch_size_actual, 1))\n",
    "            fake_labels = torch.zeros((batch_size_actual, 1))\n",
    "\n",
    "            # Treinar o discriminador\n",
    "            generated_data = generator(real_data)\n",
    "            fake_data = torch.cat((real_data, generated_data), dim=1)\n",
    "            real_data_extended = torch.cat((real_data, torch.zeros((batch_size_actual, num_generated_features))), dim=1)\n",
    "            \n",
    "            discriminator_real_loss = criterion(discriminator(real_data_extended), real_labels)\n",
    "            discriminator_fake_loss = criterion(discriminator(fake_data.detach()), fake_labels)\n",
    "            discriminator_loss = discriminator_real_loss + discriminator_fake_loss\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "            discriminator_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Treinar o gerador\n",
    "            fake_data = torch.cat((real_data, generator(real_data)), dim=1)\n",
    "            generator_loss = criterion(discriminator(fake_data), real_labels)\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "            generator_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Época [{epoch+1}/{num_epochs}] - Perda Discriminador: {discriminator_loss.item():.4f}, Perda Gerador: {generator_loss.item():.4f}\")\n",
    "\n",
    "    print(\"Treinamento concluído. Gerando novas variáveis...\")\n",
    "    with torch.no_grad():\n",
    "        new_variables = generator(data_tensor).numpy()\n",
    "\n",
    "    for i in range(num_generated_features):\n",
    "        data[f\"Generated_Var_{i+1}\"] = new_variables[:, i]\n",
    "\n",
    "    # Reorganizar as colunas para garantir que a coluna alvo seja a última\n",
    "    target_column = data.columns[-(num_generated_features + 1)]  # Identifica a coluna alvo original\n",
    "    columns = [col for col in data.columns if col != target_column] + [target_column]\n",
    "    data = data[columns]\n",
    "\n",
    "    print(\"Novas variáveis adicionadas e colunas reorganizadas com sucesso.\")\n",
    "    return data\n",
    "\n",
    "def process_and_save_files(input_dir, output_dir, num_generated_features=2, num_epochs=100, batch_size=32, learning_rate=0.001):\n",
    "    print(f\"Verificando diretório de saída: {output_dir}...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(\"Diretório de saída pronto.\")\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "            \n",
    "            print(f\"\\nProcessando arquivo: {input_path}\")\n",
    "            try:\n",
    "                processed_data = apply_gan_to_generate_variables(\n",
    "                    file_path=input_path,\n",
    "                    num_generated_features=num_generated_features,\n",
    "                    num_epochs=num_epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    learning_rate=learning_rate\n",
    "                )\n",
    "                processed_data.to_csv(output_path, sep=\",\", index=False)\n",
    "                print(f\"Arquivo processado e salvo em: {output_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar {filename}: {e}\")\n",
    "\n",
    "# Diretórios de entrada e saída\n",
    "input_directory = \"output_pre_processamento/2_transform_string_numeric\"\n",
    "output_directory = \"output_pre_processamento/3_gan_variaveis\"\n",
    "\n",
    "# Processar arquivos\n",
    "process_and_save_files(input_directory, output_directory, num_generated_features=3, num_epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo salvo: output_pre_processamento/4_normalization/processed_desharnais.txt\n",
      "Arquivo salvo: output_pre_processamento/4_normalization/processed_cocomo81.txt\n",
      "Arquivo salvo: output_pre_processamento/4_normalization/processed_isbsg.txt\n",
      "Arquivo salvo: output_pre_processamento/4_normalization/processed_maxwell.txt\n",
      "Arquivo salvo: output_pre_processamento/4_normalization/processed_china.txt\n",
      "Quantidade de arquivos processados para normalização: 5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "def load_relevant_features_txt_files(directory_path):\n",
    "    relevant_features_data = {}\n",
    "    \n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "                header = lines[0].strip()  # Mantém o cabeçalho\n",
    "                data = [line.strip() for line in lines[1:] if line.strip()]\n",
    "                relevant_features_data[filename] = (header, data)\n",
    "    \n",
    "    return relevant_features_data\n",
    "\n",
    "def normalize_data_with_minmax(relevant_features_data):\n",
    "    normalized_data = {}\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    for filename, (header, data) in relevant_features_data.items():\n",
    "        # Divide os dados em matriz X\n",
    "        split_data = [line.split(\",\") for line in data]\n",
    "        X = np.array([list(map(float, row)) for row in split_data])\n",
    "        \n",
    "        # Normaliza os dados para o intervalo [0, 1]\n",
    "        X_normalized = scaler.fit_transform(X)\n",
    "        \n",
    "        # Converte os dados normalizados de volta para o formato de string\n",
    "        normalized_lines = [header]  # Adiciona o cabeçalho\n",
    "        normalized_lines += [\",\".join(map(str, row)) for row in X_normalized]\n",
    "        normalized_data[filename] = normalized_lines\n",
    "    \n",
    "    return normalized_data\n",
    "\n",
    "def save_normalized_data(normalized_data, output_directory):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    for filename, data in normalized_data.items():\n",
    "        output_path = os.path.join(output_directory, filename)\n",
    "        with open(output_path, 'w') as file:\n",
    "            for line in data:\n",
    "                file.write(line + '\\n')\n",
    "        print(f\"Arquivo salvo: {output_path}\")\n",
    "\n",
    "# Caminho para o diretório com os arquivos TXT com variáveis relevantes\n",
    "directory_path = 'output_pre_processamento/3_gan_variaveis'\n",
    "relevant_features_data = load_relevant_features_txt_files(directory_path)\n",
    "\n",
    "# Aplicar a normalização com MinMaxScaler\n",
    "normalized_data_minmax = normalize_data_with_minmax(relevant_features_data)\n",
    "\n",
    "# Salvar os dados normalizados\n",
    "output_directory = 'output_pre_processamento/4_normalization'\n",
    "save_normalized_data(normalized_data_minmax, output_directory)\n",
    "\n",
    "print(f\"Quantidade de arquivos processados para normalização: {len(normalized_data_minmax)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seleção das variáveis mais relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo salvo: output_pre_processamento/6_selecao_variaveis/cleaned_cocomo81.txt\n",
      "Arquivo salvo: output_pre_processamento/6_selecao_variaveis/cleaned_desharnais.txt\n",
      "Arquivo salvo: output_pre_processamento/6_selecao_variaveis/cleaned_isbsg.txt\n",
      "Arquivo salvo: output_pre_processamento/6_selecao_variaveis/cleaned_maxwell.txt\n",
      "Arquivo salvo: output_pre_processamento/6_selecao_variaveis/cleaned_china.txt\n",
      "Quantidade de arquivos processados para seleção de variáveis: 5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "def load_standardized_txt_files(directory_path):\n",
    "    standardized_data = {}\n",
    "    \n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "                header = lines[0].strip()  # Mantém o cabeçalho\n",
    "                data = [line.strip() for line in lines[1:] if line.strip()]\n",
    "                standardized_data[filename] = (header, data)\n",
    "    \n",
    "    return standardized_data\n",
    "\n",
    "def select_relevant_features(standardized_txt_data, target_column_index, k=5):\n",
    "    relevant_features_data = {}\n",
    "    \n",
    "    for filename, (header, data) in standardized_txt_data.items():\n",
    "        # Divide os dados em matriz X e vetor y\n",
    "        split_data = [line.split(\",\") for line in data]\n",
    "        X = np.array([list(map(float, row[:target_column_index])) for row in split_data])\n",
    "        y = np.array([float(row[target_column_index]) for row in split_data])\n",
    "        \n",
    "        # Use SelectKBest para selecionar k variáveis relevantes\n",
    "        selector = SelectKBest(score_func=f_regression, k=k)\n",
    "        X_new = selector.fit_transform(X, y)\n",
    "        \n",
    "        # Obtenha os índices das variáveis selecionadas\n",
    "        selected_indices = selector.get_support(indices=True)\n",
    "        \n",
    "        # Atualize o cabeçalho com as variáveis selecionadas e a coluna-alvo\n",
    "        header_columns = header.split(\",\")\n",
    "        selected_header = \",\".join([header_columns[i] for i in selected_indices] + [header_columns[target_column_index]])\n",
    "        \n",
    "        # Atualize os dados com apenas as variáveis relevantes e a coluna-alvo\n",
    "        relevant_lines = [selected_header]\n",
    "        for row, y_value in zip(X_new, y):\n",
    "            relevant_lines.append(\",\".join(map(str, row)) + f\",{y_value}\")\n",
    "        \n",
    "        # Adiciona o arquivo processado ao dicionário\n",
    "        relevant_features_data[filename] = relevant_lines\n",
    "    \n",
    "    return relevant_features_data\n",
    "\n",
    "\n",
    "def save_relevant_features_data(relevant_features_data, output_directory):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    for filename, data in relevant_features_data.items():\n",
    "        output_path = os.path.join(output_directory, filename)\n",
    "        with open(output_path, 'w') as file:\n",
    "            for line in data:\n",
    "                file.write(line + '\\n')\n",
    "        print(f\"Arquivo salvo: {output_path}\")\n",
    "\n",
    "# Caminho para o diretório com os arquivos TXT padronizados\n",
    "directory_path = 'output_pre_processamento/4_normalization'\n",
    "standardized_txt_data = load_standardized_txt_files(directory_path)\n",
    "\n",
    "# Selecionar as variáveis mais relevantes\n",
    "target_column_index = -1  # Suponha que a coluna EFFORT seja a última\n",
    "relevant_features_data = select_relevant_features(standardized_txt_data, target_column_index)\n",
    "\n",
    "# Salvar os dados com variáveis relevantes\n",
    "output_directory = 'output_pre_processamento/5_selection_features'\n",
    "save_relevant_features_data(relevant_features_data, output_directory)\n",
    "\n",
    "print(f\"Quantidade de arquivos processados para seleção de variáveis: {len(relevant_features_data)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
