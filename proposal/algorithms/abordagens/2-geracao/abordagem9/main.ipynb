{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando arquivo: C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\algorithms\\abordagens\\output\\0-saida\\cocomo81.txt\n",
      "Gerando 8 novas variáveis para o arquivo cocomo81.txt...\n",
      "Arquivo processado e salvo em: C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\algorithms\\abordagens\\output\\9-saida\\cocomo81.txt\n",
      "Processando arquivo: C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\algorithms\\abordagens\\output\\0-saida\\encoded_china.txt\n",
      "Gerando 10 novas variáveis para o arquivo encoded_china.txt...\n",
      "Arquivo processado e salvo em: C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\algorithms\\abordagens\\output\\9-saida\\encoded_china.txt\n",
      "Processando arquivo: C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\algorithms\\abordagens\\output\\0-saida\\encoded_desharnais.txt\n",
      "Gerando 11 novas variáveis para o arquivo encoded_desharnais.txt...\n",
      "Arquivo processado e salvo em: C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\algorithms\\abordagens\\output\\9-saida\\encoded_desharnais.txt\n",
      "Processando arquivo: C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\algorithms\\abordagens\\output\\0-saida\\encoded_maxwell.txt\n",
      "Gerando 18 novas variáveis para o arquivo encoded_maxwell.txt...\n",
      "Arquivo processado e salvo em: C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\algorithms\\abordagens\\output\\9-saida\\encoded_maxwell.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "\n",
    "def calculate_mutual_information(data, target_column):\n",
    "    \"\"\"\n",
    "    Calcula a informação mútua entre cada variável e a variável alvo.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Dataset contendo as variáveis.\n",
    "        target_column (str): Nome da coluna alvo.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Informação mútua de cada variável em relação à variável alvo.\n",
    "    \"\"\"\n",
    "    features = data.drop(columns=[target_column])\n",
    "    target = data[target_column]\n",
    "\n",
    "    # Calcula a informação mútua\n",
    "    mi_scores = mutual_info_regression(features, target, random_state=42)\n",
    "    return pd.Series(mi_scores, index=features.columns)\n",
    "\n",
    "\n",
    "def generate_new_features(data, target_column, num_new_features):\n",
    "    \"\"\"\n",
    "    Gera novas variáveis combinando as variáveis com maior informação mútua.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Dataset original.\n",
    "        target_column (str): Nome da coluna alvo.\n",
    "        num_new_features (int): Número de novas variáveis a serem geradas.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset com as novas variáveis adicionadas.\n",
    "    \"\"\"\n",
    "    mi_scores = calculate_mutual_information(data, target_column)\n",
    "    top_features = mi_scores.nlargest(min(num_new_features, len(mi_scores))).index\n",
    "\n",
    "    new_features_generated = 0\n",
    "    for i in range(len(top_features)):\n",
    "        for j in range(i + 1, len(top_features)):\n",
    "            if new_features_generated >= num_new_features:\n",
    "                break\n",
    "\n",
    "            new_feature_name = f\"MI_{top_features[i]}_{top_features[j]}\"\n",
    "            data[new_feature_name] = data[top_features[i]] * data[top_features[j]]\n",
    "            new_features_generated += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def process_datasets(input_dir, output_dir, new_feature_percentage=0.50):\n",
    "    \"\"\"\n",
    "    Processa os datasets de um diretório, gera novas variáveis e salva os resultados em outro diretório.\n",
    "\n",
    "    Args:\n",
    "        input_dir (str): Caminho do diretório de entrada.\n",
    "        output_dir (str): Caminho do diretório de saída.\n",
    "        new_feature_percentage (float): Percentual de novas variáveis a serem geradas.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            print(f\"Processando arquivo: {input_path}\")\n",
    "            try:\n",
    "                # Carrega o dataset\n",
    "                data = pd.read_csv(input_path, delimiter=\",\", skipinitialspace=True)\n",
    "\n",
    "                # Define a coluna alvo como a última coluna do dataset\n",
    "                target_column = data.columns[-1]\n",
    "\n",
    "                # Calcula o número de novas variáveis a serem geradas\n",
    "                num_features = len(data.columns) - 1\n",
    "                num_new_features = max(1, int(num_features * new_feature_percentage))\n",
    "\n",
    "                print(f\"Gerando {num_new_features} novas variáveis para o arquivo {filename}...\")\n",
    "\n",
    "                # Gera novas variáveis\n",
    "                data_with_new_features = generate_new_features(data, target_column, num_new_features)\n",
    "\n",
    "                # Reorganizar as colunas para manter a coluna alvo no final\n",
    "                columns = [col for col in data_with_new_features.columns if col != target_column] + [target_column]\n",
    "                data_with_new_features = data_with_new_features[columns]\n",
    "\n",
    "                # Salva o dataset transformado\n",
    "                data_with_new_features.to_csv(output_path, sep=\",\", index=False)\n",
    "                print(f\"Arquivo processado e salvo em: {output_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar {filename}: {e}\")\n",
    "\n",
    "\n",
    "# Diretórios de entrada e saída\n",
    "input_directory = r\"C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\algorithms\\abordagens\\output\\0-saida\"\n",
    "output_directory = r\"C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\algorithms\\abordagens\\output\\9-saida\"\n",
    "\n",
    "# Processar arquivos\n",
    "process_datasets(input_directory, output_directory, new_feature_percentage=0.50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
