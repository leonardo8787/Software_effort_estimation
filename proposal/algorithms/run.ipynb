{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rodar todos os códigos do diretório"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executado: C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\algorithms\\processamento\\processamento.ipynb\n",
      "Saída salva: C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\algorithms\\processamento\\output\\processamento.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nbformat\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def run_notebook(notebook_path, output_path=None):\n",
    "    \"\"\"\n",
    "    Executa um notebook e salva opcionalmente o resultado.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "            notebook = nbformat.read(f, as_version=4)\n",
    "        \n",
    "        executor = ExecutePreprocessor(timeout=None, kernel_name='python3')\n",
    "        executor.preprocess(notebook, {'metadata': {'path': './'}})\n",
    "        \n",
    "        if output_path:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                nbformat.write(notebook, f)\n",
    "        print(f\"Executado: {notebook_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao executar {notebook_path}: {e}\")\n",
    "\n",
    "def save_notebook_outputs_to_txt(notebook_path, output_txt_path):\n",
    "    \"\"\"\n",
    "    Lê um notebook executado e salva as saídas no formato TXT.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "            notebook_content = nbformat.read(f, as_version=4)\n",
    "        \n",
    "        with open(output_txt_path, 'w', encoding='utf-8') as txt_file:\n",
    "            for cell in notebook_content['cells']:\n",
    "                if cell['cell_type'] == 'code' and 'outputs' in cell:\n",
    "                    txt_file.write(\"\\n# Saída\\n\")\n",
    "                    for output in cell['outputs']:\n",
    "                        if 'text' in output:\n",
    "                            txt_file.write(output['text'] + '\\n')\n",
    "                        elif 'data' in output and 'text/plain' in output['data']:\n",
    "                            txt_file.write(output['data']['text/plain'] + '\\n')\n",
    "        print(f\"Saída salva: {output_txt_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar saída de {notebook_path}: {e}\")\n",
    "\n",
    "def process_notebook(notebook_path, output_notebook_path, output_txt_path):\n",
    "    \"\"\"\n",
    "    Função combinada para execução e salvamento da saída do notebook.\n",
    "    \"\"\"\n",
    "    run_notebook(notebook_path, output_notebook_path)\n",
    "    save_notebook_outputs_to_txt(output_notebook_path, output_txt_path)\n",
    "\n",
    "def parallel_execute_notebooks(notebook_dir, output_dir, max_threads=4):\n",
    "    \"\"\"\n",
    "    Paraleliza a execução de notebooks utilizando threads.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    notebook_files = [f for f in os.listdir(notebook_dir) if f.endswith('.ipynb')]\n",
    "    tasks = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_threads) as executor:\n",
    "        for notebook_file in notebook_files:\n",
    "            notebook_path = os.path.join(notebook_dir, notebook_file)\n",
    "            output_notebook_path = os.path.join(output_dir, notebook_file)\n",
    "            output_txt_path = os.path.join(output_dir, notebook_file.replace('.ipynb', '.txt'))\n",
    "            \n",
    "            # Submetendo tarefa\n",
    "            tasks.append(executor.submit(\n",
    "                process_notebook, notebook_path, output_notebook_path, output_txt_path\n",
    "            ))\n",
    "\n",
    "        for future in as_completed(tasks):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Erro durante execução paralela: {e}\")\n",
    "\n",
    "# Diretório de notebooks e saída\n",
    "input_notebook_dir = r\"C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\algorithms\\processamento\"\n",
    "output_dir = r\"C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\algorithms\\processamento\\output\"\n",
    "\n",
    "# Executar notebooks em paralelo\n",
    "parallel_execute_notebooks(input_notebook_dir, output_dir, max_threads=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Dataset  \\\n",
      "0                    0-saida/cocomo81.txt   \n",
      "1               0-saida/encoded_china.txt   \n",
      "2          0-saida/encoded_desharnais.txt   \n",
      "3             0-saida/encoded_maxwell.txt   \n",
      "4            1-saida/2-saida_cocomo81.txt   \n",
      "5       1-saida/2-saida_encoded_china.txt   \n",
      "6  1-saida/2-saida_encoded_desharnais.txt   \n",
      "7     1-saida/2-saida_encoded_maxwell.txt   \n",
      "8                   10-saida/cocomo81.txt   \n",
      "9              10-saida/encoded_china.txt   \n",
      "\n",
      "                                             Model   MSE  RMSE   MAE  MAPE  \\\n",
      "0  Regressão ElasticNet com KFold (100 repetições)  None  None  None  None   \n",
      "1  Regressão ElasticNet com KFold (100 repetições)  None  None  None  None   \n",
      "2  Regressão ElasticNet com KFold (100 repetições)  None  None  None  None   \n",
      "3  Regressão ElasticNet com KFold (100 repetições)  None  None  None  None   \n",
      "4  Regressão ElasticNet com KFold (100 repetições)  None  None  None  None   \n",
      "5  Regressão ElasticNet com KFold (100 repetições)  None  None  None  None   \n",
      "6  Regressão ElasticNet com KFold (100 repetições)  None  None  None  None   \n",
      "7  Regressão ElasticNet com KFold (100 repetições)  None  None  None  None   \n",
      "8  Regressão ElasticNet com KFold (100 repetições)  None  None  None  None   \n",
      "9  Regressão ElasticNet com KFold (100 repetições)  None  None  None  None   \n",
      "\n",
      "     R² Explained Variance Max Error  \n",
      "0  None               None      None  \n",
      "1  None               None      None  \n",
      "2  None               None      None  \n",
      "3  None               None      None  \n",
      "4  None               None      None  \n",
      "5  None               None      None  \n",
      "6  None               None      None  \n",
      "7  None               None      None  \n",
      "8  None               None      None  \n",
      "9  None               None      None  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Caminho do arquivo\n",
    "processamento_output_txt = r\"C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\algorithms\\processamento\\output\\processamento.txt\"\n",
    "\n",
    "# Inicializar lista para armazenar os resultados\n",
    "results = []\n",
    "\n",
    "# Processar o arquivo\n",
    "with open(processamento_output_txt, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Detectar nome do dataset e modelo\n",
    "        if line.startswith(\"Arquivo:\"):\n",
    "            parts = line.split(\" - \")\n",
    "            current_dataset = parts[0].split(\":\")[1].strip()\n",
    "            current_model = parts[1] if len(parts) > 1 else None\n",
    "            \n",
    "            # Redefinir métricas para o novo bloco\n",
    "            mse = None\n",
    "            rmse = None\n",
    "            mae = None\n",
    "            mape = None\n",
    "            r2 = None\n",
    "            explained_var = None\n",
    "            max_err = None\n",
    "\n",
    "        # Detectar métricas\n",
    "        elif \"Mean Squared Error\" in line:\n",
    "            mse = float(re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", line).group())\n",
    "        elif \"Root Mean Squared Error\" in line:\n",
    "            rmse = float(re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", line).group())\n",
    "        elif \"Mean Absolute Error\" in line:\n",
    "            mae = float(re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", line).group())\n",
    "        elif \"Mean Absolute Percentage Error\" in line:\n",
    "            mape = float(re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", line).group())\n",
    "        elif \"R² Score\" in line:\n",
    "            r2 = float(re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", line).group())\n",
    "        elif \"Explained Variance Score\" in line:\n",
    "            explained_var = float(re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", line).group())\n",
    "        elif \"Max Error\" in line:\n",
    "            max_err = float(re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", line).group())\n",
    "        \n",
    "        # Adicionar resultados ao final do bloco\n",
    "        elif line.startswith(\"--------------------------------------------------\"):\n",
    "            results.append({\n",
    "                \"Dataset\": current_dataset,\n",
    "                \"Model\": current_model,\n",
    "                \"MSE\": mse,\n",
    "                \"RMSE\": rmse,\n",
    "                \"MAE\": mae,\n",
    "                \"MAPE\": mape,\n",
    "                \"R²\": r2,\n",
    "                \"Explained Variance\": explained_var,\n",
    "                \"Max Error\": max_err,\n",
    "            })\n",
    "\n",
    "# Criar DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Salvar os resultados em um arquivo CSV\n",
    "df_results.to_csv('tabela_resultados_processamento.csv', index=False)\n",
    "\n",
    "# Exibir os primeiros resultados diretamente no terminal\n",
    "print(df_results.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
