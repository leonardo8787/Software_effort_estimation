{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cria simulação (Geração de amostras Sintéticas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo dataset: tratamento_china.txt\n",
      "Lendo dataset: tratamento_cocomo81.txt\n",
      "Lendo dataset: tratamento_desharnais.txt\n",
      "Lendo dataset: tratamento_maxwell.txt\n",
      "Gerando 500 amostras sintéticas para tratamento_china...\n",
      "Epoch [0/5000] - Loss D: 1.3442, Loss G: 0.6790\n",
      "Epoch [500/5000] - Loss D: 0.3864, Loss G: 2.2634\n",
      "Epoch [1000/5000] - Loss D: 0.3813, Loss G: 2.5238\n",
      "Epoch [1500/5000] - Loss D: 0.2655, Loss G: 3.2843\n",
      "Epoch [2000/5000] - Loss D: 0.3484, Loss G: 3.4396\n",
      "Epoch [2500/5000] - Loss D: 0.3255, Loss G: 2.7824\n",
      "Epoch [3000/5000] - Loss D: 0.1266, Loss G: 3.8165\n",
      "Epoch [3500/5000] - Loss D: 0.1032, Loss G: 4.1187\n",
      "Epoch [4000/5000] - Loss D: 0.0957, Loss G: 4.3513\n",
      "Epoch [4500/5000] - Loss D: 0.0801, Loss G: 3.7662\n",
      "Epoch [4999/5000] - Loss D: 0.2042, Loss G: 4.0304\n",
      "Amostras normalizadas salvas em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\1-tratamento\\tratamento_china.txt\n",
      "Gerando 500 amostras sintéticas para tratamento_cocomo81...\n",
      "Epoch [0/5000] - Loss D: 1.3629, Loss G: 0.7282\n",
      "Epoch [500/5000] - Loss D: 1.4579, Loss G: 0.6166\n",
      "Epoch [1000/5000] - Loss D: 1.3744, Loss G: 0.7375\n",
      "Epoch [1500/5000] - Loss D: 1.1078, Loss G: 0.7518\n",
      "Epoch [2000/5000] - Loss D: 1.6216, Loss G: 0.7735\n",
      "Epoch [2500/5000] - Loss D: 1.3599, Loss G: 0.9545\n",
      "Epoch [3000/5000] - Loss D: 1.0668, Loss G: 0.8291\n",
      "Epoch [3500/5000] - Loss D: 2.3262, Loss G: 0.7798\n",
      "Epoch [4000/5000] - Loss D: 1.1965, Loss G: 0.9905\n",
      "Epoch [4500/5000] - Loss D: 1.5375, Loss G: 0.7146\n",
      "Epoch [4999/5000] - Loss D: 1.3671, Loss G: 0.7796\n",
      "Amostras normalizadas salvas em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\1-tratamento\\tratamento_cocomo81.txt\n",
      "Gerando 500 amostras sintéticas para tratamento_desharnais...\n",
      "Epoch [0/5000] - Loss D: 1.3871, Loss G: 0.6691\n",
      "Epoch [500/5000] - Loss D: 1.0131, Loss G: 1.2311\n",
      "Epoch [1000/5000] - Loss D: 1.1471, Loss G: 0.8071\n",
      "Epoch [1500/5000] - Loss D: 1.0854, Loss G: 0.9791\n",
      "Epoch [2000/5000] - Loss D: 1.0532, Loss G: 0.9151\n",
      "Epoch [2500/5000] - Loss D: 1.1151, Loss G: 0.9071\n",
      "Epoch [3000/5000] - Loss D: 0.7267, Loss G: 1.2972\n",
      "Epoch [3500/5000] - Loss D: 1.2277, Loss G: 0.9668\n",
      "Epoch [4000/5000] - Loss D: 1.3582, Loss G: 0.9192\n",
      "Epoch [4500/5000] - Loss D: 1.1955, Loss G: 0.9590\n",
      "Epoch [4999/5000] - Loss D: 1.0349, Loss G: 1.1481\n",
      "Amostras normalizadas salvas em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\1-tratamento\\tratamento_desharnais.txt\n",
      "Gerando 500 amostras sintéticas para tratamento_maxwell...\n",
      "Epoch [0/5000] - Loss D: 1.4092, Loss G: 0.7644\n",
      "Epoch [500/5000] - Loss D: 1.4393, Loss G: 0.7055\n",
      "Epoch [1000/5000] - Loss D: 1.2042, Loss G: 0.9696\n",
      "Epoch [1500/5000] - Loss D: 1.2140, Loss G: 0.8255\n",
      "Epoch [2000/5000] - Loss D: 1.3494, Loss G: 0.6213\n",
      "Epoch [2500/5000] - Loss D: 1.1938, Loss G: 0.9582\n",
      "Epoch [3000/5000] - Loss D: 1.1061, Loss G: 0.8547\n",
      "Epoch [3500/5000] - Loss D: 1.2621, Loss G: 0.8885\n",
      "Epoch [4000/5000] - Loss D: 1.1121, Loss G: 0.8813\n",
      "Epoch [4500/5000] - Loss D: 1.1138, Loss G: 1.0690\n",
      "Epoch [4999/5000] - Loss D: 1.0633, Loss G: 1.3451\n",
      "Amostras normalizadas salvas em C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\1-tratamento\\tratamento_maxwell.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Configuração da semente para reprodutibilidade\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# Definir a arquitetura da GAN\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Função para treinar a GAN\n",
    "def train_gan(real_data, num_features, latent_dim, num_samples, num_epochs=5000, batch_size=32):\n",
    "    # Normalizar os dados reais\n",
    "    scaler = MinMaxScaler()\n",
    "    try:\n",
    "        real_data = scaler.fit_transform(real_data)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Erro na normalização dos dados reais: {e}\")\n",
    "\n",
    "    # Converter para tensores\n",
    "    real_data = torch.tensor(real_data, dtype=torch.float32)\n",
    "\n",
    "    # Inicializar o gerador e o discriminador\n",
    "    generator = Generator(latent_dim, num_features)\n",
    "    discriminator = Discriminator(num_features)\n",
    "\n",
    "    # Definir otimizadores e função de perda\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Treinar o discriminador\n",
    "        for _ in range(max(1, real_data.size(0) // batch_size)):\n",
    "            idx = np.random.randint(0, real_data.size(0), batch_size)\n",
    "            real_samples = real_data[idx]\n",
    "\n",
    "            # Labels reais e falsos\n",
    "            real_labels = torch.ones((real_samples.size(0), 1))\n",
    "            fake_labels = torch.zeros((real_samples.size(0), 1))\n",
    "\n",
    "            # Amostras do gerador\n",
    "            noise = torch.randn((real_samples.size(0), latent_dim))\n",
    "            fake_samples = generator(noise)\n",
    "\n",
    "            # Predições do discriminador\n",
    "            real_preds = discriminator(real_samples)\n",
    "            fake_preds = discriminator(fake_samples.detach())\n",
    "\n",
    "            # Perda do discriminador\n",
    "            loss_real = criterion(real_preds, real_labels)\n",
    "            loss_fake = criterion(fake_preds, fake_labels)\n",
    "            loss_D = loss_real + loss_fake\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        # Treinar o gerador\n",
    "        noise = torch.randn((batch_size, latent_dim))\n",
    "        fake_samples = generator(noise)\n",
    "        fake_preds = discriminator(fake_samples)\n",
    "        loss_G = criterion(fake_preds, torch.ones((batch_size, 1)))\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Exibir progresso\n",
    "        if epoch % 500 == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - Loss D: {loss_D.item():.4f}, Loss G: {loss_G.item():.4f}\")\n",
    "\n",
    "    # Gerar novas amostras sintéticas\n",
    "    noise = torch.randn((num_samples, latent_dim))\n",
    "    synthetic_data = generator(noise).detach().numpy()\n",
    "\n",
    "    # Reverter a normalização\n",
    "    synthetic_data = scaler.inverse_transform(synthetic_data)\n",
    "    return synthetic_data\n",
    "\n",
    "# Função principal para processar múltiplos datasets\n",
    "def generate_synthetic_samples(input_directory, output_directory, total_samples=2000):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Carregar datasets\n",
    "    datasets = {}\n",
    "    for file in os.listdir(input_directory):\n",
    "        filepath = os.path.join(input_directory, file)\n",
    "        filename, ext = os.path.splitext(file)\n",
    "        if ext == '.txt':\n",
    "            try:\n",
    "                print(f\"Lendo dataset: {file}\")\n",
    "                df = pd.read_csv(filepath, delimiter=',')\n",
    "                df = df.apply(pd.to_numeric, errors='coerce')  # Forçar conversão para numérico\n",
    "                df = df.dropna()  # Remover linhas com valores não numéricos\n",
    "                if not df.empty:\n",
    "                    datasets[filename] = df\n",
    "                else:\n",
    "                    print(f\"{file} está vazio após limpeza.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao carregar {file}: {e}\")\n",
    "\n",
    "    if not datasets:\n",
    "        print(\"Nenhum dataset válido encontrado. Encerrando...\")\n",
    "        return\n",
    "\n",
    "    # Gerar amostras sintéticas para cada dataset\n",
    "    for name, df in datasets.items():\n",
    "        num_features = df.shape[1]\n",
    "        samples_per_dataset = max(1, total_samples // len(datasets))\n",
    "        latent_dim = 10  # Dimensão do espaço latente\n",
    "\n",
    "        print(f\"Gerando {samples_per_dataset} amostras sintéticas para {name}...\")\n",
    "        try:\n",
    "            synthetic_data = train_gan(df.values, num_features, latent_dim, samples_per_dataset)\n",
    "\n",
    "            # Verificar se já existe um arquivo de saída\n",
    "            output_path = os.path.join(output_directory, f\"{name}.txt\")\n",
    "            if os.path.exists(output_path):\n",
    "                existing_data = pd.read_csv(output_path, delimiter=',')\n",
    "                existing_data['source'] = 'original'\n",
    "            else:\n",
    "                existing_data = df.copy()\n",
    "                existing_data['source'] = 'original'\n",
    "\n",
    "            synthetic_df = pd.DataFrame(synthetic_data, columns=df.columns)\n",
    "            synthetic_df['source'] = 'synthetic'\n",
    "\n",
    "            # Combinar os dados originais e sintéticos\n",
    "            combined_data = pd.concat([existing_data, synthetic_df], ignore_index=True)\n",
    "\n",
    "            # Normalizar os dados combinados\n",
    "            scaler = MinMaxScaler()\n",
    "            normalized_data = scaler.fit_transform(combined_data.drop(columns=['source']))\n",
    "            normalized_df = pd.DataFrame(normalized_data, columns=combined_data.columns[:-1])\n",
    "            normalized_df['source'] = combined_data['source'].values\n",
    "\n",
    "            # Salvar os dados normalizados\n",
    "            normalized_df.to_csv(output_path, sep=',', index=False)\n",
    "            print(f\"Amostras normalizadas salvas em {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao gerar amostras para {name}: {e}\")\n",
    "\n",
    "# Diretórios de entrada e saída\n",
    "input_directory = r\"C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\1-tratamento-target-encoding-normalizado\"\n",
    "output_directory = r\"C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\\1-tratamento\"\n",
    "\n",
    "# Gerar amostras sintéticas\n",
    "generate_synthetic_samples(input_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Configuração da semente para reprodutibilidade\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# Definir a arquitetura da GAN\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Função para treinar a GAN\n",
    "def train_gan(real_data, num_features, latent_dim, num_samples, num_epochs=5000, batch_size=32):\n",
    "    scaler = MinMaxScaler()\n",
    "    real_data = scaler.fit_transform(real_data)\n",
    "    real_data = torch.tensor(real_data, dtype=torch.float32)\n",
    "\n",
    "    generator = Generator(latent_dim, num_features)\n",
    "    discriminator = Discriminator(num_features)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for _ in range(max(1, real_data.size(0) // batch_size)):\n",
    "            idx = np.random.randint(0, real_data.size(0), batch_size)\n",
    "            real_samples = real_data[idx]\n",
    "\n",
    "            real_labels = torch.ones((real_samples.size(0), 1))\n",
    "            fake_labels = torch.zeros((real_samples.size(0), 1))\n",
    "\n",
    "            noise = torch.randn((real_samples.size(0), latent_dim))\n",
    "            fake_samples = generator(noise)\n",
    "\n",
    "            real_preds = discriminator(real_samples)\n",
    "            fake_preds = discriminator(fake_samples.detach())\n",
    "\n",
    "            loss_real = criterion(real_preds, real_labels)\n",
    "            loss_fake = criterion(fake_preds, fake_labels)\n",
    "            loss_D = loss_real + loss_fake\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        noise = torch.randn((batch_size, latent_dim))\n",
    "        fake_samples = generator(noise)\n",
    "        fake_preds = discriminator(fake_samples)\n",
    "        loss_G = criterion(fake_preds, torch.ones((batch_size, 1)))\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        if epoch % 500 == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - Loss D: {loss_D.item():.4f}, Loss G: {loss_G.item():.4f}\")\n",
    "\n",
    "    noise = torch.randn((num_samples, latent_dim))\n",
    "    synthetic_data = generator(noise).detach().numpy()\n",
    "    synthetic_data = scaler.inverse_transform(synthetic_data)\n",
    "    return synthetic_data\n",
    "\n",
    "# Função para processar subpastas e gerar saídas\n",
    "def process_subfolders(input_directory, output_directory, total_samples=2000):\n",
    "    for root, dirs, files in os.walk(input_directory):\n",
    "        for file in files:\n",
    "            filepath = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(root, input_directory)\n",
    "            output_subdir = os.path.join(output_directory, relative_path)\n",
    "            os.makedirs(output_subdir, exist_ok=True)\n",
    "\n",
    "            if file.endswith('.txt'):\n",
    "                try:\n",
    "                    print(f\"Lendo arquivo: {filepath}\")\n",
    "                    df = pd.read_csv(filepath, delimiter=',')\n",
    "                    df = df.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "                    if df.empty:\n",
    "                        print(f\"Arquivo {file} está vazio após a limpeza.\")\n",
    "                        continue\n",
    "\n",
    "                    num_features = df.shape[1]\n",
    "                    samples_per_dataset = total_samples // len(files)\n",
    "                    synthetic_data = train_gan(df.values, num_features, latent_dim=10, num_samples=samples_per_dataset)\n",
    "\n",
    "                    output_path = os.path.join(output_subdir, file)\n",
    "                    synthetic_df = pd.DataFrame(synthetic_data, columns=df.columns)\n",
    "                    synthetic_df.to_csv(output_path, sep=',', index=False)\n",
    "                    print(f\"Arquivo gerado: {output_path}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao processar {file}: {e}\")\n",
    "\n",
    "# Diretórios de entrada e saída\n",
    "input_directory = r\"C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\2-geracao-variaveis\"\n",
    "output_directory = r\"C:\\Users\\CALEO\\OneDrive - Hexagon\\Documents\\GitHub\\Software_effort_estimation\\proposal\\abordagem2\\saida\\3-simulacao\"\n",
    "\n",
    "# Processar subpastas e gerar saídas\n",
    "process_subfolders(input_directory, output_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
