{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "210b8215-137c-4f4e-af0e-5d185a6bf1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Analizando o arquivo: ../datasets_2/albrecht.arff\n",
      "Atributos: ['Input', 'Output', 'Inquiry', 'File', 'FPAdj', 'RawFPcounts', 'AdjFP', 'Effort']\n",
      "Número de instâncias: 24\n",
      "Primeira instância: [25.0, 150.0, 75.0, 60.0, 1.0, 1750.0, 1750.0, 102.4]\n",
      "Última instância: [12.0, 15.0, 0.0, 15.0, 0.95, 273.68, 260.0, 6.1]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Analizando o arquivo: ../datasets_2/kemerer.arff\n",
      "Atributos: ['ID', 'Language', 'Hardware', 'Duration', 'KSLOC', 'AdjFP', 'RAWFP', 'EffortMM']\n",
      "Número de instâncias: 15\n",
      "Primeira instância: [1.0, 1.0, 1.0, 17.0, 253.6, 1217.1, 1010.0, 287.0]\n",
      "Última instância: [15.0, 3.0, 1.0, 14.0, 60.2, 1044.3, 976.0, 69.9]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Analizando o arquivo: ../datasets_2/cocomo81.arff\n",
      "Atributos: ['rely', 'data', 'cplx', 'time', 'stor', 'virt', 'turn', 'acap', 'aexp', 'pcap', 'vexp', 'lexp', 'modp', 'tool', 'sced', 'loc', 'actual']\n",
      "Número de instâncias: 63\n",
      "Primeira instância: [0.88, 1.16, 0.7, 1.0, 1.06, 1.15, 1.07, 1.19, 1.13, 1.17, 1.1, 1.0, 1.24, 1.1, 1.04, 113.0, 2040.0]\n",
      "Última instância: [1.0, 0.94, 1.15, 1.0, 1.0, 1.0, 0.87, 0.71, 0.82, 0.86, 1.0, 1.0, 0.82, 1.0, 1.0, 10.0, 15.0]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Analizando o arquivo: ../datasets_2/china.arff\n",
      "Atributos: ['ID', 'AFP', 'Input', 'Output', 'Enquiry', 'File', 'Interface', 'Added', 'Changed', 'Deleted', 'PDR_AFP', 'PDR_UFP', 'NPDR_AFP', 'NPDU_UFP', 'Resource', 'Dev.Type', 'Duration', 'N_effort', 'Effort']\n",
      "Número de instâncias: 499\n",
      "Primeira instância: [1.0, 1587.0, 774.0, 260.0, 340.0, 128.0, 0.0, 1502.0, 0.0, 0.0, 4.7, 5.0, 4.7, 5.0, 4.0, 0.0, 4.0, 7490.0, 7490.0]\n",
      "Última instância: [499.0, 213.0, 123.0, 91.0, 28.0, 0.0, 0.0, 36.0, 206.0, 0.0, 10.3, 9.0, 10.3, 9.0, 1.0, 0.0, 7.0, 2185.0, 2185.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import arff\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "file_paths = [\n",
    "    '../datasets_2/albrecht.arff',\n",
    "    '../datasets_2/kemerer.arff',\n",
    "    '../datasets_2/cocomo81.arff',\n",
    "    #'../datasets_2/desharnais.arff',\n",
    "    '../datasets_2/china.arff',\n",
    "]\n",
    "\n",
    "# Para cada arquivo ARFF\n",
    "for file_path in file_paths:\n",
    "    print(f\"\\n\\n\\nAnalizando o arquivo: {file_path}\")\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = arff.load(f)\n",
    "\n",
    "    # Exibir algumas informações sobre o arquivo ARFF\n",
    "    print(\"Atributos:\", [attr[0] for attr in data['attributes']])\n",
    "    print(\"Número de instâncias:\", len(data['data']))\n",
    "    print(\"Primeira instância:\", data['data'][0])\n",
    "    print(\"Última instância:\", data['data'][-1])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313c019b-9c31-4de1-8b24-c14c42ecf018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, r2_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "def calculate_metrics(true_values, predicted_values):\n",
    "    mae = mean_absolute_error(true_values, predicted_values)\n",
    "    medae = median_absolute_error(true_values, predicted_values)\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "    r2 = r2_score(true_values, predicted_values)\n",
    "    \n",
    "    return mae, medae, rmse, r2\n",
    "\n",
    "def load_arff(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = arff.load(f)\n",
    "\n",
    "    attributes = [attr[0] for attr in data['attributes']]\n",
    "    X = np.array(data['data'])[:, :-1]\n",
    "    y = np.array(data['data'])[:, -1].astype(float)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "output_file = \"../results/mlp/sem-pre/output_mlp.txt\"\n",
    "sys.stdout = open(output_file, \"w\")\n",
    "sys.stderr = open(output_file, \"a\")\n",
    "\n",
    "hidden_layer_sizes = (100, 100, 100)  # Tamanhos das camadas ocultas\n",
    "activation = 'relu'  # Função de ativação\n",
    "solver = 'adam'  # Algoritmo de otimização\n",
    "max_iter_values = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]  # Número máximo de iterações\n",
    "learning_rate_init_values = [0.002, 0.003, 0.004, 0.005, 0.006]  # Taxa de aprendizado inicial\n",
    "\n",
    "metrics_data = []\n",
    "\n",
    "num_runs = 30\n",
    "\n",
    "for run in range(num_runs):\n",
    "    for file_path in file_paths:\n",
    "        X, y = load_arff(file_path)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "        for max_iter in max_iter_values:\n",
    "            for learning_rate_init in learning_rate_init_values:\n",
    "                mlp = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes, activation=activation, solver=solver,\n",
    "                                   max_iter=max_iter, learning_rate_init=learning_rate_init)\n",
    "                mlp.fit(X_train, y_train)\n",
    "                y_pred = mlp.predict(X_test)\n",
    "                mae, medae, rmse, r2 = calculate_metrics(y_test, y_pred)\n",
    "                metrics_data.append([file_path, \"MAE\", mae, max_iter, learning_rate_init])\n",
    "                metrics_data.append([file_path, \"Median Absolute Error\", medae, max_iter, learning_rate_init])\n",
    "                metrics_data.append([file_path, \"RMSE\", rmse, max_iter, learning_rate_init])\n",
    "                metrics_data.append([file_path, \"R2 Score\", r2, max_iter, learning_rate_init])\n",
    "\n",
    "headers = [\"Dataset\", \"Metric\", \"metric valor\", \"t\", \"l\"]\n",
    "print(tabulate(metrics_data, headers=headers))\n",
    "print('\\n')\n",
    "sys.stdout.close()\n",
    "sys.stderr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4892ff2-257c-446c-8586-dc0d9b684e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Redirecting stdout and stderr to the specified file\n",
    "output_file = \"../results/mlp/sem-pre/output_mlp_analises_t_l_best.txt\"\n",
    "sys.stdout = open(output_file, \"w\")\n",
    "sys.stderr = open(output_file, \"a\")\n",
    "\n",
    "cont = 0\n",
    "\n",
    "# Dictionary to store metrics with nested structure for datasets, t, l, and metrics\n",
    "datasets = {\n",
    "    \"albrecht\": {},\n",
    "    \"kemerer\": {},\n",
    "    \"cocomo81\": {},\n",
    "    \"china\": {}\n",
    "}\n",
    "\n",
    "def extract_metrics_values(filename):\n",
    "    global cont\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            if not line.strip() or line.startswith('-'):\n",
    "                continue\n",
    "            \n",
    "            # Updated regex to capture t and l values\n",
    "            match = re.match(r'.*?([A-Za-z0-9_/.]+)\\s+(.*?)\\s+([\\d.]+)\\s+(\\d+)\\s+(\\d+)', line)\n",
    "            if match:\n",
    "                dataset, metric, value, t, l = match.groups()\n",
    "                value = float(value)\n",
    "                t = int(t)\n",
    "                l = int(l)\n",
    "                \n",
    "                if \"albrecht.arff\" in dataset:\n",
    "                    dataset_key = \"albrecht\"\n",
    "                elif \"kemerer.arff\" in dataset:\n",
    "                    dataset_key = \"kemerer\"\n",
    "                elif \"cocomo81.arff\" in dataset:\n",
    "                    dataset_key = \"cocomo81\"\n",
    "                elif \"china.arff\" in dataset:\n",
    "                    dataset_key = \"china\"\n",
    "                else:\n",
    "                    print(f\"Dataset not recognized: {dataset}\", file=sys.stderr)\n",
    "                    continue\n",
    "\n",
    "                # Initialize nested dictionaries if not already done\n",
    "                if t not in datasets[dataset_key]:\n",
    "                    datasets[dataset_key][t] = {}\n",
    "                if l not in datasets[dataset_key][t]:\n",
    "                    datasets[dataset_key][t][l] = {}\n",
    "                if metric not in datasets[dataset_key][t][l]:\n",
    "                    datasets[dataset_key][t][l][metric] = []\n",
    "\n",
    "                # Append the metric value\n",
    "                datasets[dataset_key][t][l][metric].append(value)\n",
    "                cont += 1\n",
    "            # else:\n",
    "            #     print(f\"Line did not match: {line.strip()}\", file=sys.stderr)\n",
    "\n",
    "def calculate_statistics(metrics):\n",
    "    results = {}\n",
    "    for metric, values in metrics.items():\n",
    "        results[metric] = {\n",
    "            'Média': np.mean(values),\n",
    "            'Mínimo': np.min(values),\n",
    "            'Máximo': np.max(values),\n",
    "            'Desvio Padrão': np.std(values)\n",
    "        }\n",
    "    return results\n",
    "\n",
    "filename = '../results/mlp/sem-pre/output_mlp.txt'\n",
    "extract_metrics_values(filename)\n",
    "\n",
    "# Function to find the best l and t for each metric\n",
    "def find_best_t_l_for_metric(dataset_name, d_values, metric_name):\n",
    "    best_t_l = None\n",
    "    best_value = float('inf') if metric_name in ['MAE', 'Median Absolute Error', 'RMSE'] else float('-inf')\n",
    "    \n",
    "    for t, k_values in d_values.items():\n",
    "        for l, metrics in k_values.items():\n",
    "            if metric_name in metrics:\n",
    "                statistics = calculate_statistics(metrics)\n",
    "                metric_value = statistics[metric_name]['Média']\n",
    "                \n",
    "                if (metric_name in ['MAE', 'Median Absolute Error', 'RMSE'] and metric_value < best_value) or \\\n",
    "                   (metric_name == 'R2 Score' and metric_value > best_value):\n",
    "                    best_value = metric_value\n",
    "                    best_t_l = (t, l, statistics)\n",
    "    \n",
    "    return best_t_l\n",
    "\n",
    "# Metrics we are interested in\n",
    "metrics_of_interest = ['MAE', 'Median Absolute Error', 'RMSE', 'R2 Score']\n",
    "\n",
    "# Iterate through the datasets and print statistics for the best l and t for each metric\n",
    "for dataset_name, d_values in datasets.items():\n",
    "    print(f\"Melhores resultados para o dataset: {dataset_name}\")\n",
    "    for metric_name in metrics_of_interest:\n",
    "        best_t_l = find_best_t_l_for_metric(dataset_name, d_values, metric_name)\n",
    "        if best_t_l:\n",
    "            t, l, statistics = best_t_l\n",
    "            print(f\"  Melhor para a métrica {metric_name}: t = {t}, l = {l}\")\n",
    "            print(f\"    Média: {statistics[metric_name]['Média']:.4f}\")\n",
    "            print(f\"    Mínimo: {statistics[metric_name]['Mínimo']:.4f}\")\n",
    "            print(f\"    Máximo: {statistics[metric_name]['Máximo']:.4f}\")\n",
    "            print(f\"    Desvio Padrão: {statistics[metric_name]['Desvio Padrão']:.4f}\")\n",
    "            print()  # Add a new line for better readability\n",
    "\n",
    "#print(\"Número de iterações das métricas: \" + str(cont))\n",
    "\n",
    "sys.stdout.close()\n",
    "sys.stderr.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0366d039-3d58-42e6-9077-1da31505523d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
