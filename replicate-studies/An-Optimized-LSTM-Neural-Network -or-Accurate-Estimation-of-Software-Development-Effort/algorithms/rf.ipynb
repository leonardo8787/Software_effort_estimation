{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "217f673b-e89e-4b05-aa1c-b88c8dd6faaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Analizando o arquivo: ../datasets_2/albrecht.arff\n",
      "Atributos: ['Input', 'Output', 'Inquiry', 'File', 'FPAdj', 'RawFPcounts', 'AdjFP', 'Effort']\n",
      "Número de instâncias: 24\n",
      "Primeira instância: [25.0, 150.0, 75.0, 60.0, 1.0, 1750.0, 1750.0, 102.4]\n",
      "Última instância: [12.0, 15.0, 0.0, 15.0, 0.95, 273.68, 260.0, 6.1]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Analizando o arquivo: ../datasets_2/kemerer.arff\n",
      "Atributos: ['ID', 'Language', 'Hardware', 'Duration', 'KSLOC', 'AdjFP', 'RAWFP', 'EffortMM']\n",
      "Número de instâncias: 15\n",
      "Primeira instância: [1.0, 1.0, 1.0, 17.0, 253.6, 1217.1, 1010.0, 287.0]\n",
      "Última instância: [15.0, 3.0, 1.0, 14.0, 60.2, 1044.3, 976.0, 69.9]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Analizando o arquivo: ../datasets_2/cocomo81.arff\n",
      "Atributos: ['rely', 'data', 'cplx', 'time', 'stor', 'virt', 'turn', 'acap', 'aexp', 'pcap', 'vexp', 'lexp', 'modp', 'tool', 'sced', 'loc', 'actual']\n",
      "Número de instâncias: 63\n",
      "Primeira instância: [0.88, 1.16, 0.7, 1.0, 1.06, 1.15, 1.07, 1.19, 1.13, 1.17, 1.1, 1.0, 1.24, 1.1, 1.04, 113.0, 2040.0]\n",
      "Última instância: [1.0, 0.94, 1.15, 1.0, 1.0, 1.0, 0.87, 0.71, 0.82, 0.86, 1.0, 1.0, 0.82, 1.0, 1.0, 10.0, 15.0]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Analizando o arquivo: ../datasets_2/china.arff\n",
      "Atributos: ['ID', 'AFP', 'Input', 'Output', 'Enquiry', 'File', 'Interface', 'Added', 'Changed', 'Deleted', 'PDR_AFP', 'PDR_UFP', 'NPDR_AFP', 'NPDU_UFP', 'Resource', 'Dev.Type', 'Duration', 'N_effort', 'Effort']\n",
      "Número de instâncias: 499\n",
      "Primeira instância: [1.0, 1587.0, 774.0, 260.0, 340.0, 128.0, 0.0, 1502.0, 0.0, 0.0, 4.7, 5.0, 4.7, 5.0, 4.0, 0.0, 4.0, 7490.0, 7490.0]\n",
      "Última instância: [499.0, 213.0, 123.0, 91.0, 28.0, 0.0, 0.0, 36.0, 206.0, 0.0, 10.3, 9.0, 10.3, 9.0, 1.0, 0.0, 7.0, 2185.0, 2185.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import arff\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "file_paths = [\n",
    "    '../datasets_2/albrecht.arff',\n",
    "    '../datasets_2/kemerer.arff',\n",
    "    '../datasets_2/cocomo81.arff',\n",
    "    #'../datasets_2/desharnais.arff',\n",
    "    '../datasets_2/china.arff',\n",
    "]\n",
    "\n",
    "# Para cada arquivo ARFF\n",
    "for file_path in file_paths:\n",
    "    print(f\"\\n\\n\\nAnalizando o arquivo: {file_path}\")\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = arff.load(f)\n",
    "\n",
    "    # Exibir algumas informações sobre o arquivo ARFF\n",
    "    print(\"Atributos:\", [attr[0] for attr in data['attributes']])\n",
    "    print(\"Número de instâncias:\", len(data['data']))\n",
    "    print(\"Primeira instância:\", data['data'][0])\n",
    "    print(\"Última instância:\", data['data'][-1])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3553b186-c04f-4c88-8ec0-fa193cae0003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import arff\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, r2_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "def calculate_metrics(true_values, predicted_values):\n",
    "    mae = mean_absolute_error(true_values, predicted_values)\n",
    "    medae = median_absolute_error(true_values, predicted_values)\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "    r2 = r2_score(true_values, predicted_values)\n",
    "    return mae, medae, rmse, r2\n",
    "\n",
    "def load_arff(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = arff.load(f)\n",
    "    X = np.array(data['data'])[:, :-1]\n",
    "    y = np.array(data['data'])[:, -1].astype(float)\n",
    "    return X, y\n",
    "\n",
    "output_file = \"../results/rf/sem-pre/output_rf_2.txt\"\n",
    "sys.stdout = open(output_file, \"w\")\n",
    "sys.stderr = open(output_file, \"a\")\n",
    "\n",
    "max_depth_values = range(5, 11)\n",
    "n_estimators_values = [50, 100, 150, 200, 250, 300]\n",
    "\n",
    "metrics_data = []\n",
    "\n",
    "num_runs = 30\n",
    "\n",
    "for run in range(num_runs):\n",
    "    for file_path in file_paths:\n",
    "        X, y = load_arff(file_path)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "        \n",
    "        for max_depth in max_depth_values:\n",
    "            for n_estimators in n_estimators_values:\n",
    "                rf = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, criterion='squared_error', min_samples_split=2, min_samples_leaf=2)\n",
    "                rf.fit(X_train, y_train)\n",
    "                y_pred = rf.predict(X_test)\n",
    "                mae, medae, rmse, r2 = calculate_metrics(y_test, y_pred)\n",
    "                metrics_data.append([file_path, \"MAE\", mae, max_depth, n_estimators])\n",
    "                metrics_data.append([file_path, \"Median Absolute Error\", medae, max_depth, n_estimators])\n",
    "                metrics_data.append([file_path, \"RMSE\", rmse, max_depth, n_estimators])\n",
    "                metrics_data.append([file_path, \"R2 Score\", r2, max_depth, n_estimators])\n",
    "\n",
    "headers = [\"Dataset\", \"Metric\", \"metric valor\", \"d\", \"t\"]\n",
    "print(tabulate(metrics_data, headers=headers))\n",
    "print('\\n')\n",
    "sys.stdout.close()\n",
    "sys.stderr.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c2508d-a2df-4e21-b435-005664293bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Redirecting stdout and stderr to the specified file\n",
    "output_file = \"../results/rf/sem-pre/output_rf_analises_d_t_best_semente.txt\"\n",
    "sys.stdout = open(output_file, \"w\")\n",
    "sys.stderr = open(output_file, \"a\")\n",
    "\n",
    "cont = 0\n",
    "\n",
    "# Dictionary to store metrics with nested structure for datasets, d, t, and metrics\n",
    "datasets = {\n",
    "    \"albrecht\": {},\n",
    "    \"kemerer\": {},\n",
    "    \"cocomo81\": {},\n",
    "    \"china\": {}\n",
    "}\n",
    "\n",
    "def extract_metrics_values(filename):\n",
    "    global cont\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            if not line.strip() or line.startswith('-'):\n",
    "                continue\n",
    "            \n",
    "            # Updated regex to capture d and t values\n",
    "            match = re.match(r'.*?([A-Za-z0-9_/.]+)\\s+(.*?)\\s+([\\d.]+)\\s+(\\d+)\\s+(\\d+)', line)\n",
    "            if match:\n",
    "                dataset, metric, value, d, t = match.groups()\n",
    "                value = float(value)\n",
    "                d = int(d)\n",
    "                t = int(t)\n",
    "                \n",
    "                if \"albrecht.arff\" in dataset:\n",
    "                    dataset_key = \"albrecht\"\n",
    "                elif \"kemerer.arff\" in dataset:\n",
    "                    dataset_key = \"kemerer\"\n",
    "                elif \"cocomo81.arff\" in dataset:\n",
    "                    dataset_key = \"cocomo81\"\n",
    "                elif \"china.arff\" in dataset:\n",
    "                    dataset_key = \"china\"\n",
    "                else:\n",
    "                    print(f\"Dataset not recognized: {dataset}\", file=sys.stderr)\n",
    "                    continue\n",
    "\n",
    "                # Initialize nested dictionaries if not already done\n",
    "                if d not in datasets[dataset_key]:\n",
    "                    datasets[dataset_key][d] = {}\n",
    "                if t not in datasets[dataset_key][d]:\n",
    "                    datasets[dataset_key][d][t] = {}\n",
    "                if metric not in datasets[dataset_key][d][t]:\n",
    "                    datasets[dataset_key][d][t][metric] = []\n",
    "\n",
    "                # Append the metric value\n",
    "                datasets[dataset_key][d][t][metric].append(value)\n",
    "                cont += 1\n",
    "            # else:\n",
    "            #     print(f\"Line did not match: {line.strip()}\", file=sys.stderr)\n",
    "\n",
    "def calculate_statistics(metrics):\n",
    "    results = {}\n",
    "    for metric, values in metrics.items():\n",
    "        results[metric] = {\n",
    "            'Média': np.mean(values),\n",
    "            'Mínimo': np.min(values),\n",
    "            'Máximo': np.max(values),\n",
    "            'Desvio Padrão': np.std(values)\n",
    "        }\n",
    "    return results\n",
    "\n",
    "filename = '../results/rf/sem-pre/output_rf_2_semente.txt'\n",
    "extract_metrics_values(filename)\n",
    "\n",
    "# Function to find the best t and d for each metric\n",
    "def find_best_d_t_for_metric(dataset_name, d_values, metric_name):\n",
    "    best_d_n = None\n",
    "    best_value = float('inf') if metric_name in ['MAE', 'Median Absolute Error', 'RMSE'] else float('-inf')\n",
    "    \n",
    "    for d, k_values in d_values.items():\n",
    "        for t, metrics in k_values.items():\n",
    "            if metric_name in metrics:\n",
    "                statistics = calculate_statistics(metrics)\n",
    "                metric_value = statistics[metric_name]['Média']\n",
    "                \n",
    "                if (metric_name in ['MAE', 'Median Absolute Error', 'RMSE'] and metric_value < best_value) or \\\n",
    "                   (metric_name == 'R2 Score' and metric_value > best_value):\n",
    "                    best_value = metric_value\n",
    "                    best_d_n = (d, t, statistics)\n",
    "    \n",
    "    return best_d_n\n",
    "\n",
    "# Metrics we are interested in\n",
    "metrics_of_interest = ['MAE', 'Median Absolute Error', 'RMSE', 'R2 Score']\n",
    "\n",
    "# Iterate through the datasets and print statistics for the best t and d for each metric\n",
    "for dataset_name, d_values in datasets.items():\n",
    "    print(f\"Melhores resultados para o dataset: {dataset_name}\")\n",
    "    for metric_name in metrics_of_interest:\n",
    "        best_d_n = find_best_d_t_for_metric(dataset_name, d_values, metric_name)\n",
    "        if best_d_n:\n",
    "            d, t, statistics = best_d_n\n",
    "            print(f\"  Melhor para a métrica {metric_name}: d = {d}, t = {t}\")\n",
    "            print(f\"    Média: {statistics[metric_name]['Média']:.4f}\")\n",
    "            print(f\"    Mínimo: {statistics[metric_name]['Mínimo']:.4f}\")\n",
    "            print(f\"    Máximo: {statistics[metric_name]['Máximo']:.4f}\")\n",
    "            print(f\"    Desvio Padrão: {statistics[metric_name]['Desvio Padrão']:.4f}\")\n",
    "            print()  # Add a new line for better readability\n",
    "\n",
    "#print(\"Número de iterações das métricas: \" + str(cont))\n",
    "\n",
    "sys.stdout.close()\n",
    "sys.stderr.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f51b9e-2b5f-4ae4-b97f-2dc6cb647032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import arff\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, r2_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "def calculate_metrics(true_values, predicted_values):\n",
    "    mae = mean_absolute_error(true_values, predicted_values)\n",
    "    medae = median_absolute_error(true_values, predicted_values)\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "    r2 = r2_score(true_values, predicted_values)\n",
    "    return mae, medae, rmse, r2\n",
    "\n",
    "def load_arff(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = arff.load(f)\n",
    "    X = np.array(data['data'])[:, :-1]\n",
    "    y = np.array(data['data'])[:, -1].astype(float)\n",
    "    return X, y\n",
    "\n",
    "output_file = \"../results/rf/sem-pre/output_rf_2_semente.txt\"\n",
    "sys.stdout = open(output_file, \"w\")\n",
    "sys.stderr = open(output_file, \"a\")\n",
    "\n",
    "max_depth_values = range(5, 11)\n",
    "n_estimators_values = [50, 100, 150, 200, 250, 300]\n",
    "\n",
    "metrics_data = []\n",
    "\n",
    "num_runs = 30\n",
    "\n",
    "for run in range(num_runs):\n",
    "    for file_path in file_paths:\n",
    "        X, y = load_arff(file_path)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        for max_depth in max_depth_values:\n",
    "            for n_estimators in n_estimators_values:\n",
    "                rf = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, criterion='squared_error', min_samples_split=2, min_samples_leaf=2)\n",
    "                rf.fit(X_train, y_train)\n",
    "                y_pred = rf.predict(X_test)\n",
    "                mae, medae, rmse, r2 = calculate_metrics(y_test, y_pred)\n",
    "                metrics_data.append([file_path, \"MAE\", mae, max_depth, n_estimators])\n",
    "                metrics_data.append([file_path, \"Median Absolute Error\", medae, max_depth, n_estimators])\n",
    "                metrics_data.append([file_path, \"RMSE\", rmse, max_depth, n_estimators])\n",
    "                metrics_data.append([file_path, \"R2 Score\", r2, max_depth, n_estimators])\n",
    "\n",
    "headers = [\"Dataset\", \"Metric\", \"metric valor\", \"d\", \"t\"]\n",
    "print(tabulate(metrics_data, headers=headers))\n",
    "print('\\n')\n",
    "sys.stdout.close()\n",
    "sys.stderr.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b2ccb2-4f4f-4c0d-9d34-3e8a439f800a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
