{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22953c5-4aa3-4e82-a6ef-267b11be81cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53fd41a7-5fb9-47a5-bb1b-75ac9f67973b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.10.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: /home/leonardodev/.pyenv/versions/3.8.14/lib/python3.8/site-packages\n",
      "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, keras-preprocessing, libclang, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a7ad9de-a61a-499b-9191-bf485d1eb9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras==2.10.0 in /home/leonardodev/.pyenv/versions/3.8.14/lib/python3.8/site-packages (2.10.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/home/leonardodev/.pyenv/versions/3.8.14/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98cd614a-4f66-4f0b-815d-3773d6e0159e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.0.1 in /home/leonardodev/.pyenv/versions/3.8.14/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/leonardodev/.pyenv/versions/3.8.14/lib/python3.8/site-packages (from scikit-learn==1.0.1) (1.4.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/leonardodev/.pyenv/versions/3.8.14/lib/python3.8/site-packages (from scikit-learn==1.0.1) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/leonardodev/.pyenv/versions/3.8.14/lib/python3.8/site-packages (from scikit-learn==1.0.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /home/leonardodev/.pyenv/versions/3.8.14/lib/python3.8/site-packages (from scikit-learn==1.0.1) (1.24.3)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/home/leonardodev/.pyenv/versions/3.8.14/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn==1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc351ec4-3ae6-416f-b089-1a34e2cc3c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: scikit-learn\n",
      "Version: 1.0.1\n",
      "Summary: A set of python modules for machine learning and data mining\n",
      "Home-page: http://scikit-learn.org\n",
      "Author: \n",
      "Author-email: \n",
      "License: new BSD\n",
      "Location: /home/leonardodev/.pyenv/versions/3.8.14/lib/python3.8/site-packages\n",
      "Requires: joblib, numpy, scipy, threadpoolctl\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f962fb4-9905-4e6b-9006-061a836ad112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import arff\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84977f27-873e-4928-874f-af92676de1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    '../datasets_2/albrecht.arff',\n",
    "    '../datasets_2/kemerer.arff',\n",
    "    '../datasets_2/cocomo81.arff',\n",
    "    #'../datasets_2/desharnais.arff',\n",
    "    '../datasets_2/china.arff',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da66ecb1-4843-4579-9ae8-1091e9b6cd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Analizando o arquivo: ../datasets_2/albrecht.arff\n",
      "Atributos: ['Input', 'Output', 'Inquiry', 'File', 'FPAdj', 'RawFPcounts', 'AdjFP', 'Effort']\n",
      "Número de instâncias: 24\n",
      "Primeira instância: [25.0, 150.0, 75.0, 60.0, 1.0, 1750.0, 1750.0, 102.4]\n",
      "Última instância: [12.0, 15.0, 0.0, 15.0, 0.95, 273.68, 260.0, 6.1]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Analizando o arquivo: ../datasets_2/kemerer.arff\n",
      "Atributos: ['ID', 'Language', 'Hardware', 'Duration', 'KSLOC', 'AdjFP', 'RAWFP', 'EffortMM']\n",
      "Número de instâncias: 15\n",
      "Primeira instância: [1.0, 1.0, 1.0, 17.0, 253.6, 1217.1, 1010.0, 287.0]\n",
      "Última instância: [15.0, 3.0, 1.0, 14.0, 60.2, 1044.3, 976.0, 69.9]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Analizando o arquivo: ../datasets_2/cocomo81.arff\n",
      "Atributos: ['rely', 'data', 'cplx', 'time', 'stor', 'virt', 'turn', 'acap', 'aexp', 'pcap', 'vexp', 'lexp', 'modp', 'tool', 'sced', 'loc', 'actual']\n",
      "Número de instâncias: 63\n",
      "Primeira instância: [0.88, 1.16, 0.7, 1.0, 1.06, 1.15, 1.07, 1.19, 1.13, 1.17, 1.1, 1.0, 1.24, 1.1, 1.04, 113.0, 2040.0]\n",
      "Última instância: [1.0, 0.94, 1.15, 1.0, 1.0, 1.0, 0.87, 0.71, 0.82, 0.86, 1.0, 1.0, 0.82, 1.0, 1.0, 10.0, 15.0]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Analizando o arquivo: ../datasets_2/china.arff\n",
      "Atributos: ['ID', 'AFP', 'Input', 'Output', 'Enquiry', 'File', 'Interface', 'Added', 'Changed', 'Deleted', 'PDR_AFP', 'PDR_UFP', 'NPDR_AFP', 'NPDU_UFP', 'Resource', 'Dev.Type', 'Duration', 'N_effort', 'Effort']\n",
      "Número de instâncias: 499\n",
      "Primeira instância: [1.0, 1587.0, 774.0, 260.0, 340.0, 128.0, 0.0, 1502.0, 0.0, 0.0, 4.7, 5.0, 4.7, 5.0, 4.0, 0.0, 4.0, 7490.0, 7490.0]\n",
      "Última instância: [499.0, 213.0, 123.0, 91.0, 28.0, 0.0, 0.0, 36.0, 206.0, 0.0, 10.3, 9.0, 10.3, 9.0, 1.0, 0.0, 7.0, 2185.0, 2185.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Para cada arquivo ARFF\n",
    "for file_path in file_paths:\n",
    "    print(f\"\\n\\n\\nAnalizando o arquivo: {file_path}\")\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = arff.load(f)\n",
    "\n",
    "    # Exibir algumas informações sobre o arquivo ARFF\n",
    "    print(\"Atributos:\", [attr[0] for attr in data['attributes']])\n",
    "    print(\"Número de instâncias:\", len(data['data']))\n",
    "    print(\"Primeira instância:\", data['data'][0])\n",
    "    print(\"Última instância:\", data['data'][-1])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584ca121-52de-4f8f-af5d-e3eabdb43e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arff\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tabulate import tabulate\n",
    "import sys\n",
    "\n",
    "def calculate_metrics(true_values, predicted_values):\n",
    "    mae = mean_absolute_error(true_values, predicted_values)\n",
    "    medae = median_absolute_error(true_values, predicted_values)\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "    r2 = r2_score(true_values, predicted_values)\n",
    "    \n",
    "    return mae, medae, rmse, r2\n",
    "\n",
    "# def load_arff(file_path):\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         data = arff.load(f)\n",
    "\n",
    "#     attributes = [attr[0] for attr in data['attributes']]\n",
    "#     X = np.array(data['data'])[:, :-1]\n",
    "#     y = np.array(data['data'])[:, -1].astype(float)\n",
    "    \n",
    "#     return X, y\n",
    "\n",
    "def load_and_normalize_arff(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = arff.load(f)\n",
    "\n",
    "    attributes = [attr[0] for attr in data['attributes']]\n",
    "    X = np.array(data['data'])[:, :-1]\n",
    "    y = np.array(data['data'])[:, -1].astype(float)\n",
    "    \n",
    "    # Normalização Min-Max\n",
    "    scaler = MinMaxScaler()\n",
    "    X_normalized = scaler.fit_transform(X)\n",
    "    \n",
    "    return X_normalized, y\n",
    "\n",
    "output_file = \"../results/knn/sem-pre/output_knn_2_kp.txt\"\n",
    "sys.stdout = open(output_file, \"w\")\n",
    "sys.stderr = open(output_file, \"a\")\n",
    "\n",
    "metrics_data = []\n",
    "\n",
    "n_neighbors = 5  \n",
    "leaf_size = 30  \n",
    "weights = 'uniform'  \n",
    "p_values = [1, 2] \n",
    "\n",
    "k_values = range(3, 10)\n",
    "\n",
    "num_runs = 30\n",
    "\n",
    "for run in range(num_runs):\n",
    "    for file_path in file_paths:\n",
    "        #X, y = load_arff(file_path)\n",
    "        X, y = load_and_normalize_arff(file_path)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "        for k in k_values:\n",
    "            for p in p_values:\n",
    "                if p == 1:\n",
    "                    distance_metric = 'manhattan'\n",
    "                elif p == 2:\n",
    "                    distance_metric = 'euclidean'\n",
    "                else:\n",
    "                    distance_metric = 'minkowski'\n",
    "                    \n",
    "                #for _ in range(5):\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "                knn = KNeighborsRegressor(n_neighbors=k, weights=weights, p=p, leaf_size=leaf_size, metric=distance_metric, random_oqiasdfn)\n",
    "                knn.fit(X_train, y_train)\n",
    "                y_pred = knn.predict(X_test)\n",
    "                mae, medae, rmse, r2 = calculate_metrics(y_test, y_pred)\n",
    "                metrics_data.append([file_path, \"MAE\", mae,p,k])\n",
    "                metrics_data.append([file_path, \"Median Absolute Error\", medae,p,k])\n",
    "                metrics_data.append([file_path, \"RMSE\", rmse,p,k])\n",
    "                metrics_data.append([file_path, \"R2 Score\", r2,p,k])\n",
    "\n",
    "headers = [\"Dataset\", \"Metric\", \"metric valor\", \"p\", \"k\"]\n",
    "print(tabulate(metrics_data, headers=headers))\n",
    "\n",
    "sys.stdout.close()\n",
    "sys.stderr.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbb9493-cc50-40fd-96b5-e031120d02dc",
   "metadata": {},
   "source": [
    "# Mostra a melhor disposição de K e P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd37a989-6412-4c50-8da0-7faf9b193332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Redirecting stdout and stderr to the specified file\n",
    "output_file = \"../results/knn/sem-pre/output_knn_analises_p_k_best_semente.txt\"\n",
    "sys.stdout = open(output_file, \"w\")\n",
    "sys.stderr = open(output_file, \"a\")\n",
    "\n",
    "cont = 0\n",
    "\n",
    "# Dictionary to store metrics with nested structure for datasets, p, k, and metrics\n",
    "datasets = {\n",
    "    \"albrecht\": {},\n",
    "    \"kemerer\": {},\n",
    "    \"cocomo81\": {},\n",
    "    \"china\": {}\n",
    "}\n",
    "\n",
    "def extract_metrics_values(filename):\n",
    "    global cont\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            if not line.strip() or line.startswith('-'):\n",
    "                continue\n",
    "            \n",
    "            # Updated regex to capture p and k values\n",
    "            match = re.match(r'.*?([A-Za-z0-9_/.]+)\\s+(.*?)\\s+([\\d.]+)\\s+(\\d+)\\s+(\\d+)', line)\n",
    "            if match:\n",
    "                dataset, metric, value, p, k = match.groups()\n",
    "                value = float(value)\n",
    "                p = int(p)\n",
    "                k = int(k)\n",
    "                \n",
    "                if \"albrecht.arff\" in dataset:\n",
    "                    dataset_key = \"albrecht\"\n",
    "                elif \"kemerer.arff\" in dataset:\n",
    "                    dataset_key = \"kemerer\"\n",
    "                elif \"cocomo81.arff\" in dataset:\n",
    "                    dataset_key = \"cocomo81\"\n",
    "                elif \"china.arff\" in dataset:\n",
    "                    dataset_key = \"china\"\n",
    "                else:\n",
    "                    print(f\"Dataset not recognized: {dataset}\", file=sys.stderr)\n",
    "                    continue\n",
    "\n",
    "                # Initialize nested dictionaries if not already done\n",
    "                if p not in datasets[dataset_key]:\n",
    "                    datasets[dataset_key][p] = {}\n",
    "                if k not in datasets[dataset_key][p]:\n",
    "                    datasets[dataset_key][p][k] = {}\n",
    "                if metric not in datasets[dataset_key][p][k]:\n",
    "                    datasets[dataset_key][p][k][metric] = []\n",
    "\n",
    "                # Append the metric value\n",
    "                datasets[dataset_key][p][k][metric].append(value)\n",
    "                cont += 1\n",
    "            # else:\n",
    "            #     print(f\"Line did not match: {line.strip()}\", file=sys.stderr)\n",
    "\n",
    "def calculate_statistics(metrics):\n",
    "    results = {}\n",
    "    for metric, values in metrics.items():\n",
    "        results[metric] = {\n",
    "            'Média': np.mean(values),\n",
    "            'Mínimo': np.min(values),\n",
    "            'Máximo': np.max(values),\n",
    "            'Desvio Padrão': np.std(values)\n",
    "        }\n",
    "    return results\n",
    "\n",
    "filename = '../results/knn/sem-pre/output_knn_2_kp_semente_42.txt'\n",
    "extract_metrics_values(filename)\n",
    "\n",
    "# Function to find the best k and p for each metric\n",
    "def find_best_k_p_for_metric(dataset_name, p_values, metric_name):\n",
    "    best_k_p = None\n",
    "    best_value = float('inf') if metric_name in ['MAE', 'Median Absolute Error', 'RMSE'] else float('-inf')\n",
    "    \n",
    "    for p, k_values in p_values.items():\n",
    "        for k, metrics in k_values.items():\n",
    "            if metric_name in metrics:\n",
    "                statistics = calculate_statistics(metrics)\n",
    "                metric_value = statistics[metric_name]['Média']\n",
    "                \n",
    "                if (metric_name in ['MAE', 'Median Absolute Error', 'RMSE'] and metric_value < best_value) or \\\n",
    "                   (metric_name == 'R2 Score' and metric_value > best_value):\n",
    "                    best_value = metric_value\n",
    "                    best_k_p = (p, k, statistics)\n",
    "    \n",
    "    return best_k_p\n",
    "\n",
    "# Metrics we are interested in\n",
    "metrics_of_interest = ['MAE', 'Median Absolute Error', 'RMSE', 'R2 Score']\n",
    "\n",
    "# Iterate through the datasets and print statistics for the best k and p for each metric\n",
    "for dataset_name, p_values in datasets.items():\n",
    "    print(f\"Melhores resultados para o dataset: {dataset_name}\")\n",
    "    for metric_name in metrics_of_interest:\n",
    "        best_k_p = find_best_k_p_for_metric(dataset_name, p_values, metric_name)\n",
    "        if best_k_p:\n",
    "            p, k, statistics = best_k_p\n",
    "            print(f\"  Melhor para a métrica {metric_name}: p = {p}, k = {k}\")\n",
    "            print(f\"    Média: {statistics[metric_name]['Média']:.4f}\")\n",
    "            print(f\"    Mínimo: {statistics[metric_name]['Mínimo']:.4f}\")\n",
    "            print(f\"    Máximo: {statistics[metric_name]['Máximo']:.4f}\")\n",
    "            print(f\"    Desvio Padrão: {statistics[metric_name]['Desvio Padrão']:.4f}\")\n",
    "            print()  # Add a new line for better readability\n",
    "\n",
    "#print(\"Número de iterações das métricas: \" + str(cont))\n",
    "\n",
    "sys.stdout.close()\n",
    "sys.stderr.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce91caf-977a-46cd-a24f-74bad37e6884",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a863b64a-1e9f-4098-b2e4-581cb25baaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arff\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tabulate import tabulate\n",
    "import sys\n",
    "\n",
    "def calculate_metrics(true_values, predicted_values):\n",
    "    mae = mean_absolute_error(true_values, predicted_values)\n",
    "    medae = median_absolute_error(true_values, predicted_values)\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "    r2 = r2_score(true_values, predicted_values)\n",
    "    \n",
    "    return mae, medae, rmse, r2\n",
    "\n",
    "# def load_arff(file_path):\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         data = arff.load(f)\n",
    "\n",
    "#     attributes = [attr[0] for attr in data['attributes']]\n",
    "#     X = np.array(data['data'])[:, :-1]\n",
    "#     y = np.array(data['data'])[:, -1].astype(float)\n",
    "    \n",
    "#     return X, y\n",
    "\n",
    "def load_and_normalize_arff(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = arff.load(f)\n",
    "\n",
    "    attributes = [attr[0] for attr in data['attributes']]\n",
    "    X = np.array(data['data'])[:, :-1]\n",
    "    y = np.array(data['data'])[:, -1].astype(float)\n",
    "    \n",
    "    # Normalização Min-Max\n",
    "    scaler = MinMaxScaler()\n",
    "    X_normalized = scaler.fit_transform(X)\n",
    "    \n",
    "    return X_normalized, y\n",
    "\n",
    "output_file = \"../results/knn/sem-pre/output_knn_2_kp_semente_42.txt\"\n",
    "sys.stdout = open(output_file, \"w\")\n",
    "sys.stderr = open(output_file, \"a\")\n",
    "\n",
    "metrics_data = []\n",
    "\n",
    "n_neighbors = 5  \n",
    "leaf_size = 30  \n",
    "weights = 'uniform'  \n",
    "p_values = [1, 2] \n",
    "\n",
    "k_values = range(3, 10)\n",
    "\n",
    "num_runs = 30\n",
    "\n",
    "for run in range(num_runs):\n",
    "    for file_path in file_paths:\n",
    "        #X, y = load_arff(file_path)\n",
    "        X, y = load_and_normalize_arff(file_path)\n",
    "\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "        for k in k_values:\n",
    "            for p in p_values:\n",
    "                if p == 1:\n",
    "                    distance_metric = 'manhattan'\n",
    "                elif p == 2:\n",
    "                    distance_metric = 'euclidean'\n",
    "                else:\n",
    "                    distance_metric = 'minkowski'\n",
    "                    \n",
    "                #for _ in range(5):\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                knn = KNeighborsRegressor(n_neighbors=k, weights=weights, p=p, leaf_size=leaf_size, metric=distance_metric)\n",
    "                knn.fit(X_train, y_train)\n",
    "                y_pred = knn.predict(X_test)\n",
    "                mae, medae, rmse, r2 = calculate_metrics(y_test, y_pred)\n",
    "                metrics_data.append([file_path, \"MAE\", mae,p,k])\n",
    "                metrics_data.append([file_path, \"Median Absolute Error\", medae,p,k])\n",
    "                metrics_data.append([file_path, \"RMSE\", rmse,p,k])\n",
    "                metrics_data.append([file_path, \"R2 Score\", r2,p,k])\n",
    "\n",
    "headers = [\"Dataset\", \"Metric\", \"metric valor\", \"p\", \"k\"]\n",
    "print(tabulate(metrics_data, headers=headers))\n",
    "\n",
    "sys.stdout.close()\n",
    "sys.stderr.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd15bf21-aad2-41c3-83b5-3972ea0faf2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05881b02-1db8-4b1e-80cc-a0ead273fd4e",
   "metadata": {},
   "source": [
    "# Testando novas fazes de pré-processamento para ver o resultado das métricas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c99f6e8-8623-4b55-ac3d-daa6f303db63",
   "metadata": {},
   "source": [
    "## KBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d76b7b1-b432-4f5f-883f-c7f8f7fdf1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arff\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, r2_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "def calculate_metrics(true_values, predicted_values):\n",
    "    mae = mean_absolute_error(true_values, predicted_values)\n",
    "    medae = median_absolute_error(true_values, predicted_values)\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "    r2 = r2_score(true_values, predicted_values)\n",
    "    \n",
    "    return mae, medae, rmse, r2\n",
    "\n",
    "def load_arff(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = arff.load(f)\n",
    "\n",
    "    attributes = [attr[0] for attr in data['attributes']]\n",
    "    X = np.array(data['data'])[:, :-1]\n",
    "    y = np.array(data['data'])[:, -1].astype(float)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "output_file = \"../results/knn/com-pre/output_knn_pre_SKB.txt\"\n",
    "sys.stdout = open(output_file, \"w\")\n",
    "sys.stderr = open(output_file, \"a\")\n",
    "\n",
    "metrics_data = []\n",
    "\n",
    "n_neighbors = 5  \n",
    "leaf_size = 30  \n",
    "weights = 'uniform'  \n",
    "p_values = [1, 2]  # Power parameter for Minkowski distance (Euclidean distance)\n",
    "\n",
    "k_values = range(3, 10)\n",
    "\n",
    "num_runs = 30\n",
    "\n",
    "for run in range(num_runs):\n",
    "    for file_path in file_paths:\n",
    "        X, y = load_arff(file_path)\n",
    "        \n",
    "        selector = SelectKBest(score_func=f_regression, k=4)  \n",
    "        X_new = selector.fit_transform(X, y)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2)\n",
    "    \n",
    "        for k in k_values:\n",
    "            for p in p_values:\n",
    "                knn = KNeighborsRegressor(n_neighbors=k, weights=weights, p=p, leaf_size=leaf_size)\n",
    "                knn.fit(X_train, y_train)\n",
    "                y_pred = knn.predict(X_test)\n",
    "                mae, medae, rmse, r2 = calculate_metrics(y_test, y_pred)\n",
    "                metrics_data.append([file_path, \"MAE\", mae, p, k])\n",
    "                metrics_data.append([file_path, \"Median Absolute Error\", medae, p, k])\n",
    "                metrics_data.append([file_path, \"RMSE\", rmse, p, k])\n",
    "                metrics_data.append([file_path, \"R2 Score\", r2, p, k])\n",
    "\n",
    "headers = [\"Dataset\", \"Metric\", \"metric valor\", \"p\", \"k\"]\n",
    "print(tabulate(metrics_data, headers=headers))\n",
    "\n",
    "sys.stdout.close()\n",
    "sys.stderr.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ad85ae-910c-49dc-b897-c429213099e7",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c54b6bb-5c18-42bb-a460-b7ed224862fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arff\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, r2_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "def calculate_metrics(true_values, predicted_values):\n",
    "    mae = mean_absolute_error(true_values, predicted_values)\n",
    "    medae = median_absolute_error(true_values, predicted_values)\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "    r2 = r2_score(true_values, predicted_values)\n",
    "    \n",
    "    return mae, medae, rmse, r2\n",
    "\n",
    "def load_arff(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = arff.load(f)\n",
    "\n",
    "    attributes = [attr[0] for attr in data['attributes']]\n",
    "    X = np.array(data['data'])[:, :-1]\n",
    "    y = np.array(data['data'])[:, -1].astype(float)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "output_file = \"../results/knn/com-pre/output_knn_pre_PCA.txt\"\n",
    "sys.stdout = open(output_file, \"w\")\n",
    "sys.stderr = open(output_file, \"a\")\n",
    "\n",
    "metrics_data = []\n",
    "\n",
    "n_neighbors = 5  \n",
    "leaf_size = 30  \n",
    "weights = 'uniform'  \n",
    "p_values = [1, 2]  # Power parameter for Minkowski distance (Euclidean distance)\n",
    "\n",
    "k_values = range(3, 10)\n",
    "\n",
    "num_runs = 30\n",
    "\n",
    "for run in range(num_runs):\n",
    "    for file_path in file_paths:\n",
    "        X, y = load_arff(file_path)\n",
    "        \n",
    "        pca = PCA(n_components=4) \n",
    "        X_new = pca.fit_transform(X)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2)\n",
    "    \n",
    "        for k in k_values:\n",
    "            for p in p_values:\n",
    "                knn = KNeighborsRegressor(n_neighbors=k, weights=weights, p=p, leaf_size=leaf_size)\n",
    "                knn.fit(X_train, y_train)\n",
    "                y_pred = knn.predict(X_test)\n",
    "                mae, medae, rmse, r2 = calculate_metrics(y_test, y_pred)\n",
    "                metrics_data.append([file_path, \"MAE\", mae, p, k])\n",
    "                metrics_data.append([file_path, \"Median Absolute Error\", medae, p, k])\n",
    "                metrics_data.append([file_path, \"RMSE\", rmse, p, k])\n",
    "                metrics_data.append([file_path, \"R2 Score\", r2, p, k])\n",
    "\n",
    "headers = [\"Dataset\", \"Metric\", \"metric valor\", \"p\", \"k\"]\n",
    "print(tabulate(metrics_data, headers=headers))\n",
    "\n",
    "sys.stdout.close()\n",
    "sys.stderr.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de89e598-b2ed-442f-9936-70ae2f921401",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlxtend'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, median_absolute_error, mean_squared_error, r2_score\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtabulate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tabulate\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlxtend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SequentialFeatureSelector \u001b[38;5;28;01mas\u001b[39;00m SFS\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_metrics\u001b[39m(true_values, predicted_values):\n\u001b[1;32m     11\u001b[0m     mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(true_values, predicted_values)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mlxtend'"
     ]
    }
   ],
   "source": [
    "import arff\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, r2_score\n",
    "from tabulate import tabulate\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "def calculate_metrics(true_values, predicted_values):\n",
    "    mae = mean_absolute_error(true_values, predicted_values)\n",
    "    medae = median_absolute_error(true_values, predicted_values)\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "    r2 = r2_score(true_values, predicted_values)\n",
    "    \n",
    "    return mae, medae, rmse, r2\n",
    "\n",
    "def load_arff(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = arff.load(f)\n",
    "\n",
    "    attributes = [attr[0] for attr in data['attributes']]\n",
    "    X = np.array(data['data'])[:, :-1]\n",
    "    y = np.array(data['data'])[:, -1].astype(float)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "output_file = \"../results/knn/com-pre/output_knn_pre_Fo.txt\"\n",
    "sys.stdout = open(output_file, \"w\")\n",
    "sys.stderr = open(output_file, \"a\")\n",
    "\n",
    "metrics_data = []\n",
    "\n",
    "n_neighbors = 5  \n",
    "leaf_size = 30  \n",
    "weights = 'uniform'  \n",
    "p_values = [1, 2]  # Power parameter for Minkowski distance (Euclidean distance)\n",
    "\n",
    "k_values = range(3, 10)\n",
    "\n",
    "num_runs = 30\n",
    "\n",
    "for run in range(num_runs):\n",
    "    for file_path in file_paths:\n",
    "        X, y = load_arff(file_path)\n",
    "    \n",
    "        for k in k_values:\n",
    "            for p in p_values:\n",
    "                knn_temp = KNeighborsRegressor(n_neighbors=n_neighbors, weights=weights, p=2, leaf_size=leaf_size)\n",
    "                sfs = SFS(knn_temp, \n",
    "                          k_features=4,  \n",
    "                          forward=True, \n",
    "                          floating=False, \n",
    "                          scoring='r2', \n",
    "                          cv=5)\n",
    "                \n",
    "                sfs = sfs.fit(X, y)\n",
    "                X_new = sfs.transform(X)\n",
    "                \n",
    "                X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2)\n",
    "                knn = KNeighborsRegressor(n_neighbors=k, weights=weights, p=p, leaf_size=leaf_size)\n",
    "                knn.fit(X_train, y_train)\n",
    "                y_pred = knn.predict(X_test)\n",
    "                mae, medae, rmse, r2 = calculate_metrics(y_test, y_pred)\n",
    "                metrics_data.append([file_path, \"MAE\", mae, p, k])\n",
    "                metrics_data.append([file_path, \"Median Absolute Error\", medae, p, k])\n",
    "                metrics_data.append([file_path, \"RMSE\", rmse, p, k])\n",
    "                metrics_data.append([file_path, \"R2 Score\", r2, p, k])\n",
    "\n",
    "headers = [\"Dataset\", \"Metric\", \"metric valor\", \"p\", \"k\"]\n",
    "print(tabulate(metrics_data, headers=headers))\n",
    "\n",
    "sys.stdout.close()\n",
    "sys.stderr.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd617004-913d-44b5-a2b5-ef34194aa87b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0987328-3898-427a-9803-9e57b2d6bf59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12655e7-b727-4c62-9d29-6197422e1232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7723ec32-6ed3-410e-876c-2e0767e22952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Redirecting stdout and stderr to the specified file\n",
    "output_file = \"../results/knn/com-pre/output_knn_pre_PCA_analise.txt\"\n",
    "sys.stdout = open(output_file, \"w\")\n",
    "sys.stderr = open(output_file, \"a\")\n",
    "\n",
    "cont = 0\n",
    "\n",
    "# Dictionary to store metrics with nested structure for datasets, p, k, and metrics\n",
    "datasets = {\n",
    "    \"albrecht\": {},\n",
    "    \"kemerer\": {},\n",
    "    \"cocomo81\": {},\n",
    "    \"china\": {}\n",
    "}\n",
    "\n",
    "def extract_metrics_values(filename):\n",
    "    global cont\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            if not line.strip() or line.startswith('-'):\n",
    "                continue\n",
    "            \n",
    "            # Updated regex to capture p and k values\n",
    "            match = re.match(r'.*?([A-Za-z0-9_/.]+)\\s+(.*?)\\s+([\\d.]+)\\s+(\\d+)\\s+(\\d+)', line)\n",
    "            if match:\n",
    "                dataset, metric, value, p, k = match.groups()\n",
    "                value = float(value)\n",
    "                p = int(p)\n",
    "                k = int(k)\n",
    "                \n",
    "                if \"albrecht.arff\" in dataset:\n",
    "                    dataset_key = \"albrecht\"\n",
    "                elif \"kemerer.arff\" in dataset:\n",
    "                    dataset_key = \"kemerer\"\n",
    "                elif \"cocomo81.arff\" in dataset:\n",
    "                    dataset_key = \"cocomo81\"\n",
    "                elif \"china.arff\" in dataset:\n",
    "                    dataset_key = \"china\"\n",
    "                else:\n",
    "                    print(f\"Dataset not recognized: {dataset}\", file=sys.stderr)\n",
    "                    continue\n",
    "\n",
    "                # Initialize nested dictionaries if not already done\n",
    "                if p not in datasets[dataset_key]:\n",
    "                    datasets[dataset_key][p] = {}\n",
    "                if k not in datasets[dataset_key][p]:\n",
    "                    datasets[dataset_key][p][k] = {}\n",
    "                if metric not in datasets[dataset_key][p][k]:\n",
    "                    datasets[dataset_key][p][k][metric] = []\n",
    "\n",
    "                # Append the metric value\n",
    "                datasets[dataset_key][p][k][metric].append(value)\n",
    "                cont += 1\n",
    "            # else:\n",
    "            #     print(f\"Line did not match: {line.strip()}\", file=sys.stderr)\n",
    "\n",
    "def calculate_statistics(metrics):\n",
    "    results = {}\n",
    "    for metric, values in metrics.items():\n",
    "        results[metric] = {\n",
    "            'Média': np.mean(values),\n",
    "            'Mínimo': np.min(values),\n",
    "            'Máximo': np.max(values),\n",
    "            'Desvio Padrão': np.std(values)\n",
    "        }\n",
    "    return results\n",
    "\n",
    "filename = '../results/knn/com-pre/output_knn_pre_PCA.txt'\n",
    "extract_metrics_values(filename)\n",
    "\n",
    "# Function to find the best k and p for each metric\n",
    "def find_best_k_p_for_metric(dataset_name, p_values, metric_name):\n",
    "    best_k_p = None\n",
    "    best_value = float('inf') if metric_name in ['MAE', 'Median Absolute Error', 'RMSE'] else float('-inf')\n",
    "    \n",
    "    for p, k_values in p_values.items():\n",
    "        for k, metrics in k_values.items():\n",
    "            if metric_name in metrics:\n",
    "                statistics = calculate_statistics(metrics)\n",
    "                metric_value = statistics[metric_name]['Média']\n",
    "                \n",
    "                if (metric_name in ['MAE', 'Median Absolute Error', 'RMSE'] and metric_value < best_value) or \\\n",
    "                   (metric_name == 'R2 Score' and metric_value > best_value):\n",
    "                    best_value = metric_value\n",
    "                    best_k_p = (p, k, statistics)\n",
    "    \n",
    "    return best_k_p\n",
    "\n",
    "# Metrics we are interested in\n",
    "metrics_of_interest = ['MAE', 'Median Absolute Error', 'RMSE', 'R2 Score']\n",
    "\n",
    "# Iterate through the datasets and print statistics for the best k and p for each metric\n",
    "for dataset_name, p_values in datasets.items():\n",
    "    print(f\"Melhores resultados para o dataset: {dataset_name}\")\n",
    "    for metric_name in metrics_of_interest:\n",
    "        best_k_p = find_best_k_p_for_metric(dataset_name, p_values, metric_name)\n",
    "        if best_k_p:\n",
    "            p, k, statistics = best_k_p\n",
    "            print(f\"  Melhor para a métrica {metric_name}: p = {p}, k = {k}\")\n",
    "            print(f\"    Média: {statistics[metric_name]['Média']:.4f}\")\n",
    "            print(f\"    Mínimo: {statistics[metric_name]['Mínimo']:.4f}\")\n",
    "            print(f\"    Máximo: {statistics[metric_name]['Máximo']:.4f}\")\n",
    "            print(f\"    Desvio Padrão: {statistics[metric_name]['Desvio Padrão']:.4f}\")\n",
    "            print()  # Add a new line for better readability\n",
    "\n",
    "#print(\"Número de iterações das métricas: \" + str(cont))\n",
    "\n",
    "sys.stdout.close()\n",
    "sys.stderr.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d68f50-10b1-414d-b7f8-ef16d81cf1df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
