{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb14a4a0-8db5-4256-ab15-61c0c0531468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import arff\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "file_paths = [\n",
    "    '../datasets_2/albrecht.arff',\n",
    "    '../datasets_2/kemerer.arff',\n",
    "    '../datasets_2/cocomo81.arff',\n",
    "    #'../datasets_2/desharnais.arff',\n",
    "    '../datasets_2/china.arff',\n",
    "]\n",
    "\n",
    "# Para cada arquivo ARFF\n",
    "for file_path in file_paths:\n",
    "    print(f\"\\n\\n\\nAnalizando o arquivo: {file_path}\")\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = arff.load(f)\n",
    "\n",
    "    # Exibir algumas informações sobre o arquivo ARFF\n",
    "    print(\"Atributos:\", [attr[0] for attr in data['attributes']])\n",
    "    print(\"Número de instâncias:\", len(data['data']))\n",
    "    print(\"Primeira instância:\", data['data'][0])\n",
    "    print(\"Última instância:\", data['data'][-1])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a79714-520b-406e-9372-6f921d9f5765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import arff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, r2_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "def calculate_metrics(true_values, predicted_values):\n",
    "    mae = mean_absolute_error(true_values, predicted_values)\n",
    "    medae = median_absolute_error(true_values, predicted_values)\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "    r2 = r2_score(true_values, predicted_values)\n",
    "    \n",
    "    return mae, medae, rmse, r2\n",
    "\n",
    "def load_arff(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = arff.load(f)\n",
    "\n",
    "    attributes = [attr[0] for attr in data['attributes']]\n",
    "    X = np.array(data['data'])[:, :-1]\n",
    "    y = np.array(data['data'])[:, -1].astype(float)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "output_file = \"../results/lstm/sem-pre/output_lstm_2.txt\"\n",
    "sys.stdout = open(output_file, \"w\")\n",
    "sys.stderr = open(output_file, \"a\")\n",
    "\n",
    "metrics_data = []\n",
    "\n",
    "activation = 'tanh'  # Função de ativação\n",
    "recurrent_activation = 'sigmoid'  # Função de ativação recorrente\n",
    "dropout = 0.5  # Probabilidade de dropout\n",
    "optimizer = 'adam'  # Algoritmo de otimização\n",
    "\n",
    "# Valores para a sintonização de parâmetros\n",
    "epochs_values = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]  # Número de épocas\n",
    "num_neurons_values = [25, 50, 75, 100]  # Número de neurônios na camada oculta\n",
    "\n",
    "num_runs = 30\n",
    "\n",
    "for run in range(num_runs):\n",
    "    for file_path in file_paths:\n",
    "        X, y = load_arff(file_path)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "        for epochs in epochs_values:\n",
    "            for num_neurons in num_neurons_values:\n",
    "                model = Sequential()\n",
    "                model.add(LSTM(num_neurons, activation=activation, recurrent_activation=recurrent_activation, input_shape=(X_train.shape[1], 1)))\n",
    "                model.add(Dropout(dropout))\n",
    "                model.add(Dense(1))\n",
    "                model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "                X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "                X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "                model.fit(X_train_reshaped, y_train, epochs=epochs, batch_size=32, verbose=0)\n",
    "                y_pred = model.predict(X_test_reshaped)\n",
    "                mae, medae, rmse, r2 = calculate_metrics(y_test, y_pred)\n",
    "                metrics_data.append([file_path, \"MAE\", mae, epochs, num_neurons])\n",
    "                metrics_data.append([file_path, \"Median Absolute Error\", medae, epochs, num_neurons])\n",
    "                metrics_data.append([file_path, \"RMSE\", rmse, epochs, num_neurons])\n",
    "                metrics_data.append([file_path, \"R2 Score\", r2, epochs, num_neurons])\n",
    "\n",
    "table = tabulate(metrics_data, headers=[\"Dataset\", \"Metric\", \"metric valor\", \"e\", \"n\"])\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(table + '\\n')\n",
    "\n",
    "sys.stdout.close()\n",
    "sys.stderr.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f42c2a-2190-4be4-9428-4b1fc91a1734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Redirecting stdout and stderr to the specified file\n",
    "output_file = \"../results/lstm/sem-pre/output_lstm_analises_e_n_best.txt\"\n",
    "sys.stdout = open(output_file, \"w\")\n",
    "sys.stderr = open(output_file, \"a\")\n",
    "\n",
    "cont = 0\n",
    "\n",
    "# Dictionary to store metrics with nested structure for datasets, e, n, and metrics\n",
    "datasets = {\n",
    "    \"albrecht\": {},\n",
    "    \"kemerer\": {},\n",
    "    \"cocomo81\": {},\n",
    "    \"china\": {}\n",
    "}\n",
    "\n",
    "def extract_metrics_values(filename):\n",
    "    global cont\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            if not line.strip() or line.startswith('-'):\n",
    "                continue\n",
    "            \n",
    "            # Updated regex to capture e and n values\n",
    "            match = re.match(r'.*?([A-Za-z0-9_/.]+)\\s+(.*?)\\s+([\\d.]+)\\s+(\\d+)\\s+(\\d+)', line)\n",
    "            if match:\n",
    "                dataset, metric, value, e, n = match.groups()\n",
    "                value = float(value)\n",
    "                e = int(e)\n",
    "                n = int(n)\n",
    "                \n",
    "                if \"albrecht.arff\" in dataset:\n",
    "                    dataset_key = \"albrecht\"\n",
    "                elif \"kemerer.arff\" in dataset:\n",
    "                    dataset_key = \"kemerer\"\n",
    "                elif \"cocomo81.arff\" in dataset:\n",
    "                    dataset_key = \"cocomo81\"\n",
    "                elif \"china.arff\" in dataset:\n",
    "                    dataset_key = \"china\"\n",
    "                else:\n",
    "                    print(f\"Dataset not recognized: {dataset}\", file=sys.stderr)\n",
    "                    continue\n",
    "\n",
    "                # Initialize nested dictionaries if not already done\n",
    "                if e not in datasets[dataset_key]:\n",
    "                    datasets[dataset_key][e] = {}\n",
    "                if n not in datasets[dataset_key][e]:\n",
    "                    datasets[dataset_key][e][n] = {}\n",
    "                if metric not in datasets[dataset_key][e][n]:\n",
    "                    datasets[dataset_key][e][n][metric] = []\n",
    "\n",
    "                # Append the metric value\n",
    "                datasets[dataset_key][e][n][metric].append(value)\n",
    "                cont += 1\n",
    "            # else:\n",
    "            #     print(f\"Line did not match: {line.strip()}\", file=sys.stderr)\n",
    "\n",
    "def calculate_statistics(metrics):\n",
    "    results = {}\n",
    "    for metric, values in metrics.items():\n",
    "        results[metric] = {\n",
    "            'Média': np.mean(values),\n",
    "            'Mínimo': np.min(values),\n",
    "            'Máximo': np.max(values),\n",
    "            'Desvio Padrão': np.std(values)\n",
    "        }\n",
    "    return results\n",
    "\n",
    "filename = '../results/lstm/sem-pre/output_lstm.txt'\n",
    "extract_metrics_values(filename)\n",
    "\n",
    "# Function to find the best n and e for each metric\n",
    "def find_best_e_n_for_metric(dataset_name, d_values, metric_name):\n",
    "    best_e_n = None\n",
    "    best_value = float('inf') if metric_name in ['MAE', 'Median Absolute Error', 'RMSE'] else float('-inf')\n",
    "    \n",
    "    for e, k_values in d_values.items():\n",
    "        for n, metrics in k_values.items():\n",
    "            if metric_name in metrics:\n",
    "                statistics = calculate_statistics(metrics)\n",
    "                metric_value = statistics[metric_name]['Média']\n",
    "                \n",
    "                if (metric_name in ['MAE', 'Median Absolute Error', 'RMSE'] and metric_value < best_value) or \\\n",
    "                   (metric_name == 'R2 Score' and metric_value > best_value):\n",
    "                    best_value = metric_value\n",
    "                    best_e_n = (e, n, statistics)\n",
    "    \n",
    "    return best_e_n\n",
    "\n",
    "# Metrics we are interested in\n",
    "metrics_of_interest = ['MAE', 'Median Absolute Error', 'RMSE', 'R2 Score']\n",
    "\n",
    "# Iterate through the datasets and print statistics for the best n and e for each metric\n",
    "for dataset_name, d_values in datasets.items():\n",
    "    print(f\"Melhores resultados para o dataset: {dataset_name}\")\n",
    "    for metric_name in metrics_of_interest:\n",
    "        best_e_n = find_best_e_n_for_metric(dataset_name, d_values, metric_name)\n",
    "        if best_e_n:\n",
    "            e, n, statistics = best_e_n\n",
    "            print(f\"  Melhor para a métrica {metric_name}: e = {e}, n = {n}\")\n",
    "            print(f\"    Média: {statistics[metric_name]['Média']:.4f}\")\n",
    "            print(f\"    Mínimo: {statistics[metric_name]['Mínimo']:.4f}\")\n",
    "            print(f\"    Máximo: {statistics[metric_name]['Máximo']:.4f}\")\n",
    "            print(f\"    Desvio Padrão: {statistics[metric_name]['Desvio Padrão']:.4f}\")\n",
    "            print()  # Add a new line for better readability\n",
    "\n",
    "#print(\"Número de iterações das métricas: \" + str(cont))\n",
    "\n",
    "sys.stdout.close()\n",
    "sys.stderr.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb80cc53-45c7-4e72-a893-735c710c74fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
