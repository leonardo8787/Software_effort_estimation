{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54829497-c89e-46ba-9622-bd23a1af0933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import arff\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52a52525-1b8f-452e-8ef9-d45f96a340d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    '../datasets_2/albrecht.arff',\n",
    "    '../datasets_2/kemerer.arff',\n",
    "    '../datasets_2/cocomo81.arff',\n",
    "    #'../datasets_2/desharnais.arff',\n",
    "    '../datasets_2/china.arff',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8343a651-077c-455c-a45a-742ecbc8ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para cada arquivo ARFF\n",
    "for file_path in file_paths:\n",
    "    print(f\"\\n\\n\\nAnalizando o arquivo: {file_path}\")\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = arff.load(f)\n",
    "\n",
    "    # Exibir algumas informações sobre o arquivo ARFF\n",
    "    print(\"Atributos:\", [attr[0] for attr in data['attributes']])\n",
    "    print(\"Número de instâncias:\", len(data['data']))\n",
    "    print(\"Primeira instância:\", data['data'][0])\n",
    "    print(\"Última instância:\", data['data'][-1])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3a862e-532c-4b4a-ae0f-742fceade3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arff\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, r2_score\n",
    "from tabulate import tabulate\n",
    "from itertools import product\n",
    "import sys\n",
    "\n",
    "def calculate_metrics(true_values, predicted_values):\n",
    "    mae = mean_absolute_error(true_values, predicted_values)\n",
    "    medae = median_absolute_error(true_values, predicted_values)\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "    r2 = r2_score(true_values, predicted_values)\n",
    "    \n",
    "    return mae, medae, rmse, r2\n",
    "\n",
    "def load_arff(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = arff.load(f)\n",
    "\n",
    "    attributes = [attr[0] for attr in data['attributes']]\n",
    "    X = np.array(data['data'])[:, :-1]\n",
    "    y = np.array(data['data'])[:, -1].astype(float)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "metrics_data = []\n",
    "\n",
    "min_samples_split = 2  \n",
    "min_samples_leaf = 1  \n",
    "max_depth_values = range(5, 11)\n",
    "max_leaf_nodes_values = range(11, 21)\n",
    "\n",
    "output_file = \"../results/dt/sem-pre/output_dt.txt\"\n",
    "sys.stdout = open(output_file, \"w\")\n",
    "sys.stderr = open(output_file, \"a\")\n",
    "\n",
    "num_runs = 100\n",
    "\n",
    "for run in range(num_runs):\n",
    "    for file_path in file_paths:\n",
    "        X, y = load_arff(file_path)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "        \n",
    "        parameter_combinations = product(max_depth_values, max_leaf_nodes_values)  # Mover para dentro do loop\n",
    "        \n",
    "        for max_depth, max_leaf_nodes in parameter_combinations:\n",
    "            dt = DecisionTreeRegressor(max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, max_leaf_nodes=max_leaf_nodes)\n",
    "            dt.fit(X_train, y_train)\n",
    "            y_pred = dt.predict(X_test)\n",
    "            \n",
    "            mae, medae, rmse, r2 = calculate_metrics(y_test, y_pred)\n",
    "            \n",
    "            metrics_data.append([\n",
    "                file_path,\n",
    "                \"MAE\",\n",
    "                mae, max_depth, max_leaf_nodes\n",
    "            ])\n",
    "            metrics_data.append([\n",
    "                file_path,\n",
    "                \"Median Absolute Error\",\n",
    "                medae, max_depth, max_leaf_nodes\n",
    "            ])\n",
    "            metrics_data.append([\n",
    "                file_path,\n",
    "                \"RMSE\",\n",
    "                rmse, max_depth, max_leaf_nodes\n",
    "            ])\n",
    "            metrics_data.append([\n",
    "                file_path,\n",
    "                \"R2 Score\",\n",
    "                r2, max_depth, max_leaf_nodes\n",
    "            ])\n",
    "\n",
    "headers = [\"Dataset\", \"Metric\", \"Metric Value\", \"d\",\"n\"]\n",
    "print(tabulate(metrics_data, headers=headers))\n",
    "print('\\n')\n",
    "sys.stdout.close()\n",
    "sys.stderr.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13671f2-75d5-4b50-a486-6cf033ccc892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Redirecting stdout and stderr to the specified file\n",
    "output_file = \"../results/dt/sem-pre/output_dt_analises_d_n_best_semente.txt\"\n",
    "sys.stdout = open(output_file, \"w\")\n",
    "sys.stderr = open(output_file, \"a\")\n",
    "\n",
    "cont = 0\n",
    "\n",
    "# Dictionary to store metrics with nested structure for datasets, d, n, and metrics\n",
    "datasets = {\n",
    "    \"albrecht\": {},\n",
    "    \"kemerer\": {},\n",
    "    \"cocomo81\": {},\n",
    "    \"china\": {}\n",
    "}\n",
    "\n",
    "def extract_metrics_values(filename):\n",
    "    global cont\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            if not line.strip() or line.startswith('-'):\n",
    "                continue\n",
    "            \n",
    "            # Updated regex to capture d and n values\n",
    "            match = re.match(r'.*?([A-Za-z0-9_/.]+)\\s+(.*?)\\s+([\\d.]+)\\s+(\\d+)\\s+(\\d+)', line)\n",
    "            if match:\n",
    "                dataset, metric, value, d, n = match.groups()\n",
    "                value = float(value)\n",
    "                d = int(d)\n",
    "                n = int(n)\n",
    "                \n",
    "                if \"albrecht.arff\" in dataset:\n",
    "                    dataset_key = \"albrecht\"\n",
    "                elif \"kemerer.arff\" in dataset:\n",
    "                    dataset_key = \"kemerer\"\n",
    "                elif \"cocomo81.arff\" in dataset:\n",
    "                    dataset_key = \"cocomo81\"\n",
    "                elif \"china.arff\" in dataset:\n",
    "                    dataset_key = \"china\"\n",
    "                else:\n",
    "                    print(f\"Dataset not recognized: {dataset}\", file=sys.stderr)\n",
    "                    continue\n",
    "\n",
    "                # Initialize nested dictionaries if not already done\n",
    "                if d not in datasets[dataset_key]:\n",
    "                    datasets[dataset_key][d] = {}\n",
    "                if n not in datasets[dataset_key][d]:\n",
    "                    datasets[dataset_key][d][n] = {}\n",
    "                if metric not in datasets[dataset_key][d][n]:\n",
    "                    datasets[dataset_key][d][n][metric] = []\n",
    "\n",
    "                # Append the metric value\n",
    "                datasets[dataset_key][d][n][metric].append(value)\n",
    "                cont += 1\n",
    "            # else:\n",
    "            #     print(f\"Line did not match: {line.strip()}\", file=sys.stderr)\n",
    "\n",
    "def calculate_statistics(metrics):\n",
    "    results = {}\n",
    "    for metric, values in metrics.items():\n",
    "        results[metric] = {\n",
    "            'Média': np.mean(values),\n",
    "            'Mínimo': np.min(values),\n",
    "            'Máximo': np.max(values),\n",
    "            'Desvio Padrão': np.std(values)\n",
    "        }\n",
    "    return results\n",
    "\n",
    "filename = '../results/dt/sem-pre/output_dt_semente_42.txt'\n",
    "extract_metrics_values(filename)\n",
    "\n",
    "# Function to find the best n and d for each metric\n",
    "def find_best_k_p_for_metric(dataset_name, d_values, metric_name):\n",
    "    best_d_n = None\n",
    "    best_value = float('inf') if metric_name in ['MAE', 'Median Absolute Error', 'RMSE'] else float('-inf')\n",
    "    \n",
    "    for d, k_values in d_values.items():\n",
    "        for n, metrics in k_values.items():\n",
    "            if metric_name in metrics:\n",
    "                statistics = calculate_statistics(metrics)\n",
    "                metric_value = statistics[metric_name]['Média']\n",
    "                \n",
    "                if (metric_name in ['MAE', 'Median Absolute Error', 'RMSE'] and metric_value < best_value) or \\\n",
    "                   (metric_name == 'R2 Score' and metric_value > best_value):\n",
    "                    best_value = metric_value\n",
    "                    best_d_n = (d, n, statistics)\n",
    "    \n",
    "    return best_d_n\n",
    "\n",
    "# Metrics we are interested in\n",
    "metrics_of_interest = ['MAE', 'Median Absolute Error', 'RMSE', 'R2 Score']\n",
    "\n",
    "# Iterate through the datasets and print statistics for the best n and d for each metric\n",
    "for dataset_name, d_values in datasets.items():\n",
    "    print(f\"Melhores resultados para o dataset: {dataset_name}\")\n",
    "    for metric_name in metrics_of_interest:\n",
    "        best_d_n = find_best_k_p_for_metric(dataset_name, d_values, metric_name)\n",
    "        if best_d_n:\n",
    "            d, n, statistics = best_d_n\n",
    "            print(f\"  Melhor para a métrica {metric_name}: d = {d}, n = {n}\")\n",
    "            print(f\"    Média: {statistics[metric_name]['Média']:.4f}\")\n",
    "            print(f\"    Mínimo: {statistics[metric_name]['Mínimo']:.4f}\")\n",
    "            print(f\"    Máximo: {statistics[metric_name]['Máximo']:.4f}\")\n",
    "            print(f\"    Desvio Padrão: {statistics[metric_name]['Desvio Padrão']:.4f}\")\n",
    "            print()  # Add a new line for better readability\n",
    "\n",
    "#print(\"Número de iterações das métricas: \" + str(cont))\n",
    "\n",
    "sys.stdout.close()\n",
    "sys.stderr.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688eb25f-cbc4-4368-82f1-82efd4d5839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arff\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, r2_score\n",
    "from tabulate import tabulate\n",
    "from itertools import product\n",
    "import sys\n",
    "\n",
    "def calculate_metrics(true_values, predicted_values):\n",
    "    mae = mean_absolute_error(true_values, predicted_values)\n",
    "    medae = median_absolute_error(true_values, predicted_values)\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "    r2 = r2_score(true_values, predicted_values)\n",
    "    \n",
    "    return mae, medae, rmse, r2\n",
    "\n",
    "def load_arff(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = arff.load(f)\n",
    "\n",
    "    attributes = [attr[0] for attr in data['attributes']]\n",
    "    X = np.array(data['data'])[:, :-1]\n",
    "    y = np.array(data['data'])[:, -1].astype(float)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "metrics_data = []\n",
    "\n",
    "min_samples_split = 2  \n",
    "min_samples_leaf = 1  \n",
    "max_depth_values = range(5, 11)\n",
    "max_leaf_nodes_values = range(11, 21)\n",
    "\n",
    "output_file = \"../results/dt/sem-pre/output_dt_semente_42.txt\"\n",
    "sys.stdout = open(output_file, \"w\")\n",
    "sys.stderr = open(output_file, \"a\")\n",
    "\n",
    "num_runs = 30\n",
    "\n",
    "for run in range(num_runs):\n",
    "    for file_path in file_paths:\n",
    "        X, y = load_arff(file_path)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        parameter_combinations = product(max_depth_values, max_leaf_nodes_values)  # Mover para dentro do loop\n",
    "        \n",
    "        for max_depth, max_leaf_nodes in parameter_combinations:\n",
    "            dt = DecisionTreeRegressor(max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, max_leaf_nodes=max_leaf_nodes)\n",
    "            dt.fit(X_train, y_train)\n",
    "            y_pred = dt.predict(X_test)\n",
    "            \n",
    "            mae, medae, rmse, r2 = calculate_metrics(y_test, y_pred)\n",
    "            \n",
    "            metrics_data.append([\n",
    "                file_path,\n",
    "                \"MAE\",\n",
    "                mae, max_depth, max_leaf_nodes\n",
    "            ])\n",
    "            metrics_data.append([\n",
    "                file_path,\n",
    "                \"Median Absolute Error\",\n",
    "                medae, max_depth, max_leaf_nodes\n",
    "            ])\n",
    "            metrics_data.append([\n",
    "                file_path,\n",
    "                \"RMSE\",\n",
    "                rmse, max_depth, max_leaf_nodes\n",
    "            ])\n",
    "            metrics_data.append([\n",
    "                file_path,\n",
    "                \"R2 Score\",\n",
    "                r2, max_depth, max_leaf_nodes\n",
    "            ])\n",
    "\n",
    "headers = [\"Dataset\", \"Metric\", \"Metric Value\", \"d\",\"n\"]\n",
    "print(tabulate(metrics_data, headers=headers))\n",
    "print('\\n')\n",
    "sys.stdout.close()\n",
    "sys.stderr.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3039489b-1628-4972-8dcb-1fe7bfad1e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
